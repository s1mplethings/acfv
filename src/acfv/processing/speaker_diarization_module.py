#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¯´è¯äººè¯†åˆ«æ¨¡å— - ä»pyannote_gui_test.pyæå–çš„æ ¸å¿ƒåŠŸèƒ½
"""

import os
import sys
import json
import time
import pickle
import hashlib
import tempfile
import threading
import subprocess
from pathlib import Path
from datetime import datetime, timedelta
from tqdm.auto import tqdm
from acfv.runtime.token_loader import get_hf_token

# è§£å†³OpenMPå†²çª - å¿…é¡»åœ¨å¯¼å…¥å…¶ä»–åº“ä¹‹å‰è®¾ç½®
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

# ç»Ÿä¸€ HuggingFace token å¤„ç†
HF_TOKEN_AVAILABLE = bool(get_hf_token())
if not HF_TOKEN_AVAILABLE:
    # token_loader å·²è®°å½•ä¸€æ¬¡è­¦å‘Šï¼Œè¿™é‡Œä¸é‡å¤
    pass

# æ·»åŠ è½¬å½•ç›¸å…³å¯¼å…¥
try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False

# æ·»åŠ è½¬å½•åŠŸèƒ½å¯¼å…¥
try:
    from acfv.processing.transcribe_audio import (
        transcribe_audio_segment_safe,
        extract_audio_segment_safe,
    )
except ImportError:
    print("âš ï¸ transcribe_audioæ¨¡å—ä¸å¯ç”¨")

class SpeakerDiarizationProcessor:
    """è¯´è¯äººè¯†åˆ«å¤„ç†å™¨ - ç‹¬ç«‹æ¨¡å—ç‰ˆæœ¬"""
    
    def __init__(self, video_path, output_dir, progress_callback=None):
        self.video_path = video_path
        self.output_dir = output_dir
        self.progress_callback = progress_callback or self._default_progress_callback
        self.whisper_model = None
        self._should_stop = False
        
    def _default_progress_callback(self, stage, message, progress):
        """é»˜è®¤è¿›åº¦å›è°ƒ"""
        print(f"[{stage}] {message} - {progress}%")
    
    def stop(self):
        """åœæ­¢å¤„ç†"""
        self._should_stop = True
    
    def _expand_segments(self, segments, padding=0.3):
        """æ‰©å±•ç‰‡æ®µè¾¹ç•Œï¼Œé¿å…è¯­éŸ³è¢«åˆ‡æ–­ï¼Œä½†ä¿ç•™çŸ­ä¿ƒå£°éŸ³"""
        expanded_segments = []
        for i, segment in enumerate(segments):
            new_segment = segment.copy()
            
            # æ ¹æ®ç‰‡æ®µæ—¶é•¿è°ƒæ•´padding
            duration = segment['duration']
            if duration < 1.0:  # çŸ­ä¿ƒå£°éŸ³ï¼ˆå¦‚ç¬‘å£°ï¼‰
                # å¯¹çŸ­å£°éŸ³ä½¿ç”¨è¾ƒå°çš„padding
                adjusted_padding = min(padding * 0.5, 0.2)
            else:
                # å¯¹é•¿å£°éŸ³ä½¿ç”¨æ­£å¸¸padding
                adjusted_padding = padding
            
            new_segment['start'] = max(0, segment['start'] - adjusted_padding)
            
            if i < len(segments) - 1:
                next_start = segments[i + 1]['start']
                # é¿å…ä¸ä¸‹ä¸€ä¸ªç‰‡æ®µé‡å 
                max_end = min(segment['end'] + adjusted_padding, next_start - 0.05)
                new_segment['end'] = max_end
            else:
                new_segment['end'] = segment['end'] + adjusted_padding
            
            new_segment['duration'] = new_segment['end'] - new_segment['start']
            expanded_segments.append(new_segment)
        
        return expanded_segments

    def _merge_close_segments(self, segments, max_gap=2.0):
        """åˆå¹¶ç›¸è¿‘çš„åŒä¸€è¯´è¯äººç‰‡æ®µï¼Œä½†ä¿ç•™çŸ­ä¿ƒå£°éŸ³å¦‚ç¬‘å£°"""
        if not segments:
            return segments
        
        speaker_segments = {}
        for seg in segments:
            speaker = seg['speaker']
            if speaker not in speaker_segments:
                speaker_segments[speaker] = []
            speaker_segments[speaker].append(seg)
        
        merged_segments = []
        for speaker, segs in speaker_segments.items():
            segs.sort(key=lambda x: x['start'])
            current = None
            
            for seg in segs:
                if current is None:
                    current = seg.copy()
                else:
                    gap = seg['start'] - current['end']
                    
                    # æ™ºèƒ½åˆå¹¶ç­–ç•¥ï¼šä¿ç•™çŸ­ä¿ƒå£°éŸ³
                    if gap <= max_gap:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯çŸ­ä¿ƒå£°éŸ³ï¼ˆå¦‚ç¬‘å£°ï¼‰
                        is_short_sound = seg['duration'] < 1.0  # å°äº1ç§’çš„å£°éŸ³
                        is_current_short = current['duration'] < 1.0
                        
                        # å¦‚æœä¸¤ä¸ªéƒ½æ˜¯çŸ­å£°éŸ³ï¼Œæˆ–è€…é—´éš”å¾ˆå°ï¼Œåˆ™åˆå¹¶
                        if gap <= 0.5 or (is_short_sound and is_current_short):
                            current['end'] = seg['end']
                            current['duration'] = current['end'] - current['start']
                        else:
                            # ä¿å­˜å½“å‰ç‰‡æ®µï¼Œå¼€å§‹æ–°ç‰‡æ®µ
                            merged_segments.append(current)
                            current = seg.copy()
                    else:
                        # é—´éš”å¤ªå¤§ï¼Œä¿å­˜å½“å‰ç‰‡æ®µï¼Œå¼€å§‹æ–°ç‰‡æ®µ
                        merged_segments.append(current)
                        current = seg.copy()
            
            if current:
                merged_segments.append(current)
        
        return merged_segments

    def _load_whisper_model(self):
        """åŠ è½½Whisperæ¨¡å‹"""
        if not WHISPER_AVAILABLE:
            return None
        
        try:
            self.progress_callback("è½¬å½•", "ğŸ“ åŠ è½½Whisperæ¨¡å‹...", 0)
            # ä½¿ç”¨tinyæ¨¡å‹ä»¥æé«˜é€Ÿåº¦
            model = whisper.load_model("tiny")
            self.progress_callback("è½¬å½•", "âœ… Whisperæ¨¡å‹åŠ è½½å®Œæˆ", 10)
            return model
        except Exception as e:
            print(f"Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None

    def _extract_audio_from_video(self):
        """ä»è§†é¢‘ä¸­æå–éŸ³é¢‘"""
        try:
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            video_name = os.path.splitext(os.path.basename(self.video_path))[0]
            safe_name = "".join(c for c in video_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_name = safe_name[:30]  # é™åˆ¶é•¿åº¦
            
            audio_path = os.path.join(self.output_dir, f"{safe_name}_audio.wav")
            
            # ä½¿ç”¨ffmpegæå–éŸ³é¢‘
            cmd = [
                'ffmpeg', '-y',
                '-i', self.video_path,
                '-vn',  # ä¸åŒ…å«è§†é¢‘
                '-acodec', 'pcm_s16le',  # 16ä½PCMç¼–ç 
                '-ar', '16000',  # 16kHzé‡‡æ ·ç‡
                '-ac', '1',  # å•å£°é“
                audio_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore', timeout=300)
            
            if result.returncode == 0 and os.path.exists(audio_path):
                return audio_path
            else:
                print(f"éŸ³é¢‘æå–å¤±è´¥: {result.stderr}")
                return None
                
        except Exception as e:
            print(f"éŸ³é¢‘æå–å¼‚å¸¸: {e}")
            return None

    def _identify_host_speaker(self, segments, speakers):
        """è¯†åˆ«ä¸»æ’­ - é€‰æ‹©è¯´è¯æ—¶é—´æœ€é•¿çš„äºº"""
        if not segments or not speakers:
            return None
        
        # ç»Ÿè®¡æ¯ä¸ªè¯´è¯äººçš„æ€»æ—¶é•¿
        speaker_duration = {}
        for segment in segments:
            speaker = segment['speaker']
            if speaker not in speaker_duration:
                speaker_duration[speaker] = 0
            speaker_duration[speaker] += segment['duration']
        
        # è¿”å›è¯´è¯æ—¶é—´æœ€é•¿çš„äºº
        host_speaker = max(speaker_duration.items(), key=lambda x: x[1])[0]
        return host_speaker

    def _generate_host_audio(self, host_segments, audio_path):
        """ç”Ÿæˆä¸»æ’­éŸ³é¢‘æ–‡ä»¶"""
        if not host_segments:
            return None
        
        try:
            # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶åç”Ÿæˆ
            video_name = os.path.splitext(os.path.basename(self.video_path))[0]
            safe_name = "".join(c for c in video_name if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_name = safe_name[:20]  # é™åˆ¶é•¿åº¦
            
            host_audio_file = os.path.join(self.output_dir, f"{safe_name}_host_audio.wav")
            
            # åˆ›å»ºffmpegè¿‡æ»¤å™¨å­—ç¬¦ä¸²
            filter_parts = []
            for i, segment in enumerate(host_segments):
                start = segment['start']
                duration = segment['duration']
                filter_parts.append(f"[0:a]atrim=start={start}:duration={duration},asetpts=PTS-STARTPTS[a{i}]")
            
            # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
            if len(filter_parts) > 1:
                concat_inputs = ''.join([f"[a{i}]" for i in range(len(filter_parts))])
                concat_filter = f"{concat_inputs}concat=n={len(filter_parts)}:v=0:a=1[out]"
                full_filter = ';'.join(filter_parts) + ';' + concat_filter
                output_map = "[out]"
            else:
                full_filter = filter_parts[0]
                output_map = "[a0]"
            
            cmd = [
                'ffmpeg', '-y',
                '-i', audio_path,
                '-filter_complex', full_filter,
                '-map', output_map,
                host_audio_file
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore', timeout=1000)
            
            if result.returncode == 0 and os.path.exists(host_audio_file):
                return host_audio_file
            else:
                print(f"éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {result.stderr}")
                return None
                
        except Exception as e:
            print(f"éŸ³é¢‘ç”Ÿæˆå¼‚å¸¸: {e}")
            return None

    def process_video(self):
        """å¤„ç†è§†é¢‘ - ä¸»è¦æµç¨‹"""
        try:
            # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦åº”è¯¥åœæ­¢
            if self._should_stop:
                return None
                
            self.progress_callback("åˆå§‹åŒ–", "ğŸ” æ£€æŸ¥ç¯å¢ƒå’Œä¾èµ–...", 5)
            
            # æ£€æŸ¥ä¾èµ–
            try:
                from pyannote.audio import Pipeline
                self.progress_callback("åˆå§‹åŒ–", "âœ… pyannote.audio å¯ç”¨", 10)
            except ImportError as e:
                error_msg = f"pyannote.audio æœªå®‰è£…: {e}"
                self.progress_callback("é”™è¯¯", error_msg, 0)
                return None
            
            # æ£€æŸ¥token
            if not HF_TOKEN_AVAILABLE:
                error_msg = "HuggingFace token æœªé…ç½®ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ HUGGINGFACE_TOKEN æˆ– secrets/config.json"
                self.progress_callback("é”™è¯¯", error_msg, 0)
                return None
            
            token = os.environ.get('HUGGINGFACE_HUB_TOKEN')
            
            self.progress_callback("è®¤è¯", "âœ… HuggingFaceè®¤è¯é€šè¿‡", 15)
            
            # æå–éŸ³é¢‘
            self.progress_callback("æå–", "ğŸµ ä»è§†é¢‘æå–éŸ³é¢‘...", 20)
            audio_path = self._extract_audio_from_video()
            if not audio_path:
                error_msg = "éŸ³é¢‘æå–å¤±è´¥"
                self.progress_callback("é”™è¯¯", error_msg, 0)
                return None
            
            self.progress_callback("æå–", "âœ… éŸ³é¢‘æå–å®Œæˆ", 30)
            
            # è¯´è¯äººåˆ†ç¦»
            self.progress_callback("åˆ†ç¦»", "ğŸ¤ å¼€å§‹è¯´è¯äººåˆ†ç¦»...", 40)
            
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=token
            )
            
            # ä¼˜åŒ–å‚æ•°ä»¥æé«˜é€Ÿåº¦å’Œè¯†åˆ«éè¯­éŸ³å£°éŸ³
            try:
                pipeline.instantiate({
                    # ä¼˜åŒ–åˆ†å‰²å‚æ•° - é™ä½æœ€å°æ—¶é•¿ä»¥æ•è·ç¬‘å£°ç­‰çŸ­å£°éŸ³
                    "segmentation": {
                        "min_duration": 0.1,  # é™ä½æœ€å°ç‰‡æ®µæ—¶é•¿ï¼Œæ•è·çŸ­ä¿ƒå£°éŸ³
                        "max_duration": 30.0,  # è®¾ç½®æœ€å¤§ç‰‡æ®µæ—¶é•¿
                        "threshold": 0.5,  # é™ä½é˜ˆå€¼ä»¥æ•è·æ›´å¤šå£°éŸ³
                        "min_activity": 0.1  # é™ä½æœ€å°æ´»åŠ¨åº¦
                    },
                    # ä¼˜åŒ–èšç±»å‚æ•° - æé«˜å¯¹éè¯­éŸ³å£°éŸ³çš„æ•æ„Ÿåº¦
                    "clustering": {
                        "method": "centroid",
                        "min_cluster_size": 1,  # é™ä½æœ€å°èšç±»å¤§å°
                        "threshold": 0.25,  # é™ä½èšç±»é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿ
                        "covariance_type": "diag"  # ä½¿ç”¨å¯¹è§’åæ–¹å·®çŸ©é˜µï¼Œæé«˜é€Ÿåº¦
                    },
                })
            except Exception as e:
                print(f"å‚æ•°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°: {e}")
            
            # æ‰§è¡Œè¯´è¯äººåˆ†ç¦»
            diarization = pipeline(audio_path)
            
            # æå–ç‰‡æ®µä¿¡æ¯
            segments = []
            speakers = set()
            
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                segment = {
                    'start': turn.start,
                    'end': turn.end,
                    'duration': turn.end - turn.start,
                    'speaker': speaker
                }
                segments.append(segment)
                speakers.add(speaker)
            
            self.progress_callback("åˆ†ç¦»", f"âœ… è¯´è¯äººåˆ†ç¦»å®Œæˆï¼Œè¯†åˆ«åˆ° {len(speakers)} ä¸ªè¯´è¯äºº", 60)
            
            # æ‰©å±•å’Œåˆå¹¶ç‰‡æ®µ
            self.progress_callback("å¤„ç†", "ğŸ”§ ä¼˜åŒ–ç‰‡æ®µè¾¹ç•Œ...", 65)
            expanded_segments = self._expand_segments(segments)
            merged_segments = self._merge_close_segments(expanded_segments)
            
            # è¯†åˆ«ä¸»æ’­
            self.progress_callback("è¯†åˆ«", "ğŸ¯ è¯†åˆ«ä¸»æ’­...", 70)
            host_speaker = self._identify_host_speaker(merged_segments, speakers)
            
            if host_speaker:
                self.progress_callback("è¯†åˆ«", f"âœ… ä¸»æ’­è¯†åˆ«å®Œæˆ: {host_speaker}", 75)
                
                # æå–ä¸»æ’­ç‰‡æ®µ
                host_segments = [seg for seg in merged_segments if seg['speaker'] == host_speaker]
                
                # ç”Ÿæˆä¸»æ’­éŸ³é¢‘
                self.progress_callback("ç”Ÿæˆ", "ğŸµ ç”Ÿæˆä¸»æ’­éŸ³é¢‘...", 80)
                host_audio_path = self._generate_host_audio(host_segments, audio_path)
                
                # å‡†å¤‡ç»“æœ
                result = {
                    'video_path': self.video_path,
                    'audio_path': audio_path,
                    'host_speaker': host_speaker,
                    'host_audio_path': host_audio_path,
                    'all_segments': merged_segments,
                    'host_segments': host_segments,
                    'speakers': list(speakers),
                    'total_speakers': len(speakers),
                    'total_segments': len(merged_segments),
                    'host_segments_count': len(host_segments),
                    'processing_time': datetime.now().isoformat()
                }
                
                self.progress_callback("å®Œæˆ", "âœ… å¤„ç†å®Œæˆ", 100)
                return result
            else:
                error_msg = "æ— æ³•è¯†åˆ«ä¸»æ’­"
                self.progress_callback("é”™è¯¯", error_msg, 0)
                return None
                
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {e}"
            self.progress_callback("é”™è¯¯", error_msg, 0)
            print(f"å¤„ç†å¼‚å¸¸: {e}")
            return None

def process_video_with_speaker_diarization(video_path, output_dir, progress_callback=None):
    """ä¾¿æ·å‡½æ•°ï¼šå¤„ç†è§†é¢‘å¹¶è¿”å›è¯´è¯äººè¯†åˆ«ç»“æœ"""
    processor = SpeakerDiarizationProcessor(video_path, output_dir, progress_callback)
    return processor.process_video()

if __name__ == "__main__":
    print("ğŸ¤ è¯´è¯äººè¯†åˆ«æ¨¡å—")
    print("ä½¿ç”¨æ–¹æ³•: python speaker_diarization_module.py") 