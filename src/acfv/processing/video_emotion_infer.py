# video_emotion_infer.py - åˆ†æ®µYOLOå¤„ç†å™¨å’Œæƒ…ç»ªæ¨ç†
import os
import cv2
import json
import numpy as np
import subprocess
import tempfile
from pathlib import Path
from sklearn.cluster import DBSCAN

# å°è¯•å¯¼å…¥YOLOï¼Œå¦‚æœå¤±è´¥åˆ™æä¾›fallback
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    # å°†printè¯­å¥ç§»åˆ°å‡½æ•°å†…éƒ¨ï¼Œé¿å…åœ¨å¯¼å…¥æ—¶æ‰§è¡Œ
    YOLO_AVAILABLE = False

class SegmentedYOLOProcessor:
    def __init__(self, yolo_weights="best.pt", segment_length=4.0, confidence_threshold=0.5):
        """
        åˆ†æ®µYOLOå¤„ç†å™¨ - æ¯Nç§’åŠ¨æ€æ£€æµ‹å’Œè£å‰ª
        
        Args:
            yolo_weights: YOLOæ¨¡å‹æƒé‡æ–‡ä»¶
            segment_length: åˆ†æ®µé•¿åº¦ï¼ˆç§’ï¼‰
            confidence_threshold: æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
        """
        self.segment_length = segment_length
        self.confidence_threshold = confidence_threshold
        self.yolo_model = None
        self.yolo_available = YOLO_AVAILABLE
        
        if not YOLO_AVAILABLE:
            print("âŒ YOLOä¸å¯ç”¨ï¼Œå°†è·³è¿‡ç›®æ ‡æ£€æµ‹")
            return
            
        try:
            print(f"ğŸ” åŠ è½½YOLOæ¨¡å‹: {yolo_weights}")
            
            # æ£€æŸ¥æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(yolo_weights):
                print(f"âš ï¸ YOLOæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {yolo_weights}")
                print("ğŸ”„ å°è¯•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹...")
                # ä½¿ç”¨ultralyticsçš„é¢„è®­ç»ƒæ¨¡å‹
                yolo_weights = "yolov8n.pt"  # å°æ¨¡å‹ï¼Œå¿«é€Ÿä¸‹è½½
            
            self.yolo_model = YOLO(yolo_weights)
            print("âœ… YOLOæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ YOLOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("âš ï¸ å°†è·³è¿‡YOLOå¤„ç†")
            self.yolo_available = False

    def get_video_info(self, video_path):
        """è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        duration = total_frames / fps if fps > 0 else 0
        cap.release()
        
        return {
            'fps': fps,
            'total_frames': total_frames,
            'width': width,
            'height': height,
            'duration': duration
        }

    def detect_segment_boxes(self, video_path, start_time, end_time, sample_interval=0.5):
        """
        æ£€æµ‹æŒ‡å®šæ—¶é—´æ®µå†…çš„ç›®æ ‡æ¡†
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            start_time: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
            sample_interval: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
        """
        if not self.yolo_available or self.yolo_model is None:
            return []
            
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        boxes = []
        current_time = start_time
        
        while current_time < end_time:
            frame_number = int(current_time * fps)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            
            ret, frame = cap.read()
            if not ret:
                break
            
            try:
                results = self.yolo_model(frame, verbose=False)
                
                # æ‰¾åˆ°æœ€å¤§çš„æ£€æµ‹æ¡†
                max_area = 0
                best_box = None
                
                for r in results:
                    if len(r.boxes) > 0:
                        for i, box in enumerate(r.boxes.xyxy):
                            conf = float(r.boxes.conf[i])
                            if conf < self.confidence_threshold:
                                continue
                                
                            x1, y1, x2, y2 = map(int, box[:4])
                            area = (x2 - x1) * (y2 - y1)
                            
                            if area > max_area:
                                max_area = area
                                best_box = [x1, y1, x2, y2]
                
                if best_box is not None:
                    boxes.append(best_box)
                    
            except Exception as e:
                print(f"âš ï¸ æ£€æµ‹æ—¶é—´ {current_time:.2f}s æ—¶å‡ºé”™: {e}")
            
            current_time += sample_interval
        
        cap.release()
        return boxes

    def cluster_boxes(self, boxes, eps=20, min_samples=3):
        """å¯¹æ£€æµ‹æ¡†è¿›è¡Œèšç±»ï¼Œè¿”å›ä¸»è¦åŒºåŸŸ"""
        if not boxes:
            return None
            
        if len(boxes) < min_samples:
            # æ¡†å¤ªå°‘ï¼Œè¿”å›è”åˆè¾¹ç•Œæ¡†
            x1 = min(b[0] for b in boxes)
            y1 = min(b[1] for b in boxes)
            x2 = max(b[2] for b in boxes)
            y2 = max(b[3] for b in boxes)
            return [x1, y1, x2, y2]
        
        # è®¡ç®—ä¸­å¿ƒç‚¹
        centers = np.array([[(box[0] + box[2]) / 2, (box[1] + box[3]) / 2] for box in boxes])
        
        # èšç±»
        db = DBSCAN(eps=eps, min_samples=min_samples).fit(centers)
        labels = db.labels_
        
        # æ‰¾åˆ°æœ€å¤§çš„èšç±»
        valid_indices = [i for i, label in enumerate(labels) if label != -1]
        if not valid_indices:
            # æ²¡æœ‰æœ‰æ•ˆèšç±»ï¼Œè¿”å›æ‰€æœ‰æ¡†çš„è”åˆè¾¹ç•Œæ¡†
            x1 = min(b[0] for b in boxes)
            y1 = min(b[1] for b in boxes)
            x2 = max(b[2] for b in boxes)
            y2 = max(b[3] for b in boxes)
            return [x1, y1, x2, y2]
        
        # é€‰æ‹©æœ€å¤§èšç±»
        unique_labels, counts = np.unique([labels[i] for i in valid_indices], return_counts=True)
        best_cluster = unique_labels[np.argmax(counts)]
        cluster_boxes = [boxes[i] for i in range(len(boxes)) if labels[i] == best_cluster]
        
        # è¿”å›èšç±»çš„è”åˆè¾¹ç•Œæ¡†
        x1 = min(b[0] for b in cluster_boxes)
        y1 = min(b[1] for b in cluster_boxes)
        x2 = max(b[2] for b in cluster_boxes)
        y2 = max(b[3] for b in cluster_boxes)
        
        return [x1, y1, x2, y2]

    def smooth_crop_regions(self, crop_regions, video_info):
        """
        å¹³æ»‘è£å‰ªåŒºåŸŸï¼Œé¿å…å‰§çƒˆè·³è·ƒ
        
        Args:
            crop_regions: æ¯æ®µçš„è£å‰ªåŒºåŸŸåˆ—è¡¨
            video_info: è§†é¢‘ä¿¡æ¯
        """
        if not crop_regions:
            return []
        
        smoothed = []
        width, height = video_info['width'], video_info['height']
        
        for i, region in enumerate(crop_regions):
            if region is None:
                # ä½¿ç”¨å‰ä¸€ä¸ªåŒºåŸŸæˆ–é»˜è®¤åŒºåŸŸ
                if smoothed:
                    smoothed.append(smoothed[-1])
                else:
                    smoothed.append([0, 0, width, height])
                continue
            
            x1, y1, x2, y2 = region
            
            # è¾¹ç•Œæ£€æŸ¥
            x1 = max(0, min(x1, width - 1))
            x2 = max(x1 + 1, min(x2, width))
            y1 = max(0, min(y1, height - 1))
            y2 = max(y1 + 1, min(y2, height))
            
            if smoothed:
                # ä¸å‰ä¸€ä¸ªåŒºåŸŸè¿›è¡Œå¹³æ»‘
                prev_x1, prev_y1, prev_x2, prev_y2 = smoothed[-1]
                
                # ä½¿ç”¨åŠ æƒå¹³å‡è¿›è¡Œå¹³æ»‘
                alpha = 0.7  # å½“å‰å¸§æƒé‡
                x1 = int(alpha * x1 + (1 - alpha) * prev_x1)
                y1 = int(alpha * y1 + (1 - alpha) * prev_y1)
                x2 = int(alpha * x2 + (1 - alpha) * prev_x2)
                y2 = int(alpha * y2 + (1 - alpha) * prev_y2)
            
            smoothed.append([x1, y1, x2, y2])
        
        return smoothed

    def extract_audio(self, input_video, output_audio):
        """ä½¿ç”¨ ffmpeg æå–è§†é¢‘ä¸­çš„éŸ³é¢‘"""
        cmd = [
            'ffmpeg', '-y',
            '-loglevel', 'error',
            '-i', input_video,
            '-vn',
            '-acodec', 'copy',
            output_audio
        ]
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            return True
        except subprocess.CalledProcessError as e:
            print(f"æå–éŸ³é¢‘å¤±è´¥: {e}")
            return False
        except FileNotFoundError:
            print("âŒ ffmpegä¸å¯ç”¨ï¼Œæ— æ³•æå–éŸ³é¢‘")
            return False

    def merge_audio_video(self, input_video, temp_video, output_video):
        """åˆå¹¶éŸ³é¢‘å’Œè§†é¢‘"""
        cmd = [
            'ffmpeg', '-y',
            '-loglevel', 'error',
            '-i', temp_video,
            '-i', input_video,
            '-c:v', 'copy',
            '-c:a', 'aac',
            '-map', '0:v:0',
            '-map', '1:a:0',
            output_video
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            return True
        except Exception as e:
            print(f"éŸ³é¢‘åˆå¹¶å¤±è´¥: {e}")
            return False

    def process_video_segments(self, input_path, output_path):
        """
        åˆ†æ®µå¤„ç†è§†é¢‘ - æ¯æ®µä½¿ç”¨ä¸åŒçš„è£å‰ªåŒºåŸŸ
        
        Args:
            input_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        """
        print(f"ğŸ¬ å¼€å§‹åˆ†æ®µYOLOå¤„ç†: {input_path}")
        
        # å¦‚æœYOLOä¸å¯ç”¨ï¼Œç›´æ¥å¤åˆ¶åŸè§†é¢‘
        if not self.yolo_available:
            print("âš ï¸ YOLOä¸å¯ç”¨ï¼Œç›´æ¥å¤åˆ¶åŸè§†é¢‘")
            import shutil
            shutil.copy2(input_path, output_path)
            return True
        
        # è·å–è§†é¢‘ä¿¡æ¯
        video_info = self.get_video_info(input_path)
        duration = video_info['duration']
        fps = video_info['fps']
        
        print(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯: {duration:.2f}ç§’, {fps:.1f}fps, {video_info['width']}x{video_info['height']}")
        
        # è®¡ç®—åˆ†æ®µæ•°é‡
        num_segments = int(np.ceil(duration / self.segment_length))
        print(f"ğŸ“Š å°†å¤„ç† {num_segments} ä¸ª{self.segment_length}ç§’æ®µ")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šä¸ºæ¯ä¸ªæ®µæ£€æµ‹æœ€ä½³è£å‰ªåŒºåŸŸ
        print("ğŸ” ç¬¬ä¸€é˜¶æ®µï¼šæ£€æµ‹æ¯æ®µçš„æœ€ä½³è£å‰ªåŒºåŸŸ...")
        crop_regions = []
        
        for i in range(num_segments):
            start_time = i * self.segment_length
            end_time = min((i + 1) * self.segment_length, duration)
            
            print(f"  æ®µ {i+1}/{num_segments}: {start_time:.1f}s - {end_time:.1f}s")
            
            # æ£€æµ‹è¿™ä¸ªæ—¶é—´æ®µçš„ç›®æ ‡æ¡†
            boxes = self.detect_segment_boxes(input_path, start_time, end_time)
            
            # è®¡ç®—æœ€ä½³è£å‰ªåŒºåŸŸ
            crop_region = self.cluster_boxes(boxes) if boxes else None
            crop_regions.append(crop_region)
            
            if crop_region:
                x1, y1, x2, y2 = crop_region
                print(f"    è£å‰ªåŒºåŸŸ: [{x1}, {y1}, {x2}, {y2}] å°ºå¯¸: {x2-x1}x{y2-y1}")
            else:
                print(f"    æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œå°†ä½¿ç”¨é»˜è®¤åŒºåŸŸ")
        
        # å¹³æ»‘è£å‰ªåŒºåŸŸ
        print("ğŸ”§ å¹³æ»‘è£å‰ªåŒºåŸŸ...")
        crop_regions = self.smooth_crop_regions(crop_regions, video_info)
        
        # ç¬¬äºŒé˜¶æ®µï¼šæŒ‰åˆ†æ®µè£å‰ªè§†é¢‘
        print("âœ‚ï¸ ç¬¬äºŒé˜¶æ®µï¼šåˆ†æ®µè£å‰ªè§†é¢‘...")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_video_path = output_path.replace('.mp4', '_temp_nosound.mp4')
        
        # å¤„ç†è§†é¢‘
        success = self._crop_video_segments(input_path, temp_video_path, crop_regions, video_info)
        
        if not success:
            print("âŒ è§†é¢‘è£å‰ªå¤±è´¥")
            return False
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šæ·»åŠ éŸ³é¢‘
        print("ğŸµ ç¬¬ä¸‰é˜¶æ®µï¼šæ·»åŠ éŸ³é¢‘...")
        temp_audio_path = output_path.replace('.mp4', '_temp_audio.aac')
        
        if self.extract_audio(input_path, temp_audio_path):
            if self.merge_audio_video(input_path, temp_video_path, output_path):
                print("âœ… åˆ†æ®µYOLOå¤„ç†å®Œæˆ")
                success = True
            else:
                print("âš ï¸ éŸ³é¢‘åˆå¹¶å¤±è´¥ï¼Œä½¿ç”¨æ— éŸ³é¢‘ç‰ˆæœ¬")
                import shutil
                shutil.move(temp_video_path, output_path)
                success = True
        else:
            print("âš ï¸ éŸ³é¢‘æå–å¤±è´¥ï¼Œä½¿ç”¨æ— éŸ³é¢‘ç‰ˆæœ¬")
            import shutil
            shutil.move(temp_video_path, output_path)
            success = True
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp_file in [temp_video_path, temp_audio_path]:
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except:
                    pass
        
        return success

    def _crop_video_segments(self, input_path, output_path, crop_regions, video_info):
        """è£å‰ªè§†é¢‘æ®µ"""
        try:
            cap = cv2.VideoCapture(input_path)
            fps = video_info['fps']
            
            # è®¡ç®—è¾“å‡ºè§†é¢‘çš„å°ºå¯¸ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆè£å‰ªåŒºåŸŸçš„å°ºå¯¸ï¼‰
            valid_region = None
            for region in crop_regions:
                if region is not None:
                    valid_region = region
                    break
            
            if valid_region is None:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è£å‰ªåŒºåŸŸ")
                cap.release()
                return False
            
            x1, y1, x2, y2 = valid_region
            crop_w, crop_h = x2 - x1, y2 - y1
            
            # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (crop_w, crop_h))
            
            current_segment = 0
            frame_count = 0
            total_frames = video_info['total_frames']
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # è®¡ç®—å½“å‰å¸§å±äºå“ªä¸ªæ®µ
                current_time = frame_count / fps
                segment_index = min(int(current_time / self.segment_length), len(crop_regions) - 1)
                
                # è·å–å½“å‰æ®µçš„è£å‰ªåŒºåŸŸ
                crop_region = crop_regions[segment_index]
                if crop_region is None:
                    crop_region = [0, 0, video_info['width'], video_info['height']]
                
                x1, y1, x2, y2 = crop_region
                
                # è£å‰ªå¸§
                h, w = frame.shape[:2]
                x1_c = max(0, min(x1, w - 1))
                x2_c = max(x1_c + 1, min(x2, w))
                y1_c = max(0, min(y1, h - 1))
                y2_c = max(y1_c + 1, min(y2, h))
                
                cropped = frame[y1_c:y2_c, x1_c:x2_c]
                
                # ç¡®ä¿å°ºå¯¸ä¸€è‡´
                if cropped.shape[1] != crop_w or cropped.shape[0] != crop_h:
                    cropped = cv2.resize(cropped, (crop_w, crop_h))
                
                out.write(cropped)
                
                frame_count += 1
                if frame_count % 1000 == 0:
                    progress = frame_count / total_frames * 100
                    print(f"  è£å‰ªè¿›åº¦: {progress:.1f}%")
            
            cap.release()
            out.release()
            
            print(f"âœ… è§†é¢‘è£å‰ªå®Œæˆï¼Œå…±å¤„ç† {frame_count} å¸§")
            return True
            
        except Exception as e:
            print(f"âŒ è§†é¢‘è£å‰ªå¤±è´¥: {e}")
            if 'cap' in locals():
                cap.release()
            if 'out' in locals():
                out.release()
            return False

# æ–°å¢ï¼šæƒ…ç»ªæ¨ç†åŠŸèƒ½
class VideoEmotionInference:
    def __init__(self, segment_length=4.0):
        """
        è§†é¢‘æƒ…ç»ªæ¨ç†å™¨
        
        Args:
            segment_length: åˆ†æ®µé•¿åº¦ï¼ˆç§’ï¼‰
        """
        self.segment_length = segment_length
    
    def analyze_video_emotion(self, video_path, output_file):
        """
        åˆ†æè§†é¢‘çš„æƒ…ç»ªå˜åŒ–
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ­ å¼€å§‹è§†é¢‘æƒ…ç»ªåˆ†æ: {video_path}")
        
        try:
            # è·å–è§†é¢‘ä¿¡æ¯
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps if fps > 0 else 0
            cap.release()
            
            print(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯: {duration:.2f}ç§’, {fps:.1f}fps")
            
            # è®¡ç®—åˆ†æ®µæ•°é‡
            num_segments = int(np.ceil(duration / self.segment_length))
            print(f"ğŸ“Š å°†åˆ†æ {num_segments} ä¸ª{self.segment_length}ç§’æ®µ")
            
            # åˆ†ææ¯ä¸ªæ—¶é—´æ®µ
            emotion_data = []
            
            for i in range(num_segments):
                start_time = i * self.segment_length
                end_time = min((i + 1) * self.segment_length, duration)
                
                # æ¨¡æ‹Ÿæƒ…ç»ªåˆ†æï¼ˆè¿™é‡Œå¯ä»¥æ›¿æ¢ä¸ºå®é™…çš„æƒ…ç»ªæ¨ç†æ¨¡å‹ï¼‰
                emotion_score = self._mock_emotion_analysis(video_path, start_time, end_time)
                
                segment_data = {
                    "start": start_time,
                    "end": end_time,
                    "emotion_score": emotion_score,
                    "emotion_type": self._classify_emotion(emotion_score),
                    "segment_index": i
                }
                
                emotion_data.append(segment_data)
                print(f"  æ®µ {i+1}/{num_segments}: {start_time:.1f}s-{end_time:.1f}s æƒ…ç»ªåˆ†æ•°: {emotion_score:.3f}")
            
            # ä¿å­˜ç»“æœ
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(emotion_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æƒ…ç»ªåˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")
            return True
            
        except Exception as e:
            print(f"âŒ æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
            # åˆ›å»ºç©ºçš„ç»“æœæ–‡ä»¶
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump([], f)
            return False
    
    def _mock_emotion_analysis(self, video_path, start_time, end_time):
        """
        æ¨¡æ‹Ÿæƒ…ç»ªåˆ†æï¼ˆå¯ä»¥æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹æ¨ç†ï¼‰
        
        Args:
            video_path: è§†é¢‘è·¯å¾„
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
        
        Returns:
            emotion_score: æƒ…ç»ªåˆ†æ•° (0.0-1.0)
        """
        # è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿå®ç°
        # å®é™…åº”ç”¨ä¸­å¯ä»¥æ›¿æ¢ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¦‚ï¼š
        # - é¢éƒ¨è¡¨æƒ…è¯†åˆ«
        # - åŠ¨ä½œè¯†åˆ«
        # - éŸ³é¢‘æƒ…ç»ªè¯†åˆ«ç­‰
        
        try:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            # è·³è½¬åˆ°æŒ‡å®šæ—¶é—´
            start_frame = int(start_time * fps)
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
            
            # é‡‡æ ·å‡ å¸§è¿›è¡Œåˆ†æ
            sample_frames = []
            current_time = start_time
            
            while current_time < end_time and len(sample_frames) < 5:
                ret, frame = cap.read()
                if not ret:
                    break
                sample_frames.append(frame)
                # è·³è¿‡ä¸€äº›å¸§
                for _ in range(int(fps * 0.5)):  # æ¯0.5ç§’é‡‡æ ·ä¸€å¸§
                    cap.read()
                    current_time += 0.5
            
            cap.release()
            
            if not sample_frames:
                return 0.5  # é»˜è®¤ä¸­æ€§æƒ…ç»ª
            
            # ç®€å•çš„å›¾åƒç‰¹å¾åˆ†æï¼ˆå¯ä»¥æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹ï¼‰
            emotion_score = 0.0
            for frame in sample_frames:
                # è®¡ç®—å›¾åƒçš„äº®åº¦å˜åŒ–ä½œä¸ºç®€å•çš„æƒ…ç»ªæŒ‡æ ‡
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                brightness = np.mean(gray) / 255.0
                contrast = np.std(gray) / 255.0
                
                # ç®€å•çš„æƒ…ç»ªè¯„åˆ†ç®—æ³•ï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
                frame_emotion = (brightness * 0.6 + contrast * 0.4)
                emotion_score += frame_emotion
            
            emotion_score = emotion_score / len(sample_frames)
            return min(max(emotion_score, 0.0), 1.0)  # é™åˆ¶åœ¨0-1èŒƒå›´å†…
            
        except Exception as e:
            print(f"âš ï¸ åˆ†ææ®µ {start_time:.1f}s-{end_time:.1f}s æ—¶å‡ºé”™: {e}")
            return 0.5  # è¿”å›ä¸­æ€§æƒ…ç»ªåˆ†æ•°
    
    def _classify_emotion(self, emotion_score):
        """
        æ ¹æ®æƒ…ç»ªåˆ†æ•°åˆ†ç±»æƒ…ç»ªç±»å‹
        
        Args:
            emotion_score: æƒ…ç»ªåˆ†æ•° (0.0-1.0)
        
        Returns:
            emotion_type: æƒ…ç»ªç±»å‹å­—ç¬¦ä¸²
        """
        if emotion_score < 0.2:
            return "very_low"
        elif emotion_score < 0.4:
            return "low"
        elif emotion_score < 0.6:
            return "neutral"
        elif emotion_score < 0.8:
            return "high"
        else:
            return "very_high"

# ä¸»è¦çš„è¿è¡Œå‡½æ•°ï¼Œä¾›ä¸»ç¨‹åºè°ƒç”¨
def run(video_path, output_file, args=None):
    """
    è¿è¡Œè§†é¢‘æƒ…ç»ªæ¨ç†
    
    Args:
        video_path: è¾“å…¥è§†é¢‘è·¯å¾„
        output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        args: å‚æ•°å¯¹è±¡ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰
    """
    try:
        # ä»argsè·å–å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        segment_length = getattr(args, 'segment_length', 4.0) if args else 4.0
        
        # åˆ›å»ºæƒ…ç»ªæ¨ç†å™¨
        emotion_analyzer = VideoEmotionInference(segment_length=segment_length)
        
        # æ‰§è¡Œåˆ†æ
        success = emotion_analyzer.analyze_video_emotion(video_path, output_file)
        
        if success:
            print(f"ğŸ‰ è§†é¢‘æƒ…ç»ªæ¨ç†å®Œæˆ: {output_file}")
        else:
            print(f"âŒ è§†é¢‘æƒ…ç»ªæ¨ç†å¤±è´¥")
            
        return success
        
    except Exception as e:
        print(f"âŒ è§†é¢‘æƒ…ç»ªæ¨ç†å‡ºé”™: {e}")
        # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å­˜åœ¨ï¼Œå³ä½¿æ˜¯ç©ºçš„
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump([], f)
        except:
            pass
        return False

# ä¾¿æ·å‡½æ•°
def process_video_with_segmented_yolo(input_path, output_path, yolo_weights="best.pt", segment_length=4.0):
    """
    ä½¿ç”¨åˆ†æ®µYOLOå¤„ç†è§†é¢‘
    
    Args:
        input_path: è¾“å…¥è§†é¢‘è·¯å¾„
        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        yolo_weights: YOLOæƒé‡æ–‡ä»¶
        segment_length: åˆ†æ®µé•¿åº¦ï¼ˆç§’ï¼‰
    """
    processor = SegmentedYOLOProcessor(
        yolo_weights=yolo_weights,
        segment_length=segment_length
    )
    
    return processor.process_video_segments(input_path, output_path)

if __name__ == "__main__":
    print("ğŸ¬ è§†é¢‘æƒ…ç»ªæ¨ç†æ¨¡å—")
    print("ä½¿ç”¨æ–¹æ³•: python video_emotion_infer.py <è§†é¢‘è·¯å¾„> <è¾“å‡ºæ–‡ä»¶>")