# Extracted core backend functionality from main.py

from __future__ import annotations

from typing import List, Tuple

def _segment_score(seg: dict) -> float:
    for key in ("score", "interest_score", "rating", "density", "clip_score"):
        val = seg.get(key)
        if val is None:
            continue
        try:
            return float(val)
        except Exception:
            continue
    return 0.0

def _normalize_segments_to_target(
    segments: List[dict],
    desired_count: int,
    video_duration: float,
    min_sec: float,
    target_sec: float,
    max_sec: float,
    prefer_score: bool = True,
) -> List[dict]:
    processed: List[Tuple[float, int, float, float, dict]] = []
    for idx, seg in enumerate(segments):
        try:
            start = float(seg.get("start", 0.0))
            end = float(seg.get("end", 0.0))
        except Exception:
            continue
        if end <= start:
            continue
        processed.append((_segment_score(seg), idx, start, end, seg))

    if not processed:
        return []

    highest_end = max(item[3] for item in processed)
    if not video_duration or video_duration < highest_end:
        video_duration = max(highest_end, video_duration or 0.0)

    if prefer_score:
        ordering = sorted(processed, key=lambda x: (x[0], -x[2]), reverse=True)
    else:
        ordering = sorted(processed, key=lambda x: x[2])

    selected: List[dict] = []
    used_indices = set()

    def overlaps(range_a, range_b):
        inter = min(range_a[1], range_b[1]) - max(range_a[0], range_b[0])
        if inter <= 0:
            return 0.0
        union = max(range_a[1], range_b[1]) - min(range_a[0], range_b[0])
        return inter / union if union > 0 else 0.0

    def adjust_window(start: float, end: float) -> Tuple[float, float]:
        length = end - start
        target = target_sec
        if length > max_sec:
            target = max_sec
        elif length < min_sec:
            target = target_sec
        else:
            target = max(min(length, max_sec), min_sec)
        center = (start + end) / 2.0
        start_new = center - target / 2.0
        if start_new < 0.0:
            start_new = 0.0
        end_new = start_new + target
        if end_new > video_duration:
            end_new = video_duration
            start_new = max(0.0, end_new - target)
        if end_new - start_new < min_sec:
            if end_new == video_duration:
                start_new = max(0.0, video_duration - min_sec)
                end_new = video_duration
            else:
                end_new = min(video_duration, start_new + min_sec)
        return start_new, end_new

    def add_candidate(score: float, idx_seg: int, start: float, end: float, origin: dict) -> bool:
        start_adj, end_adj = adjust_window(start, end)
        candidate_range = (start_adj, end_adj)
        for existing in selected:
            if overlaps(candidate_range, (existing['start'], existing['end'])) > 0.35:
                return False
        seg_copy = dict(origin)
        seg_copy['start'] = float(start_adj)
        seg_copy['end'] = float(end_adj)
        seg_copy['score'] = float(score)
        seg_copy['_source_index'] = idx_seg
        selected.append(seg_copy)
        used_indices.add(idx_seg)
        return True

    for score, idx_seg, start, end, seg in ordering:
        if desired_count > 0 and len(selected) >= desired_count:
            break
        add_candidate(score, idx_seg, start, end, seg)

    if desired_count > 0 and len(selected) < desired_count:
        remaining = [item for item in sorted(processed, key=lambda x: x[2]) if item[1] not in used_indices]
        for score, idx_seg, start, end, seg in remaining:
            if desired_count > 0 and len(selected) >= desired_count:
                break
            if not add_candidate(score, idx_seg, start, end, seg):
                start_adj, end_adj = adjust_window(start, end)
                seg_copy = dict(seg)
                seg_copy['start'] = float(start_adj)
                seg_copy['end'] = float(end_adj)
                seg_copy['score'] = float(score)
                seg_copy['_source_index'] = idx_seg
                selected.append(seg_copy)

    if desired_count > 0 and len(selected) > desired_count:
        selected = sorted(selected, key=lambda x: x.get('score', 0.0), reverse=True)[:desired_count]

    selected.sort(key=lambda x: float(x.get('start', 0.0)))
    for seg in selected:
        seg.pop('_source_index', None)
    return selected


import os
import json
import threading
import shutil
import importlib
import logging
import pickle
import subprocess
from pathlib import Path
import re
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("faiss æ¨¡å—æœªå®‰è£…ï¼Œå°†è·³è¿‡ç›¸å…³å†…å®¹ç´¢å¼•åŠŸèƒ½")
try:
    from PyQt5.QtCore import QThread, pyqtSignal
    PYTQT5_AVAILABLE = True
except ImportError:
    PYTQT5_AVAILABLE = False
    print("PyQt5 æ¨¡å—æœªå®‰è£…ï¼Œå°†è·³è¿‡ç›¸å…³åŠŸèƒ½")

from acfv import config
from acfv.utils import safe_slug
from acfv.runtime.storage import processing_path, settings_path
import sys


def _sanitize_component(text: str) -> str:
    """Sanitize and shorten a filename component for filesystem usage."""
    return safe_slug(text, max_length=80)

# æ¡ä»¶å¯¼å…¥å„ä¸ªæ¨¡å—
try:
    from acfv.processing.extract_chat import extract_chat
    EXTRACT_CHAT_AVAILABLE = True
except ImportError as e:
    EXTRACT_CHAT_AVAILABLE = False
    print(f"extract_chat æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

try:
    from acfv.processing.transcribe_audio import process_audio_segments
    TRANSCRIBE_AUDIO_AVAILABLE = True
except ImportError as e:
    TRANSCRIBE_AUDIO_AVAILABLE = False
    print(f"transcribe_audio æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# å°†analyze_dataçš„å¯¼å…¥ç§»åˆ°å‡½æ•°å†…éƒ¨ï¼Œé¿å…å¾ªç¯å¯¼å…¥
ANALYZE_DATA_AVAILABLE = True

try:
    from utils import filter_meaningless_content, build_content_index
    UTILS_AVAILABLE = True
except ImportError as e:
    UTILS_AVAILABLE = False
    print(f"utils æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

try:
    from acfv.processing.clip_video import clip_video
    CLIP_VIDEO_AVAILABLE = True
except ImportError as e:
    CLIP_VIDEO_AVAILABLE = False
    print(f"clip_video æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

try:
    from acfv.processing.video_emotion_infer import run as infer_emotion
    VIDEO_EMOTION_AVAILABLE = True
except ImportError as e:
    VIDEO_EMOTION_AVAILABLE = False
    print(f"video_emotion_infer æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
import logging.handlers
import os

# åˆ›å»ºlogsç›®å½•
os.makedirs("logs", exist_ok=True)

# é…ç½®æ—¥å¿—å¤„ç†å™¨
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
for handler in logger.handlers[:]:
    logger.removeHandler(handler)

# æ§åˆ¶å°å¤„ç†å™¨
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
console_handler.setFormatter(console_formatter)
logger.addHandler(console_handler)

# æ–‡ä»¶å¤„ç†å™¨ - processing.log
file_handler = logging.handlers.RotatingFileHandler(
    "processing.log", 
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5,
    encoding='utf-8'
)
file_handler.setLevel(logging.INFO)
file_formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
file_handler.setFormatter(file_formatter)
logger.addHandler(file_handler)

# è¯¦ç»†æ—¥å¿—æ–‡ä»¶ - video_processor.log
detailed_handler = logging.handlers.RotatingFileHandler(
    "logs/video_processor.log", 
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5,
    encoding='utf-8'
)
detailed_handler.setLevel(logging.DEBUG)
detailed_formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
detailed_handler.setFormatter(detailed_formatter)
logger.addHandler(detailed_handler)

def log_info(message):
    """è®°å½•ä¿¡æ¯æ—¥å¿—"""
    logging.info(message)
    # ç¡®ä¿æ—¥å¿—ç«‹å³å†™å…¥æ–‡ä»¶
    for handler in logging.getLogger().handlers:
        if isinstance(handler, logging.handlers.RotatingFileHandler):
            handler.flush()


def log_error(message):
    """è®°å½•é”™è¯¯æ—¥å¿—"""
    logging.error(message)
    # ç¡®ä¿æ—¥å¿—ç«‹å³å†™å…¥æ–‡ä»¶
    for handler in logging.getLogger().handlers:
        if isinstance(handler, logging.handlers.RotatingFileHandler):
            handler.flush()

def log_warning(message):
    """è®°å½•è­¦å‘Šæ—¥å¿—"""
    logging.warning(message)
    # ç¡®ä¿æ—¥å¿—ç«‹å³å†™å…¥æ–‡ä»¶
    for handler in logging.getLogger().handlers:
        if isinstance(handler, logging.handlers.RotatingFileHandler):
            handler.flush()


class ConfigManager:
    def __init__(self, config_file=None):
        self.config_file = config_file or str(settings_path("config.json"))
        self.cfg = {
            "VIDEO_FILE": "",
            "CHAT_FILE": "",
            "CHAT_OUTPUT": str(processing_path("chat_with_emotes.json")),
            "TRANSCRIPTION_OUTPUT": str(processing_path("transcription.json")),
            "ANALYSIS_OUTPUT": str(processing_path("high_interest_segments.json")),
            "OUTPUT_CLIPS_DIR": str(processing_path("output_clips")),
            "CLIPS_BASE_DIR": "clips",
            "MAX_CLIP_COUNT": 10,
            "WHISPER_MODEL": "large",
            "LLM_DEVICE": 0,
            "CHAT_DENSITY_WEIGHT": 0.3,
            "CHAT_SENTIMENT_WEIGHT": 0.4,
            "VIDEO_EMOTION_WEIGHT": 0.3,
            "AUDIO_TARGET_BONUS": 1.0,
            "TEXT_TARGET_BONUS": 1.0,
            "INTEREST_SCORE_THRESHOLD": 0.5,
            "LOCAL_EMOTION_MODEL_PATH": "",
            "VIDEO_EMOTION_MODEL_PATH": "",
            "VIDEO_EMOTION_SEGMENT_LENGTH": 4.0,
            "ENABLE_VIDEO_EMOTION": False,
            "twitch_client_id": "",
            "twitch_oauth_token": "",
            "twitch_username": "",
            "twitch_download_folder": "./data/twitch",
        }
        self.load()

    def load(self):
        if not os.path.isfile(self.config_file):
            self.save()
            return
        try:
            with open(self.config_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            self.cfg.update(data)
        except Exception:
            pass

    def save(self):
        try:
            with open(self.config_file, "w", encoding="utf-8") as f:
                json.dump(self.cfg, f, ensure_ascii=False, indent=4)
        except Exception:
            pass

    def get(self, key):
        return self.cfg.get(key)

    def set(self, key, value):
        self.cfg[key] = value


if PYTQT5_AVAILABLE:
    class Worker(QThread):
        finished = pyqtSignal(object)
        error = pyqtSignal(str)
        progress_update = pyqtSignal(str)
        progress_percent = pyqtSignal(int)

        def __init__(self, func, *args, parent=None, **kwargs):
            super().__init__(parent)
            self.func = func
            self.args = args
            self.kwargs = kwargs
            self._should_stop = False
else:
    class Worker:
        def __init__(self, func, *args, parent=None, **kwargs):
            self.func = func
            self.args = args
            self.kwargs = kwargs
            self._should_stop = False

        def run(self):
            try:
                # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦åº”è¯¥åœæ­¢
                if self._should_stop:
                    return
                    
                import inspect
                sig = inspect.signature(self.func)
                if 'progress_callback' in sig.parameters:
                    self.kwargs['progress_callback'] = self.emit_progress
                res = self.func(*self.args, **self.kwargs)
                
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                if not self._should_stop:
                    self.finished.emit(res)
            except Exception as e:
                if not self._should_stop:
                    self.error.emit(str(e))

        def emit_progress(self, stage, current, total, message=""):
            if self._should_stop:
                return
                
            if total > 0:
                percent = int((current / total) * 100)
                self.progress_percent.emit(percent)
            progress_text = f"[{stage}] {current}/{total} - {message}"
            self.progress_update.emit(progress_text)
        
        def stop(self):
            """åœæ­¢çº¿ç¨‹"""
            self._should_stop = True
            self.quit()
            if not self.wait(2000):  # ç­‰å¾…2ç§’
                self.terminate()
                self.wait(1000)


def run_pipeline(cfg_manager, video, chat, has_chat, chat_output, transcription_output,
                 video_emotion_output, analysis_output, output_clips_dir,
                 video_clips_dir, progress_callback=None):
    """è§†é¢‘å¤„ç†ç®¡é“ä¸»å‡½æ•° - æ”¯æŒä¸­æ–­åœæ­¢"""
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    # å…¨å±€åœæ­¢æ ‡å¿—æ£€æŸ¥å‡½æ•°
    def should_stop():
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢å¤„ç†"""
        try:
            stop_flag_file = os.path.join("processing", "stop_flag.txt")
            return os.path.exists(stop_flag_file)
        except Exception:
            return False
    
    def cleanup_stop_flag():
        """æ¸…ç†åœæ­¢æ ‡å¿—æ–‡ä»¶"""
        try:
            stop_flag_file = os.path.join("processing", "stop_flag.txt")
            if os.path.exists(stop_flag_file):
                os.remove(stop_flag_file)
        except Exception:
            pass
    
    # æ¸…ç†ä¹‹å‰çš„åœæ­¢æ ‡å¿—
    cleanup_stop_flag()
    
    def emit_progress(stage, current, total, message=""):
        # æ£€æŸ¥åœæ­¢æ ‡å¿—
        if should_stop():
            logging.info(f"æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢å¤„ç†: {stage}")
            raise InterruptedError("ç”¨æˆ·ä¸­æ–­å¤„ç†")
            
        if progress_callback:
            progress_callback(stage, current, total, message)
        
        # æ›´æ–°è¿›åº¦æ–‡ä»¶ï¼ŒåŒ…å«æ›´è¯¦ç»†çš„ä¿¡æ¯
        try:
            import time
            import json
            
            # è®¡ç®—æ›´å‡†ç¡®çš„è¿›åº¦ç™¾åˆ†æ¯”
            stage_weights = {
                "å¹¶è¡Œæ•°æ®å‡†å¤‡": 0.4,
                "è§†é¢‘æƒ…ç»ªåˆ†æ": 0.15,
                "æ•°æ®å‡†å¤‡": 0.05,
                "æ™ºèƒ½åˆ†æ": 0.2,
                "å¹¶è¡Œè§†é¢‘åˆ‡ç‰‡": 0.15,
                "å®Œæˆ": 0.05
            }
            
            # è·å–å½“å‰é˜¶æ®µæƒé‡
            stage_weight = stage_weights.get(stage, 0.1)
            
            # è®¡ç®—é˜¶æ®µå†…è¿›åº¦
            stage_progress = (current / total) if total > 0 else 0
            
            # è®¡ç®—ç´¯ç§¯è¿›åº¦ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å®é™…æƒ…å†µè°ƒæ•´ï¼‰
            base_progress = sum(stage_weights.get(s, 0) for s in stage_weights.keys() 
                               if s != stage and "å‰é¢å·²å®Œæˆçš„é˜¶æ®µ")
            current_stage_contribution = stage_weight * stage_progress
            total_percentage = (base_progress + current_stage_contribution) * 100
            
            progress_data = {
                "stage": stage,
                "current": current,
                "total": total,
                "message": message,
                "timestamp": time.time(),
                "percentage": min(100, total_percentage),
                "estimated_remaining_minutes": _calculate_smart_remaining_time(total_percentage)
            }
            
            progress_file = processing_path("analysis_progress.json")
            progress_file.parent.mkdir(parents=True, exist_ok=True)
            with progress_file.open('w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except InterruptedError:
            raise  # é‡æ–°æŠ›å‡ºä¸­æ–­å¼‚å¸¸
        except Exception as e:
            logging.error(f"æ›´æ–°è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
    
    def _calculate_smart_remaining_time(percentage):
        """æ™ºèƒ½è®¡ç®—å‰©ä½™æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨æ™ºèƒ½é¢„æµ‹å™¨ï¼Œå¤±è´¥åˆ™æŒ‰ç™¾åˆ†æ¯”ä¼°ç®—"""
        try:
            if 'smart_predictor' in locals() and smart_predictor:
                remain_str = smart_predictor.get_estimated_remaining_time()
                if remain_str:
                    if "å³å°†å®Œæˆ" in remain_str:
                        return 1
                    if "å°æ—¶" in remain_str:
                        try:
                            # å½¢å¦‚ "2å°æ—¶15åˆ†é’Ÿ"
                            parts = remain_str.replace("å°æ—¶", ":").replace("åˆ†é’Ÿ", "").split(":")
                            hours = int(parts[0])
                            minutes = int(parts[1]) if len(parts) > 1 and parts[1] else 0
                            return max(1, hours * 60 + minutes)
                        except Exception:
                            pass
                    if "åˆ†é’Ÿ" in remain_str:
                        try:
                            minutes = int(remain_str.replace("åˆ†é’Ÿ", "").strip())
                            return max(1, minutes)
                        except Exception:
                            pass
                    if "ç§’" in remain_str:
                        try:
                            secs = int(remain_str.replace("ç§’", "").strip())
                            return max(1, (secs + 59) // 60)
                        except Exception:
                            pass
        except Exception:
            pass

        if percentage <= 0:
            return 30
        if percentage >= 100:
            return 0
        remaining_percent = 100 - percentage
        return max(1, int(remaining_percent / 8))
    
    # å¯åŠ¨æ™ºèƒ½è¿›åº¦é¢„æµ‹ (å¯é€šè¿‡ç¯å¢ƒå˜é‡æˆ–é…ç½®ç¦ç”¨)
    predicted_time_info = None
    disable_smart = os.environ.get('DISABLE_SMART_PROGRESS', '0') == '1' or \
        str(cfg_manager.get('DISABLE_SMART_PROGRESS') or '').lower() in ('1','true','yes')
    smart_predictor = None
    if disable_smart:
        log_info("âš™ï¸ å·²æ ¹æ®é…ç½®/ç¯å¢ƒç¦ç”¨æ™ºèƒ½è¿›åº¦é¢„æµ‹ (DISABLE_SMART_PROGRESS=1)")
    try:
        if not disable_smart:
            from .smart_progress_predictor import SmartProgressPredictor
            smart_predictor = SmartProgressPredictor()
        
        # é¢„æµ‹è§†é¢‘å¤„ç†æ—¶é—´
        if os.path.exists(video):
            cmd = ["ffprobe", "-v", "quiet", "-show_entries", "format=duration", "-of", "csv=p=0", video]
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore')
            if result.returncode == 0:
                duration = float(result.stdout.strip())
                size_mb = os.path.getsize(video) / (1024 * 1024)
                predicted_time = smart_predictor.predict_video_processing_time(duration, size_mb)
                predicted_time_info = predicted_time
                log_info(f"ğŸ¯ é¢„æµ‹æ€»å¤„ç†æ—¶é—´: {predicted_time}")
                # å¼€å§‹æ–°çš„é¢„æµ‹ä¼šè¯ï¼Œè®°å½•æ•´ä½“ç”¨æ—¶
                try:
                    smart_predictor.start_session(duration_seconds=duration, size_mb=size_mb, video_path=video)
                except Exception:
                    pass
                
                # é€šè¿‡è¿›åº¦å›è°ƒä¼ é€’é¢„æµ‹æ—¶é—´ä¿¡æ¯
                if progress_callback:
                    progress_callback("é¢„æµ‹æ—¶é—´", 1, 1, f"é¢„è®¡å¤„ç†æ—¶é—´: {predicted_time}")
        
        # å¯åŠ¨å„ä¸ªå¤„ç†é˜¶æ®µ
        if smart_predictor:
            smart_predictor.start_stage("éŸ³é¢‘æå–", 1)
            smart_predictor.start_stage("è¯´è¯äººåˆ†ç¦»", 1)
            smart_predictor.start_stage("éŸ³é¢‘è½¬å½•", 10)
            smart_predictor.start_stage("æƒ…æ„Ÿåˆ†æ", 1)
            smart_predictor.start_stage("åˆ‡ç‰‡ç”Ÿæˆ", 1)
            log_info("âœ… æ™ºèƒ½è¿›åº¦é¢„æµ‹å¯åŠ¨æˆåŠŸ")
        
    except ImportError as e:
        log_info("âš ï¸ æ™ºèƒ½è¿›åº¦é¢„æµ‹æ¨¡å—åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–é¢„æµ‹å™¨")
        # ä½¿ç”¨ç®€åŒ–ç‰ˆé¢„æµ‹å™¨ä½œä¸ºfallback
        try:
            from .smart_progress_predictor import SimplePredictor
            smart_predictor = SimplePredictor() if not disable_smart else None
            
            # ç®€å•é¢„æµ‹å¤„ç†æ—¶é—´
            if os.path.exists(video):
                try:
                    cmd = ["ffprobe", "-v", "quiet", "-show_entries", "format=duration", "-of", "csv=p=0", video]
                    result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore', timeout=10)
                    if result.returncode == 0:
                        duration = float(result.stdout.strip())
                        size_mb = os.path.getsize(video) / (1024 * 1024)
                        predicted_time = smart_predictor.predict_video_processing_time(duration, size_mb)
                        predicted_time_info = predicted_time
                        log_info(f"ğŸ¯ é¢„æµ‹æ€»å¤„ç†æ—¶é—´(ç®€åŒ–): {predicted_time}")
                        
                        # é€šè¿‡è¿›åº¦å›è°ƒä¼ é€’é¢„æµ‹æ—¶é—´ä¿¡æ¯
                        if progress_callback:
                            progress_callback("é¢„æµ‹æ—¶é—´", 1, 1, f"é¢„è®¡å¤„ç†æ—¶é—´: {predicted_time}")
                except Exception:
                    pass
                    
            if smart_predictor:
                log_info("âœ… ä½¿ç”¨ç®€åŒ–è¿›åº¦é¢„æµ‹å™¨")
        except ImportError:
            # å¦‚æœè¿SimplePredictoréƒ½æ— æ³•å¯¼å…¥ï¼Œåˆ›å»ºä¸€ä¸ªæœ€åŸºç¡€çš„æ›¿ä»£
            class BasicPredictor:
                def predict_video_processing_time(self, duration, size_mb):
                    return f"{int(duration/30)}-{int(duration/15)}åˆ†é’Ÿ"
                def start_stage(self, stage_name, weight): pass
                def update_progress(self, stage_name, progress): pass
                def complete_stage(self, stage_name): pass
                def finish_stage(self, stage_name): pass
            smart_predictor = BasicPredictor()
            log_info("âœ… ä½¿ç”¨åŸºç¡€è¿›åº¦é¢„æµ‹å™¨")
        
    except Exception as e:
        log_info(f"âš ï¸ æ™ºèƒ½è¿›åº¦é¢„æµ‹å¯åŠ¨å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€é¢„æµ‹å™¨: {e}")
        # åˆ›å»ºåŸºç¡€é¢„æµ‹å™¨
        if not disable_smart:
            class BasicPredictor:
                def predict_video_processing_time(self, duration, size_mb):
                    return f"{int(duration/30)}-{int(duration/15)}åˆ†é’Ÿ"
                def start_stage(self, stage_name, weight): pass
                def update_progress(self, stage_name, progress): pass
                def complete_stage(self, stage_name): pass
                def finish_stage(self, stage_name): pass
            smart_predictor = BasicPredictor()

    enable_video_emotion = cfg_manager.get("ENABLE_VIDEO_EMOTION")
    log_info(f"[pipeline] è§†é¢‘æƒ…ç»ªåˆ†æå¼€å…³çŠ¶æ€: {enable_video_emotion}")

    has_transcription = os.path.exists(transcription_output) and os.path.getsize(transcription_output) > 10
    has_chat_json = has_chat and os.path.exists(chat_output) and os.path.getsize(chat_output) > 10
    has_video_emotion = enable_video_emotion and os.path.exists(video_emotion_output) and os.path.getsize(video_emotion_output) > 10

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å®Œæ•´å¤„ç†å†…å®¹
    has_analysis = os.path.exists(analysis_output) and os.path.getsize(analysis_output) > 10
    
    # æ£€æŸ¥clipsç›®å½•æ˜¯å¦å­˜åœ¨ä¸”æœ‰å†…å®¹ï¼ˆä½¿ç”¨è¿è¡Œçº§è¾“å‡ºç›®å½•ï¼‰
    clips_dir_exists = os.path.exists(output_clips_dir)
    existing_clips = []
    if clips_dir_exists:
        try:
            for file in os.listdir(output_clips_dir):
                if file.lower().endswith('.mp4'):
                    clip_path = os.path.join(output_clips_dir, file)
                    if os.path.isfile(clip_path) and os.path.getsize(clip_path) > 1024:  # å¤§äº1KB
                        existing_clips.append(file)
        except Exception as e:
            log_error(f"[pipeline] æ£€æŸ¥åˆ‡ç‰‡ç›®å½•å¤±è´¥: {e}")
    
    # æ£€æŸ¥dataç›®å½•æ˜¯å¦å­˜åœ¨ä¸”æœ‰å†…å®¹ï¼ˆåŸºäºè½¬å½•è¾“å‡ºæ‰€åœ¨ç›®å½•ï¼‰
    data_dir = os.path.dirname(transcription_output)
    data_dir_exists = os.path.exists(data_dir)
    has_data_files = False
    if data_dir_exists:
        try:
            data_files = os.listdir(data_dir)
            has_data_files = len(data_files) > 0
            log_info(f"[pipeline] dataç›®å½•åŒ…å« {len(data_files)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            log_error(f"[pipeline] æ£€æŸ¥dataç›®å½•å¤±è´¥: {e}")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å®Œæ•´å¤„ç†å†…å®¹
    has_complete_processing = (
        has_transcription and 
        has_analysis and 
        clips_dir_exists and 
        len(existing_clips) > 0
    )
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    log_info(f"[DEBUG] å®Œæ•´å¤„ç†æ£€æŸ¥:")
    log_info(f"[DEBUG] - has_transcription: {has_transcription} ({transcription_output})")
    log_info(f"[DEBUG] - has_analysis: {has_analysis} ({analysis_output})")
    log_info(f"[DEBUG] - clips_dir_exists: {clips_dir_exists} ({output_clips_dir})")
    log_info(f"[DEBUG] - existing_clips: {len(existing_clips)} ä¸ª")
    log_info(f"[DEBUG] - has_complete_processing: {has_complete_processing}")
    
    # å¦‚æœå·²æœ‰å®Œæ•´å¤„ç†å†…å®¹ï¼Œç›´æ¥è¿”å›
    if has_complete_processing:
        log_info(f"[pipeline] æ£€æµ‹åˆ°å®Œæ•´å¤„ç†å†…å®¹ï¼Œè·³è¿‡å¤„ç†")
        log_info(f"[pipeline] è½¬å½•æ–‡ä»¶: {'âœ…' if has_transcription else 'âŒ'}")
        log_info(f"[pipeline] åˆ†ææ–‡ä»¶: {'âœ…' if has_analysis else 'âŒ'}")
        log_info(f"[pipeline] åˆ‡ç‰‡ç›®å½•: {'âœ…' if clips_dir_exists else 'âŒ'}")
        log_info(f"[pipeline] åˆ‡ç‰‡æ–‡ä»¶: {len(existing_clips)} ä¸ª")
        log_info(f"[pipeline] dataç›®å½•: {'âœ…' if data_dir_exists else 'âŒ'}")
        log_info(f"[pipeline] dataæ–‡ä»¶: {'âœ…' if has_data_files else 'âŒ'}")
        
        # æ›´æ–°UIè¿›åº¦æ˜¾ç¤º
        emit_progress("æ£€æŸ¥ç°æœ‰å†…å®¹", 1, 6, "æ£€æµ‹åˆ°å®Œæ•´å¤„ç†å†…å®¹...")
        emit_progress("è·³è¿‡è½¬å½•", 2, 6, "è½¬å½•æ–‡ä»¶å·²å­˜åœ¨")
        emit_progress("è·³è¿‡åˆ†æ", 3, 6, "åˆ†ææ–‡ä»¶å·²å­˜åœ¨")
        emit_progress("è·³è¿‡åˆ‡ç‰‡", 4, 6, f"å·²æœ‰{len(existing_clips)}ä¸ªåˆ‡ç‰‡æ–‡ä»¶")
        emit_progress("å®Œæˆ", 6, 6, f"ä½¿ç”¨ç°æœ‰å¤„ç†ç»“æœï¼Œå·²æœ‰{len(existing_clips)}ä¸ªåˆ‡ç‰‡")
        
        # æ›´æ–°æ™ºèƒ½è¿›åº¦é¢„æµ‹ - ç«‹å³å®Œæˆæ‰€æœ‰é˜¶æ®µ
        if smart_predictor:
            smart_predictor.finish_stage("éŸ³é¢‘æå–")
            smart_predictor.finish_stage("è¯´è¯äººåˆ†ç¦»")
            smart_predictor.finish_stage("éŸ³é¢‘è½¬å½•")
            smart_predictor.finish_stage("æƒ…æ„Ÿåˆ†æ")
            smart_predictor.finish_stage("åˆ‡ç‰‡ç”Ÿæˆ")
            # å¼ºåˆ¶æ›´æ–°è¿›åº¦æ˜¾ç¤º
            emit_progress("æ£€æŸ¥", 1, 1, "âœ… æ£€æµ‹åˆ°å·²æœ‰å¤„ç†å†…å®¹ï¼Œè·³è¿‡æ‰€æœ‰æ­¥éª¤")
        
        return output_clips_dir, existing_clips, has_chat

    total_steps = 6
    current_step = 0

    # step 1-3: å¹¶è¡Œæ•°æ®å‡†å¤‡
    current_step += 1
    emit_progress("å¹¶è¡Œæ•°æ®å‡†å¤‡", current_step, total_steps, "å¹¶è¡Œå¤„ç†èŠå¤©æå–ã€è½¬å½•ã€æƒ…ç»ªåˆ†æå’Œä¸»æ’­åˆ†ç¦»...")
    
    # æ·»åŠ åœæ­¢æ£€æŸ¥
    if should_stop():
        logging.info("å¤„ç†è¢«ä¸­æ–­ - å¹¶è¡Œæ•°æ®å‡†å¤‡é˜¶æ®µ")
        cleanup_stop_flag()
        return None, None, False
    
    # æ£€æŸ¥å¼ºåˆ¶é‡è½¬å½•
    force_retranscription_value = cfg_manager.get("FORCE_RETRANSCRIPTION", False)
    if isinstance(force_retranscription_value, str):
        force_retranscription = force_retranscription_value
    else:
        force_retranscription = bool(force_retranscription_value)
    
    # æ£€æŸ¥ä¸»æ’­åˆ†ç¦»
    enable_speaker_separation_value = cfg_manager.get("ENABLE_SPEAKER_SEPARATION", False)
    if isinstance(enable_speaker_separation_value, str):
        enable_speaker_separation = enable_speaker_separation_value
    else:
        enable_speaker_separation = bool(enable_speaker_separation_value)
    
    # å¹¶è¡Œæ‰§è¡Œæ•°æ®å‡†å¤‡ä»»åŠ¡
    host_audio_path = None
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {}
        
        # åœæ­¢æ£€æŸ¥
        if should_stop():
            logging.info("å¤„ç†è¢«ä¸­æ–­ - æ•°æ®å‡†å¤‡é˜¶æ®µ")
            cleanup_stop_flag()
            return None, None, False
        
        # èŠå¤©æå–ä»»åŠ¡
        if has_chat and not has_chat_json:
            log_info(f"[pipeline] å¹¶è¡Œæå–èŠå¤©: {chat} -> {chat_output}")
            futures['chat'] = executor.submit(extract_chat, chat, chat_output)
        
        # éŸ³é¢‘æå–ä»»åŠ¡ï¼ˆä¼˜å…ˆæ‰§è¡Œï¼Œç¡®ä¿å®Œæ•´æå–ï¼‰
        audio_save_dir = os.path.join(os.path.dirname(transcription_output), "audio")
        # åªåœ¨çœŸæ­£éœ€è¦æ—¶æ‰åˆ›å»ºç›®å½•
        # os.makedirs(audio_save_dir, exist_ok=True)
        audio_save_path = os.path.join(audio_save_dir, "extracted_audio.wav")
        
        if not os.path.exists(audio_save_path):
            # åœæ­¢æ£€æŸ¥
            if should_stop():
                logging.info("å¤„ç†è¢«ä¸­æ–­ - éŸ³é¢‘æå–å‰")
                cleanup_stop_flag()
                return None, None, False
            
            # åœ¨çœŸæ­£éœ€è¦éŸ³é¢‘æå–æ—¶æ‰åˆ›å»ºç›®å½•
            os.makedirs(audio_save_dir, exist_ok=True)
            
            log_info("[pipeline] å¼€å§‹å®Œæ•´éŸ³é¢‘æå–...")
            emit_progress("éŸ³é¢‘æå–", 1, 3, "æ­£åœ¨ä»è§†é¢‘ä¸­æå–å®Œæ•´éŸ³é¢‘...")
            
            try:
                cmd = [
                    "ffmpeg", "-y",
                    "-hide_banner", "-loglevel", "error", "-nostdin",
                    "-i", video, "-vn", "-acodec", "pcm_s16le",
                    "-ar", "16000", "-ac", "1",
                    "-threads", "0",
                    audio_save_path
                ]
                # æ ¹æ®è§†é¢‘æ—¶é•¿åŠ¨æ€è®¡ç®—è¶…æ—¶æ—¶é—´
                try:
                    probe_cmd = ['ffprobe', '-v', 'quiet', '-print_format', 'json', '-show_format', video]
                    probe_result = subprocess.run(probe_cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore', timeout=30)
                    if probe_result.returncode == 0:
                        import json
                        probe_data = json.loads(probe_result.stdout)
                        video_duration = float(probe_data['format']['duration'])
                        # è¶…æ—¶æ—¶é—´ = è§†é¢‘æ—¶é•¿ * 2 + 300ç§’ç¼“å†²
                        timeout_seconds = min(int(video_duration * 2) + 300, 7200)  # æœ€å¤§2å°æ—¶
                    else:
                        timeout_seconds = 3600  # é»˜è®¤1å°æ—¶
                except:
                    timeout_seconds = 3600  # é»˜è®¤1å°æ—¶
                
                log_info(f"[pipeline] éŸ³é¢‘æå–è¶…æ—¶è®¾ç½®: {timeout_seconds}ç§’")
                result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore', timeout=timeout_seconds)
                
                # å†æ¬¡åœæ­¢æ£€æŸ¥
                if should_stop():
                    logging.info("å¤„ç†è¢«ä¸­æ–­ - éŸ³é¢‘æå–å")
                    cleanup_stop_flag()
                    return None, None, False
                
                # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦æˆåŠŸç”Ÿæˆï¼ˆå³ä½¿FFmpegè¢«ä¸­æ–­ï¼Œæ–‡ä»¶å¯èƒ½å·²ç»ç”Ÿæˆï¼‰
                if os.path.exists(audio_save_path) and os.path.getsize(audio_save_path) > 1024 * 1024:  # å¤§äº1MB
                    file_size_mb = os.path.getsize(audio_save_path) / (1024 * 1024)
                    log_info(f"[pipeline] éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜: {audio_save_path} ({file_size_mb:.1f}MB)")
                    emit_progress("éŸ³é¢‘æå–", 2, 3, f"éŸ³é¢‘æå–å®Œæˆ ({file_size_mb:.1f}MB)")
                elif result.returncode == 0:
                    file_size_mb = os.path.getsize(audio_save_path) / (1024 * 1024)
                    log_info(f"[pipeline] éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜: {audio_save_path} ({file_size_mb:.1f}MB)")
                    emit_progress("éŸ³é¢‘æå–", 2, 3, f"éŸ³é¢‘æå–å®Œæˆ ({file_size_mb:.1f}MB)")
                else:
                    log_error(f"[pipeline] éŸ³é¢‘æ–‡ä»¶ä¿å­˜å¤±è´¥: {result.stderr}")
                    emit_progress("éŸ³é¢‘æå–", 3, 3, "éŸ³é¢‘æå–å¤±è´¥")
                    cleanup_stop_flag()
                    return None, None, False
            except subprocess.TimeoutExpired:
                log_error("[pipeline] éŸ³é¢‘æå–è¶…æ—¶")
                emit_progress("éŸ³é¢‘æå–", 3, 3, "éŸ³é¢‘æå–è¶…æ—¶")
                cleanup_stop_flag()
                return None, None, False
            except InterruptedError:
                log_info("[pipeline] éŸ³é¢‘æå–è¢«ç”¨æˆ·ä¸­æ–­")
                cleanup_stop_flag()
                return None, None, False
            except Exception as e:
                log_error(f"[pipeline] éŸ³é¢‘æ–‡ä»¶ä¿å­˜å¼‚å¸¸: {e}")
                cleanup_stop_flag()
                return None, None, False
                emit_progress("éŸ³é¢‘æå–", 3, 3, f"éŸ³é¢‘æå–å¼‚å¸¸: {e}")
                return None, None, False
        else:
            file_size_mb = os.path.getsize(audio_save_path) / (1024 * 1024)
            log_info(f"[pipeline] éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨: {audio_save_path} ({file_size_mb:.1f}MB)")
            emit_progress("éŸ³é¢‘æå–", 3, 3, f"ä½¿ç”¨ç°æœ‰éŸ³é¢‘æ–‡ä»¶ ({file_size_mb:.1f}MB)")
        
        # è½¬å½•ä»»åŠ¡ï¼ˆä½¿ç”¨æå–çš„éŸ³é¢‘ï¼‰
        if not has_transcription or force_retranscription:
            log_info(f"[pipeline] å¼€å§‹éŸ³é¢‘è½¬å½•: {audio_save_path} -> {transcription_output}")
            whisper_model_name = cfg_manager.get("WHISPER_MODEL", "medium")
            emit_progress("éŸ³é¢‘è½¬å½•", 1, 2, f"ä½¿ç”¨ {whisper_model_name} æ¨¡å‹è¿›è¡Œè½¬å½•...")
            
            futures['transcription'] = executor.submit(
                process_audio_segments,
                audio_path=audio_save_path,  # ä½¿ç”¨æå–çš„éŸ³é¢‘æ–‡ä»¶
                output_file=transcription_output,
                segment_length=cfg_manager.get("SEGMENT_LENGTH", 300),
                whisper_model_name=whisper_model_name
            )
        
        # æƒ…ç»ªåˆ†æä»»åŠ¡
        if enable_video_emotion and not has_video_emotion:
            log_info(f"[pipeline] å¹¶è¡Œæƒ…ç»ªåˆ†æ: {video} -> {video_emotion_output}")
            class EmotionArgs:
                def __init__(self, cfg_manager):
                    self.segment_length = float(cfg_manager.get("VIDEO_EMOTION_SEGMENT_LENGTH") or 4.0)
                    self.model_path = cfg_manager.get("VIDEO_EMOTION_MODEL_PATH") or ""
                    self.device = cfg_manager.get("LLM_DEVICE") or 0
            emotion_args = EmotionArgs(cfg_manager)
            futures['emotion'] = executor.submit(infer_emotion, video, video_emotion_output, emotion_args)
        
        # ä¸»æ’­åˆ†ç¦»ä»»åŠ¡ï¼ˆå¯é€‰ï¼Œå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼‰
        if enable_speaker_separation:
            log_info("[pipeline] å¹¶è¡Œä¸»æ’­éŸ³é¢‘åˆ†ç¦»...")
            try:
                from acfv.processing.speaker_separation_integration import SpeakerSeparationIntegration
                separation_output_dir = os.path.join(os.path.dirname(transcription_output), "speaker_separation")
                speaker_separation = SpeakerSeparationIntegration(cfg_manager)
                speaker_separation.set_progress_callback(emit_progress)
                
                # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…é˜»å¡
                futures['speaker_separation'] = executor.submit(
                    speaker_separation.process_video_with_speaker_separation,
                    video_path=video,
                    output_dir=separation_output_dir
                )
            except Exception as e:
                log_error(f"[pipeline] ä¸»æ’­åˆ†ç¦»ä»»åŠ¡åˆ›å»ºå¤±è´¥: {e}")
                # ä¸é˜»æ­¢æ•´ä¸ªæµç¨‹ç»§ç»­
                pass
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for name, future in futures.items():
            try:
                # ä¸ºè¯´è¯äººåˆ†ç¦»è®¾ç½®å¯é…ç½®çš„è¶…æ—¶æ—¶é—´ï¼Œå› ä¸ºéŸ³é¢‘æ–‡ä»¶å¯èƒ½å¾ˆå¤§
                speaker_timeout = cfg_manager.get("SPEAKER_SEPARATION_TIMEOUT", 1800)
                timeout = speaker_timeout if name == 'speaker_separation' else 1800  # è¯´è¯äººåˆ†ç¦»å¯é…ç½®ï¼Œå…¶ä»–30åˆ†é’Ÿ
                result = future.result(timeout=timeout)
                if name == 'speaker_separation' and result and result.get('host_audio_file'):
                    host_audio_path = result['host_audio_file']
                    log_info(f"[pipeline] ä¸»æ’­éŸ³é¢‘åˆ†ç¦»å®Œæˆ: {host_audio_path}")
                log_info(f"[pipeline] å¹¶è¡Œä»»åŠ¡ {name} å®Œæˆ")
                
                # æ›´æ–°æ™ºèƒ½è¿›åº¦é¢„æµ‹
                if smart_predictor:
                    if name == 'chat':
                        smart_predictor.finish_stage("éŸ³é¢‘æå–")
                    elif name == 'speaker_separation':
                        smart_predictor.finish_stage("è¯´è¯äººåˆ†ç¦»")
                    elif name == 'transcription':
                        smart_predictor.finish_stage("éŸ³é¢‘è½¬å½•")
                    elif name == 'emotion':
                        smart_predictor.finish_stage("æƒ…æ„Ÿåˆ†æ")
                        
            except Exception as e:
                log_error(f"[pipeline] å¹¶è¡Œä»»åŠ¡ {name} å¤±è´¥: {e}")
                # å¯¹äºè¯´è¯äººåˆ†ç¦»å¤±è´¥ï¼Œä¸é˜»æ­¢æ•´ä¸ªæµç¨‹
                if name == 'speaker_separation':
                    log_warning(f"[pipeline] è¯´è¯äººåˆ†ç¦»å¤±è´¥ï¼Œç»§ç»­å…¶ä»–å¤„ç†: {e}")
                    # æ›´æ–°æ™ºèƒ½è¿›åº¦é¢„æµ‹ï¼Œæ ‡è®°è¯´è¯äººåˆ†ç¦»å®Œæˆï¼ˆå³ä½¿å¤±è´¥ï¼‰
                    if smart_predictor:
                        smart_predictor.finish_stage("è¯´è¯äººåˆ†ç¦»")
                else:
                    log_error(f"[pipeline] å…³é”®ä»»åŠ¡ {name} å¤±è´¥ï¼Œå¯èƒ½å½±å“åç»­å¤„ç†")
                                    # æ›´æ–°æ™ºèƒ½è¿›åº¦é¢„æµ‹
                if smart_predictor:
                    if name == 'chat':
                        smart_predictor.finish_stage("éŸ³é¢‘æå–")
                    elif name == 'transcription':
                        smart_predictor.finish_stage("éŸ³é¢‘è½¬å½•")
                    elif name == 'emotion':
                        smart_predictor.finish_stage("æƒ…æ„Ÿåˆ†æ")
                    elif name == 'speaker_separation':
                        smart_predictor.finish_stage("è¯´è¯äººåˆ†ç¦»")
    
    # å¤„ç†æœªå¹¶è¡Œæ‰§è¡Œçš„ä»»åŠ¡
    if has_chat and not has_chat_json and 'chat' not in futures:
        log_info(f"[pipeline] ä¸²è¡Œæå–èŠå¤©: {chat} -> {chat_output}")
        try:
            extract_chat(chat, chat_output)
        except Exception as e:
            log_error(f"[pipeline] èŠå¤©æå–å¤±è´¥: {e}")
    
    if not has_transcription or force_retranscription:
        if 'transcription' not in futures:
            log_info(f"[pipeline] ä¸²è¡Œè½¬å½•: {video} -> {transcription_output}")
            try:
                process_audio_segments(
                    audio_path=video,
                    output_file=transcription_output,
                    segment_length=cfg_manager.get("SEGMENT_LENGTH", 300),
                    whisper_model_name="medium"
                )
                
                # åŒæ—¶ä¿å­˜éŸ³é¢‘æ–‡ä»¶åˆ°clipç›®å½•
                audio_save_dir = os.path.join(os.path.dirname(transcription_output), "audio")
                # åªåœ¨çœŸæ­£éœ€è¦æ—¶æ‰åˆ›å»ºç›®å½•
                # os.makedirs(audio_save_dir, exist_ok=True)
                audio_save_path = os.path.join(audio_save_dir, "extracted_audio.wav")
                
                # æå–å¹¶ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                log_info("[pipeline] ä¿å­˜éŸ³é¢‘æ–‡ä»¶...")
                try:
                    cmd = [
                        "ffmpeg", "-y",
                        "-hide_banner", "-loglevel", "error", "-nostdin",
                        "-i", video, "-vn", "-acodec", "pcm_s16le",
                        "-ar", "16000", "-ac", "1",
                        "-threads", "0",
                        audio_save_path
                    ]
                    result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore', timeout=600)
                    if result.returncode == 0:
                        log_info(f"[pipeline] éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜: {audio_save_path}")
                        emit_progress("éŸ³é¢‘æå–", 1, 1, f"éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜: {os.path.basename(audio_save_path)}")
                    else:
                        log_error(f"[pipeline] éŸ³é¢‘æ–‡ä»¶ä¿å­˜å¤±è´¥: {result.stderr}")
                except Exception as e:
                    log_error(f"[pipeline] éŸ³é¢‘æ–‡ä»¶ä¿å­˜å¼‚å¸¸: {e}")
                    
            except Exception as e:
                log_error(f"[pipeline] è½¬å½•å¤±è´¥: {e}")
    
    if enable_video_emotion and not has_video_emotion:
        if 'emotion' not in futures:
            log_info(f"[pipeline] ä¸²è¡Œæƒ…ç»ªåˆ†æ: {video} -> {video_emotion_output}")
            try:
                class EmotionArgs:
                    def __init__(self, cfg_manager):
                        self.segment_length = float(cfg_manager.get("VIDEO_EMOTION_SEGMENT_LENGTH") or 4.0)
                        self.model_path = cfg_manager.get("VIDEO_EMOTION_MODEL_PATH") or ""
                        self.device = cfg_manager.get("LLM_DEVICE") or 0
                emotion_args = EmotionArgs(cfg_manager)
                infer_emotion(video, video_emotion_output, emotion_args)
            except Exception as e:
                log_error(f"[pipeline] æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
    
    if enable_speaker_separation and 'speaker_separation' not in futures:
        log_info("[pipeline] ä¸²è¡Œä¸»æ’­éŸ³é¢‘åˆ†ç¦»...")
        try:
            from acfv.processing.speaker_separation_integration import SpeakerSeparationIntegration
            separation_output_dir = os.path.join(os.path.dirname(transcription_output), "speaker_separation")
            speaker_separation = SpeakerSeparationIntegration(cfg_manager)
            speaker_separation.set_progress_callback(emit_progress)
            
            separation_result = speaker_separation.process_video_with_speaker_separation(
                video_path=video,
                output_dir=separation_output_dir
            )
            
            if separation_result and separation_result.get('host_audio_file'):
                host_audio_path = separation_result['host_audio_file']
                log_info(f"[pipeline] ä¸»æ’­éŸ³é¢‘åˆ†ç¦»å®Œæˆ: {host_audio_path}")
        except Exception as e:
            log_error(f"[pipeline] ä¸»æ’­åˆ†ç¦»å¤±è´¥: {e}")
            log_warning(f"[pipeline] è¯´è¯äººåˆ†ç¦»å¤±è´¥ï¼Œä½†ä¸ä¼šé˜»æ­¢æ•´ä¸ªå¤„ç†æµç¨‹")
    
    log_info("[pipeline] å¹¶è¡Œæ•°æ®å‡†å¤‡å®Œæˆ")

    # step 3 video emotion
    current_step += 1
    emit_progress("è§†é¢‘æƒ…ç»ªåˆ†æ", current_step, total_steps,
                  "ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†æè§†é¢‘æƒ…ç»ª..." if enable_video_emotion else "è·³è¿‡è§†é¢‘æƒ…ç»ªåˆ†æ...")
    t3 = None
    if enable_video_emotion and not has_video_emotion:
        log_info(f"[pipeline] Processing video emotion inference: {video} -> {video_emotion_output}")
        try:
            class EmotionArgs:
                def __init__(self, cfg_manager):
                    self.segment_length = float(cfg_manager.get("VIDEO_EMOTION_SEGMENT_LENGTH") or 4.0)
                    self.model_path = cfg_manager.get("VIDEO_EMOTION_MODEL_PATH") or ""
                    self.device = cfg_manager.get("LLM_DEVICE") or 0
            emotion_args = EmotionArgs(cfg_manager)
            t3 = threading.Thread(target=infer_emotion, args=(video, video_emotion_output, emotion_args))
            t3.start()
        except Exception as e:
            log_error(f"[pipeline] Error starting video emotion inference: {e}")
            with open(video_emotion_output, "w", encoding="utf-8") as f:
                import json
                json.dump([], f)
    elif not enable_video_emotion:
        log_info(f"[pipeline] Video emotion analysis disabled, creating empty emotion file: {video_emotion_output}")
        with open(video_emotion_output, "w", encoding="utf-8") as f:
            import json
            json.dump([], f)
    else:
        log_info(f"[pipeline] Using existing video emotion file: {video_emotion_output}")

    # ç­‰å¾…t3çº¿ç¨‹å®Œæˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if t3:
        t3.join()

    # step 4 data prep
    current_step += 1
    emit_progress("æ•°æ®å‡†å¤‡", current_step, total_steps, "å‡†å¤‡åˆ†ææ•°æ®...")
    if not has_chat and not os.path.exists(chat_output):
        with open(chat_output, "w", encoding="utf-8") as f:
            import json
            json.dump([], f)
            log_info(f"[pipeline] Created empty chat file: {chat_output}")

    # step 5 analyze
    current_step += 1
    emit_progress("æ™ºèƒ½åˆ†æ", current_step, total_steps, "ä½¿ç”¨AIè¿›è¡Œå†…å®¹å…´è¶£åº¦åˆ†æ...")
    # å®‰å…¨åœ°é‡è½½/å¯¼å…¥ configï¼Œé¿å… "module config not in sys.modules" å¼‚å¸¸
    try:
        if 'config' in sys.modules:
            importlib.reload(sys.modules['config'])
        else:
            importlib.import_module('config')
    except Exception as e:
        log_error(f"[pipeline] config æ¨¡å—é‡è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨ cfg_manager å€¼: {e}")

    # å°†é…ç½®å†™å› config æ¨¡å—ï¼ˆè‹¥å­˜åœ¨ï¼‰ä¾›ä¸‹æ¸¸è¯»å–ï¼›å¤±è´¥åˆ™å¿½ç•¥å¹¶ä¾èµ– cfg_manager
    try:
        cfg_mod = sys.modules.get('config')
        if cfg_mod is not None:
            cfg_mod.CHAT_DENSITY_WEIGHT = cfg_manager.get("CHAT_DENSITY_WEIGHT")
            cfg_mod.CHAT_SENTIMENT_WEIGHT = cfg_manager.get("CHAT_SENTIMENT_WEIGHT")
            cfg_mod.TEXT_TARGET_BONUS = cfg_manager.get("TEXT_TARGET_BONUS")
            cfg_mod.AUDIO_TARGET_BONUS = cfg_manager.get("AUDIO_TARGET_BONUS")
            cfg_mod.CLIPS_BASE_DIR = cfg_manager.get("CLIPS_BASE_DIR")
            cfg_mod.OUTPUT_CLIPS_DIR = output_clips_dir
    except Exception as e:
        log_error(f"[pipeline] å›å†™ config æ¨¡å—é…ç½®å¤±è´¥ï¼ˆå°†ç›´æ¥ä½¿ç”¨ cfg_managerï¼‰: {e}")
    max_clips = int(cfg_manager.get("MAX_CLIP_COUNT") or 0)
    video_emotion_weight = float(cfg_manager.get("VIDEO_EMOTION_WEIGHT") or 0.3) if enable_video_emotion else 0.0
    log_info(f"[pipeline] Analysis configuration: max_clips={max_clips}, video_emotion_weight={video_emotion_weight}, enable_video_emotion={enable_video_emotion}")
    segments_data = []
    analysis_success = False
    try:
        import inspect
        # å°è¯•ä» processing.analyze_data å¯¼å…¥å‡½æ•°
        analyze_params = []
        _analyze_func = None
        try:
            from acfv.processing.analyze_data import analyze_data as _analyze_func  # å…¼å®¹æ—§æ¥å£
        except ImportError:
            try:
                from acfv.processing.analyze_data import analyze_data_with_checkpoint as _analyze_func
            except ImportError:
                _analyze_func = None
        if _analyze_func is not None:
            analyze_sig = inspect.signature(_analyze_func)
            analyze_params = list(analyze_sig.parameters.keys())
            log_info(f"[pipeline] analyze_data function parameters: {analyze_params}")
        else:
            log_warning("[pipeline] æœªæ‰¾åˆ° processing.analyze_data ä¸­çš„åˆ†æå‡½æ•°ï¼Œåç»­å°†ç›´æ¥å›é€€")
        analyze_kwargs = {
            'chat_file': chat_output,
            'transcription_file': transcription_output,
            'output_file': analysis_output
        }
        if 'progress_callback' in analyze_params:
            analyze_kwargs['progress_callback'] = emit_progress
        if 'enable_video_emotion' in analyze_params:
            analyze_kwargs.update({
                'video_emotion_file': video_emotion_output,
                'video_emotion_weight': video_emotion_weight,
                'top_n': max_clips if max_clips > 0 else 9999,
                'enable_video_emotion': enable_video_emotion,
                'device': 'cuda:0'
            })
        elif 'video_emotion_file' in analyze_params and 'video_emotion_weight' in analyze_params:
            analyze_kwargs.update({
                'video_emotion_file': video_emotion_output,
                'video_emotion_weight': video_emotion_weight,
                'top_n': max_clips if max_clips > 0 else 9999
            })
        elif 'top_n' in analyze_params:
            analyze_kwargs['top_n'] = max_clips if max_clips > 0 else 9999
        
        # è¯­ä¹‰è‡ªé€‚åº”åˆ†æ
        log_info("[pipeline] ä½¿ç”¨è¯­ä¹‰è‡ªé€‚åº”åˆ†ææ¨¡å¼")
        if _analyze_func is not None:
            segments_data = _analyze_func(**analyze_kwargs)
        else:
            try:
                from acfv.processing.analyze_data import analyze_data_with_checkpoint as _fallback_analyze
                segments_data = _fallback_analyze(**analyze_kwargs)
            except Exception as _ie:
                log_warning(f"[pipeline] æ— æ³•å¯¼å…¥ processing.analyze_data: {_ie}; ä½¿ç”¨ç©ºç»“æœå›é€€")
                segments_data = []
        if segments_data:
            analysis_success = True
            log_info("[pipeline] Analysis completed successfully")
            
            # æ›´æ–°æ™ºèƒ½è¿›åº¦é¢„æµ‹
            if smart_predictor:
                smart_predictor.finish_stage("æƒ…æ„Ÿåˆ†æ")
            # âœ… é¢å¤–æ ¡éªŒï¼šç¡®è®¤åˆ†æè¾“å‡ºæ–‡ä»¶æ˜¯å¦çœŸæ­£å†™å‡º
            try:
                if not os.path.exists(analysis_output) or os.path.getsize(analysis_output) < 50:
                    log_warning(f"[pipeline][diagnostic] åˆ†æå‡½æ•°è¿”å›äº† {len(segments_data)} ä¸ªç‰‡æ®µï¼Œä½†æœªæ£€æµ‹åˆ°æœ‰æ•ˆåˆ†æè¾“å‡ºæ–‡ä»¶: {analysis_output}ï¼Œå°è¯•è¡¥å†™â€¦")
                    try:
                        os.makedirs(os.path.dirname(analysis_output), exist_ok=True)
                        with open(analysis_output, 'w', encoding='utf-8') as _af:
                            json.dump(segments_data, _af, ensure_ascii=False, indent=2)
                        log_info("[pipeline][diagnostic] å·²è¡¥å†™ analysis_output æ–‡ä»¶")
                    except Exception as _we:
                        log_error(f"[pipeline][diagnostic] è¡¥å†™ analysis_output å¤±è´¥: {_we}")
                else:
                    log_info(f"[pipeline][diagnostic] æ£€æµ‹åˆ°åˆ†æè¾“å‡ºæ–‡ä»¶: {analysis_output} ({os.path.getsize(analysis_output)} bytes)")
            except Exception as _ce:
                log_warning(f"[pipeline][diagnostic] åˆ†æè¾“å‡ºæ–‡ä»¶æ ¡éªŒå¤±è´¥: {_ce}")
            
            # âœ… è¯„ä¼°scoreåˆ†å¸ƒï¼Œè¾…åŠ©å‘ç°å…¨0é—®é¢˜
            try:
                scores = [float(s.get('score', 0) or 0) for s in segments_data if isinstance(s, dict)]
                if scores:
                    mx = max(scores); mn = min(scores); avg = sum(scores)/len(scores)
                    non_zero = sum(1 for v in scores if v > 0)
                    log_info(f"[pipeline][diagnostic] è¯„åˆ†ç»Ÿè®¡: count={len(scores)}, non_zero={non_zero}, min={mn:.4f}, max={mx:.4f}, avg={avg:.4f}")
                    if mx <= 0.05:
                        log_warning("[pipeline][diagnostic] æ£€æµ‹åˆ°æ‰€æœ‰è¯„åˆ†éå¸¸ä½ (max <= 0.05)ï¼Œå¯èƒ½èŠå¤©/æ–‡æœ¬/æƒé‡å…¨éƒ¨ä¸º0 æˆ–è¢«æ‹†åˆ†ç¨€é‡Š")
                else:
                    log_warning("[pipeline][diagnostic] åˆ†æè¿”å›çš„ç‰‡æ®µç¼ºå°‘ score å­—æ®µ")
            except Exception as _se:
                log_warning(f"[pipeline][diagnostic] è¯„åˆ†ç»Ÿè®¡å¤±è´¥: {_se}")
                
    except Exception as e:
        log_error(f"[pipeline] Analysis failed: {e}")
        analysis_success = False

    current_step += 1
    emit_progress("å¹¶è¡Œè§†é¢‘åˆ‡ç‰‡", current_step, total_steps, "å¹¶è¡Œç”Ÿæˆè§†é¢‘åˆ‡ç‰‡æ–‡ä»¶...")

    if not analysis_success:
        if os.path.exists(analysis_output) and os.path.getsize(analysis_output) > 10:
            log_info(f"[pipeline] Reading analysis result: {analysis_output}")
            try:
                with open(analysis_output, "r", encoding="utf-8") as f:
                    segments_data = json.load(f)
                log_info(f"[pipeline] Found {len(segments_data)} segments in analysis result")
                # ç¡®ä¿å…ˆæŒ‰è¯„åˆ†æ’åºå†é™åˆ¶æ•°é‡ï¼Œé¿å…æŒ‰æ—¶é—´æˆªå–
                try:
                    segments_data = sorted(segments_data, key=lambda x: x.get('score', 0), reverse=True)
                except Exception:
                    pass
                if max_clips > 0 and len(segments_data) > max_clips:
                    original_count = len(segments_data)
                    segments_data = segments_data[:max_clips]
                    log_info(f"[pipeline] Limited segments from {original_count} to {len(segments_data)} based on MAX_CLIP_COUNT={max_clips}")
                # Fallback æƒ…å†µä¸‹åŒæ ·ç»™å‡ºè¯„åˆ†åˆ†å¸ƒè¯Šæ–­
                try:
                    scores = [float(s.get('score', 0) or 0) for s in segments_data if isinstance(s, dict)]
                    if scores:
                        mx = max(scores); mn = min(scores); avg = sum(scores)/len(scores)
                        non_zero = sum(1 for v in scores if v > 0)
                        log_info(f"[pipeline][diagnostic][fallback] è¯„åˆ†ç»Ÿè®¡: count={len(scores)}, non_zero={non_zero}, min={mn:.4f}, max={mx:.4f}, avg={avg:.4f}")
                        if mx <= 0.05:
                            log_warning("[pipeline][diagnostic][fallback] åˆ†æç»“æœè¯„åˆ†å…¨éƒ¨æä½æˆ–ä¸º0ï¼Œå¯èƒ½ upstream æœªå†™å…¥æœ‰æ•ˆè¯„åˆ†")
                except Exception:
                    pass
            except Exception as e:
                log_error(f"[pipeline] Error reading analysis result: {e}")
                segments_data = []
        else:
            log_warning("[pipeline][diagnostic] åˆ†æå¤±è´¥ä¸”æœªæ‰¾åˆ°å¯ç”¨çš„åˆ†æè¾“å‡ºæ–‡ä»¶ï¼Œåç»­æ­¥éª¤å°†ä½¿ç”¨ç©ºç‰‡æ®µåˆ—è¡¨")

    # åˆ‡ç‰‡å‰ç®€å•æ£€æµ‹æ˜¯å¦å­˜åœ¨æ–°çš„è¯„åˆ†ï¼ˆä»…æ—¥å¿—æç¤ºï¼‰
    try:
        run_dir = os.path.dirname(analysis_output)
        ratings_log_path = os.path.join(run_dir, 'acfv_ratings.jsonl')
        if os.path.exists(ratings_log_path) and os.path.getsize(ratings_log_path) > 0:
            with open(ratings_log_path, 'r', encoding='utf-8') as f:
                ratings_lines = sum(1 for _ in f)
            log_info(f"[pipeline][RAG] æ£€æµ‹åˆ°è¯„åˆ†è®°å½• {ratings_lines} æ¡ï¼ˆä»…æ£€æµ‹ï¼Œä¸è¿›è¡ŒRAGå¤„ç†ï¼‰")
        else:
            log_info("[pipeline][RAG] æœªæ£€æµ‹åˆ°è¯„åˆ†è®°å½•æˆ–è¯„åˆ†æ–‡ä»¶ä¸ºç©º")
    except Exception as e:
        log_warning(f"[pipeline][RAG] è¯„åˆ†æ£€æµ‹å¤±è´¥: {e}")

    # äºŒæ¬¡ä¿éšœï¼ˆå¯é€‰ï¼‰ï¼šä» ratings.json é‡å»ºç‰‡æ®µé¡ºåº
    try:
        prefer_from_ratings = False
        try:
            prefer_from_ratings = bool(cfg_manager.get("PREFER_RATINGS_JSON", False))
        except Exception:
            prefer_from_ratings = False
        if prefer_from_ratings and not use_semantic_segment_mode:
            ratings_path = os.path.join(os.path.dirname(analysis_output), "ratings.json")
            video_dir = os.path.dirname(video)
            latest_dir = os.path.join(video_dir, "runs", "latest")
            candidate_latest = os.path.join(latest_dir, "ratings.json")
            candidate_video = os.path.join(video_dir, "ratings.json")
            if not os.path.exists(ratings_path):
                if os.path.exists(candidate_latest):
                    ratings_path = candidate_latest
                elif os.path.exists(candidate_video):
                    ratings_path = candidate_video
            if os.path.exists(ratings_path):
                with open(ratings_path, 'r', encoding='utf-8') as f:
                    ratings_data = json.load(f)
                rated_segments = []
                for clip_name, data in ratings_data.items():
                    try:
                        rated_segments.append({
                            'start': float(data.get('start', 0.0)),
                            'end': float(data.get('end', 0.0)),
                            'score': float(data.get('rating', 0.0)),
                            'text': data.get('text', ''),
                            'source': 'ratings.json'
                        })
                    except Exception:
                        continue
                if rated_segments:
                    rated_segments.sort(key=lambda x: x.get('score', 0.0), reverse=True)
                    if max_clips > 0 and len(rated_segments) > max_clips:
                        rated_segments = rated_segments[:max_clips]
                    segments_data = rated_segments
                    log_info(f"[pipeline] é‡‡ç”¨ ratings.json è¯„åˆ†é‡å»ºç‰‡æ®µé¡ºåºï¼Œå…± {len(segments_data)} ä¸ª")
        else:
            log_info("[pipeline] å·²ç¦ç”¨ä» ratings.json é‡å»ºç‰‡æ®µï¼ˆPREFER_RATINGS_JSON=Falseï¼‰")
    except Exception as e:
        log_warning(f"[pipeline] ä½¿ç”¨ ratings.json é‡æ’å¤±è´¥: {e}")

    log_info(f"[pipeline] Final segments count: {len(segments_data)}")

    def _has_ranking_signals(items):
        try:
            for seg in items:
                if not isinstance(seg, dict):
                    continue
                score = seg.get("score")
                if score is not None:
                    try:
                        if float(score) > 0:
                            return True
                    except Exception:
                        return True
                source = str(seg.get("source", "")).lower()
                if source in {"ratings.json", "manual", "acfv_ratings", "manual_rating"}:
                    return True
            return False
        except Exception:
            return False

    ranked_segments_detected = bool(segments_data) and _has_ranking_signals(segments_data)

    # è¯­ä¹‰åˆ†æ®µæ¨¡å¼ï¼ˆä»å¤´åˆ°å°¾æŒ‰è¯­ä¹‰è¿ç»­åˆ†æ®µï¼Œç›®æ ‡çº¦4åˆ†é’Ÿï¼Œé¿å…è¿‡çŸ­ï¼‰
    try:
        val = cfg_manager.get("SEMANTIC_SEGMENT_MODE")
        # é»˜è®¤å¼€å¯è¯­ä¹‰åˆ†æ®µæ¨¡å¼ï¼ˆç”¨æˆ·æœŸæœ›"ä»ä¸€å¼€å§‹å°±æŒ‰è¯­ä¹‰åˆ‡å—"ï¼‰
        use_semantic_segment_mode = bool(val) if val is not None else True
    except Exception:
        use_semantic_segment_mode = True

    if use_semantic_segment_mode and ranked_segments_detected:
        try:
            raw_force_semantic = cfg_manager.get("FORCE_SEMANTIC_SEGMENT")
        except Exception:
            raw_force_semantic = None
        force_semantic = bool(raw_force_semantic) if raw_force_semantic is not None else False
        if not force_semantic:
            log_info("[pipeline] æ£€æµ‹åˆ°è¯„åˆ†é©±åŠ¨çš„ç‰‡æ®µæ’åºï¼Œä¼˜å…ˆä¿ç•™è¯„åˆ†ç»“æœï¼Œè·³è¿‡è¯­ä¹‰åˆ†æ®µã€‚å¦‚éœ€å¼ºåˆ¶è¯­ä¹‰åˆ‡å—ï¼Œè¯·å¼€å¯ FORCE_SEMANTIC_SEGMENTã€‚")
            use_semantic_segment_mode = False
        else:
            log_info("[pipeline] FORCE_SEMANTIC_SEGMENT å·²å¯ç”¨ï¼Œæ£€æµ‹åˆ°è¯„åˆ†ä¹Ÿç»§ç»­æ‰§è¡Œè¯­ä¹‰åˆ†æ®µã€‚")

    if use_semantic_segment_mode:
        log_info("[pipeline] å¯ç”¨è¯­ä¹‰åˆ†æ®µæ¨¡å¼ï¼šä»å¤´æŒ‰è¯­ä¹‰è¿ç»­åˆ‡åˆ†ï¼ˆçº¦4åˆ†é’Ÿï¼‰")
        try:
            # åŠ è½½å®Œæ•´è½¬å½•ä½œä¸ºåˆ†æ®µä¾æ®
            if os.path.exists(transcription_output):
                with open(transcription_output, 'r', encoding='utf-8') as f:
                    transcription_data = json.load(f)
            else:
                transcription_data = []
            # å‚æ•°
            target_sec = float(cfg_manager.get("SEMANTIC_TARGET_DURATION") or 240.0)
            min_sec = float(cfg_manager.get("MIN_CLIP_DURATION") or max(60.0, target_sec * 0.6))
            max_sec = float(cfg_manager.get("MAX_CLIP_DURATION") or min(target_sec * 1.6, 600.0))
            sim_threshold = float(cfg_manager.get("SEMANTIC_SIMILARITY_THRESHOLD") or 0.75)
            max_gap = float(cfg_manager.get("SEMANTIC_MAX_TIME_GAP") or 60.0)

            # é¢„å¤„ç†è½¬å½•ç‰‡æ®µ
            segs = []
            for seg in transcription_data:
                try:
                    s = float(seg.get('start', 0.0)); e = float(seg.get('end', 0.0))
                    txt = seg.get('text', '') or ''
                    if e > s and txt.strip():
                        segs.append({'start': s, 'end': e, 'text': txt})
                except Exception:
                    continue
            segs.sort(key=lambda x: x['start'])

            # å‘é‡åŒ–ï¼ˆTF-IDFä¼˜å…ˆï¼›å¤±è´¥åˆ™BOWï¼‰
            try:
                from sklearn.feature_extraction.text import TfidfVectorizer
                from sklearn.metrics.pairwise import cosine_similarity
                texts = [s['text'] for s in segs]
                vectorizer = TfidfVectorizer(max_features=5000)
                mat = vectorizer.fit_transform(texts) if texts else None
                def cosine(i, j):
                    try:
                        return float(cosine_similarity(mat[i], mat[j])[0][0]) if mat is not None else 1.0
                    except Exception:
                        return 1.0
            except Exception:
                # é€€åŒ–ä¸ºç®€å•è¯è¢‹ä½™å¼¦
                def to_bow(t):
                    import re
                    toks = re.findall(r"\w+", (t or '').lower())
                    from collections import Counter
                    return Counter(toks)
                bows = [to_bow(s['text']) for s in segs]
                import math
                def cosine(i, j):
                    a, b = bows[i], bows[j]
                    if not a or not b:
                        return 0.0
                    keys = set(a) | set(b)
                    dot = sum(a.get(k,0) * b.get(k,0) for k in keys)
                    na = math.sqrt(sum(v*v for v in a.values())); nb = math.sqrt(sum(v*v for v in b.values()))
                    return (dot / (na*nb)) if na>0 and nb>0 else 0.0

            # é¡ºåºåˆå¹¶ä¸ºè¯­ä¹‰å—
            def _avg(values):
                return sum(values) / len(values) if values else None

            def _aggregate_segment_scores(group, group_start, group_end):
                scores = []
                interest_scores = []
                densities = []
                rag_priors = []
                volumes = []
                for seg in group:
                    try:
                        if seg.get("score") is not None:
                            scores.append(float(seg.get("score")))
                    except Exception:
                        pass
                    try:
                        if seg.get("interest_score") is not None:
                            interest_scores.append(float(seg.get("interest_score")))
                    except Exception:
                        pass
                    try:
                        if seg.get("density") is not None:
                            densities.append(float(seg.get("density")))
                    except Exception:
                        pass
                    try:
                        if seg.get("rag_prior") is not None:
                            rag_priors.append(float(seg.get("rag_prior")))
                    except Exception:
                        pass
                    try:
                        if seg.get("volume_penalty") is not None:
                            volumes.append(float(seg.get("volume_penalty")))
                    except Exception:
                        pass
                payload = {}
                avg_interest = _avg(scores or interest_scores)
                if avg_interest is not None:
                    payload["score"] = avg_interest
                    payload["interest_score"] = avg_interest
                avg_density = _avg(densities)
                if avg_density is not None:
                    payload["density"] = avg_density
                avg_rag = _avg(rag_priors)
                if avg_rag is not None:
                    payload["rag_prior"] = avg_rag
                avg_volume = _avg(volumes)
                if avg_volume is not None:
                    payload["volume_penalty"] = avg_volume
                if "score" not in payload:
                    duration_score = max(group_end - group_start, 0.005)
                    payload["score"] = duration_score
                    payload["interest_score"] = duration_score
                return payload

            semantic_segments = []
            cur_start = None; cur_end = None; cur_last_idx = None; cur_texts = []; cur_segments = []
            for idx, seg in enumerate(segs):
                s = seg['start']; e = seg['end']
                if cur_start is None:
                    cur_start, cur_end, cur_last_idx, cur_texts = s, e, idx, [seg['text']]
                    cur_segments = [seg]
                    continue
                gap = s - cur_end
                similar = True
                try:
                    similar = cosine(cur_last_idx, idx) >= sim_threshold
                except Exception:
                    similar = True
                new_dur = max(cur_end, e) - cur_start
                # æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶åˆ™åˆ‡å—ï¼š
                # 1) é—´éš”è¿‡å¤§ï¼›2) è¾¾åˆ°ä¸Šé™ï¼›3) å·²æ¥è¿‘ç›®æ ‡ä¸”ç›¸ä¼¼åº¦ä¸è¶³
                if (gap > max_gap) or (new_dur >= max_sec) or ((new_dur >= target_sec) and (not similar)):
                    # è‹¥å½“å‰å—è¿‡çŸ­ï¼Œå°½é‡å¹¶å…¥
                    if (cur_end - cur_start) < min_sec and (new_dur <= max_sec):
                        cur_end = max(cur_end, e)
                        cur_last_idx = idx
                        cur_texts.append(seg['text'])
                        cur_segments.append(seg)
                    else:
                        merged = {'start': cur_start, 'end': cur_end, 'text': ' '.join(cur_texts)}
                        merged.update(_aggregate_segment_scores(cur_segments, cur_start, cur_end))
                        semantic_segments.append(merged)
                        cur_start, cur_end, cur_last_idx, cur_texts = s, e, idx, [seg['text']]
                        cur_segments = [seg]
                else:
                    cur_end = max(cur_end, e)
                    cur_last_idx = idx
                    cur_texts.append(seg['text'])
                    cur_segments.append(seg)
            if cur_start is not None:
                merged = {'start': cur_start, 'end': cur_end, 'text': ' '.join(cur_texts)}
                merged.update(_aggregate_segment_scores(cur_segments, cur_start, cur_end))
                semantic_segments.append(merged)

            # è¦†ç›– segments_dataï¼ˆé¡ºåºè¾“å‡ºï¼Œä¸å†æŒ‰åˆ†æ•°é‡æ’ï¼‰
            segments_data = semantic_segments
            log_info(f"[pipeline] è¯­ä¹‰åˆ†æ®µå®Œæˆï¼Œå…± {len(segments_data)} æ®µï¼ˆç›®æ ‡â‰ˆ{target_sec:.0f}sï¼‰")

            # ä¿è¯è¾“å‡ºæ°å¥½ N æ®µä¸”ä¸é‡å ï¼ˆä¸è¶³åˆ™æŒ‰è½¬å½•è¾¹ç•Œæ‹†åˆ†æœ€é•¿æ®µï¼Œè¶…å‡ºåˆ™è£å‰ªï¼‰
            try:
                desired_count = int(cfg_manager.get("MAX_CLIP_COUNT") or 10)
                if desired_count <= 0:
                    desired_count = 10
            except Exception:
                desired_count = 10

            # æ ¹æ®è½¬å½•è¾¹ç•Œåœ¨ç‰‡æ®µå†…éƒ¨å¯»æ‰¾æœ€ä¼˜æ‹†åˆ†ç‚¹
            def _find_split_time_within(seg_start: float, seg_end: float, transcription_list, prefer_time: float,
                                        min_side: float) -> float:
                try:
                    candidates = []
                    for t in transcription_list:
                        try:
                            ts = float(t.get('start', 0.0)); te = float(t.get('end', 0.0))
                        except Exception:
                            continue
                        if ts <= seg_start or te >= seg_end:
                            continue
                        # é€‰ç”¨å¥å­è¾¹ç•Œçš„ä¸­ç‚¹ä½œä¸ºå€™é€‰ï¼Œä»¥åå‘è‡ªç„¶åœé¡¿
                        mid = (ts + te) / 2.0
                        # ä¸¤ä¾§éœ€ä¿ç•™æœ€å°é•¿åº¦
                        if (mid - seg_start) >= min_side and (seg_end - mid) >= min_side:
                            candidates.append(mid)
                    if not candidates:
                        return 0.0
                    # é€‰æ‹©æœ€æ¥è¿‘æœŸæœ›æ—¶é—´ç‚¹ï¼ˆé€šå¸¸ä¸ºä¸­ç‚¹ï¼‰çš„è¾¹ç•Œ
                    best = min(candidates, key=lambda x: abs(x - prefer_time))
                    return float(best)
                except Exception:
                    return 0.0

            def _split_longest_until_exact(segments, target_n: int, transcription_list, min_len: float, video_len: float):
                # å…è®¸çš„æœ€ä½æ‹†åˆ†å­ç‰‡æ®µé•¿åº¦ï¼ˆåœ¨min_lenåŸºç¡€ä¸Šé€‚åº¦æ”¾å®½ï¼‰
                min_child = max(min_len * 0.75, 30.0)
                safety_counter = 0
                while len(segments) < target_n and safety_counter < 200:
                    safety_counter += 1
                    # é€‰å¯æ‹†åˆ†çš„æœ€é•¿ç‰‡æ®µ
                    idx = -1
                    max_dur = -1.0
                    for i, s in enumerate(segments):
                        try:
                            ds = float(s.get('start', 0.0)); de = float(s.get('end', 0.0))
                        except Exception:
                            continue
                        dur = max(0.0, de - ds)
                        # è‡³å°‘èƒ½æ‹†æˆä¸¤ä¸ªä¸å°äº min_child çš„å­æ®µ
                        if dur >= (2.0 * min_child) and dur > max_dur:
                            max_dur = dur
                            idx = i
                    if idx < 0:
                        break  # æ²¡æœ‰å¯æ‹†åˆ†çš„ç‰‡æ®µ

                    base = segments[idx]
                    s0 = float(base.get('start', 0.0)); e0 = float(base.get('end', 0.0))
                    mid_pref = (s0 + e0) / 2.0
                    split_t = _find_split_time_within(s0, e0, transcription_list, mid_pref, min_child)
                    if split_t <= 0.0:
                        # æ²¡æœ‰åˆé€‚çš„è½¬å½•è¾¹ç•Œï¼Œä½¿ç”¨ä¸­ç‚¹ä½†éµå®ˆæœ€å°é•¿åº¦
                        left = max(s0, min(mid_pref, e0 - min_child))
                        right = min(e0, max(mid_pref, s0 + min_child))
                        split_t = (left + right) / 2.0
                    # æ„é€ ä¸¤ä¸ªæ–°ç‰‡æ®µ
                    left_seg = dict(base)
                    right_seg = dict(base)
                    left_seg['start'] = float(s0)
                    left_seg['end'] = float(split_t)
                    right_seg['start'] = float(split_t)
                    right_seg['end'] = float(e0)
                    # æ ¡éªŒé•¿åº¦
                    if (left_seg['end'] - left_seg['start']) < min_child or (right_seg['end'] - right_seg['start']) < min_child:
                        # æ— æ³•æ»¡è¶³æœ€å°é•¿åº¦ï¼Œæ”¾å¼ƒæœ¬æ¬¡æ‹†åˆ†
                        break
                    # æ›¿æ¢å¹¶ä¿æŒæ—¶é—´é¡ºåº
                    segments.pop(idx)
                    segments.insert(idx, right_seg)
                    segments.insert(idx, left_seg)
                    segments.sort(key=lambda x: float(x.get('start', 0.0)))
                return segments

            # è£å‰ªæˆ–æ‹†åˆ†ï¼Œä¿è¯æ°å¥½ desired_count æ®µ
            try:
                # å…ˆåˆè§„æ’åº
                segments_data = sorted(segments_data, key=lambda x: float(x.get('start', 0.0)))
                if len(segments_data) > desired_count:
                    # è‹¥æ— scoreå­—æ®µï¼Œåˆ™ç›´æ¥å–æ—¶é—´é¡ºåºå‰Næ®µ
                    try:
                        segments_data = sorted(segments_data, key=lambda x: x.get('score', 0.0), reverse=True)[:desired_count]
                        segments_data = sorted(segments_data, key=lambda x: float(x.get('start', 0.0)))
                    except Exception:
                        segments_data = segments_data[:desired_count]
                elif len(segments_data) < desired_count:
                    # æ‹†åˆ†æœ€é•¿æ®µç›´è‡³è¾¾åˆ°Næ®µ
                    segments_data = _split_longest_until_exact(segments_data, desired_count, transcription_data, min_sec, 0.0)
                # å†æ¬¡ç¡®è®¤æ•°é‡
                if len(segments_data) != desired_count:
                    log_warning(f"[pipeline] æ— æ³•ä¸¥æ ¼è¾¾åˆ° {desired_count} æ®µï¼Œå½“å‰ {len(segments_data)} æ®µï¼ˆå·²å°½æœ€å¤§åŠªåŠ›ï¼‰")
                # æœ€ç»ˆç¡®ä¿ä¸é‡å ï¼ˆé¡ºåºå‹ç´§ï¼šæ¯æ®µç»“æŸä¸è¶…è¿‡ä¸‹ä¸€æ®µå¼€å§‹ï¼‰
                segments_data = sorted(segments_data, key=lambda x: float(x.get('start', 0.0)))
                for i in range(len(segments_data) - 1):
                    try:
                        if float(segments_data[i]['end']) > float(segments_data[i+1]['start']):
                            segments_data[i]['end'] = float(segments_data[i+1]['start'])
                    except Exception:
                        pass
            except Exception as _e:
                log_warning(f"[pipeline] è°ƒæ•´ä¸ºæ°å¥½Næ®µå¤±è´¥ï¼Œå°†ä½¿ç”¨è¯­ä¹‰åˆ†æ®µåŸå§‹ç»“æœ: {_e}")
        except Exception as e:
            log_warning(f"[pipeline] è¯­ä¹‰åˆ†æ®µæ¨¡å¼å¤±è´¥ï¼Œå›é€€åˆ°åˆ†æç»“æœ: {e}")
    
    # åº”ç”¨åˆ‡ç‰‡æ—¶é•¿æ‰©å±•é€»è¾‘
    if segments_data:
        # è¯­ä¹‰å¯å˜æ—¶é•¿ï¼šä½¿ç”¨é…ç½®æˆ–é»˜è®¤å€¼
        if use_semantic_segment_mode:
            # åœ¨è¯­ä¹‰åˆ†æ®µæ¨¡å¼ä¸‹ï¼Œå›ºå®šå¼ºçº¦æŸï¼Œé¿å…å¤–éƒ¨æŠŠminè®¾åˆ°300s
            target_sec = float(cfg_manager.get("SEMANTIC_TARGET_DURATION") or 240.0)
            min_clip_duration = max(150.0, target_sec * 0.6)
            context_extend = float(cfg_manager.get("CLIP_CONTEXT_EXTEND") or 0.0)
        else:
            min_clip_duration = float(cfg_manager.get("MIN_CLIP_DURATION") or 60.0)
            context_extend = float(cfg_manager.get("CLIP_CONTEXT_EXTEND") or 0.0)
        
        # è·å–è§†é¢‘æ€»æ—¶é•¿
        try:
            probe_cmd = ['ffprobe', '-v', 'quiet', '-print_format', 'json', '-show_format', video]
            probe_result = subprocess.run(probe_cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore', timeout=30)
            if probe_result.returncode == 0:
                import json
                probe_data = json.loads(probe_result.stdout)
                video_duration = float(probe_data['format']['duration'])
            else:
                video_duration = 30000  # é»˜è®¤30åˆ†é’Ÿ
        except:
            video_duration = 30000  # é»˜è®¤30åˆ†é’Ÿ
        
        log_info(f"[pipeline] è§†é¢‘æ€»æ—¶é•¿: {video_duration:.1f}ç§’")
        log_info(f"[pipeline] åˆ‡ç‰‡é…ç½®: æœ€å°æ—¶é•¿={min_clip_duration}ç§’, å‰åæ–‡æ‰©å±•={context_extend}ç§’")
        
        # æ‰©å±•ç‰‡æ®µæ—¶é•¿
        from acfv.processing.clip_video import ensure_min_duration, extend_segment
        
        # æ­¥éª¤1ï¼šæ‰©å±•ç‰‡æ®µå‰åæ–‡
        if context_extend > 0:
            log_info(f"[pipeline] æ‰©å±•ç‰‡æ®µå‰åæ–‡ {context_extend}ç§’...")
            segments_data = [extend_segment(seg, context_extend, video_duration) for seg in segments_data]
        
        # æ­¥éª¤2ï¼šç¡®ä¿è¾¾åˆ°æœ€å°æ—¶é•¿
        log_info(f"[pipeline] ç¡®ä¿åˆ‡ç‰‡è¾¾åˆ°æœ€å°æ—¶é•¿ {min_clip_duration}ç§’...")
        segments_data = ensure_min_duration(segments_data, min_clip_duration, video_duration)
        
        # æ­¥éª¤3ï¼šä»¥è¯„åˆ†ä¼˜å…ˆå®‰æ’ä¸º"ä¸¥æ ¼ä¸é‡å "çš„æ—¶é—´è¡¨ï¼ˆå«å¯é…ç½®ç¼“å†²ï¼‰
        buffer_sec = 0.0
        try:
            buf = cfg_manager.get("NON_OVERLAP_BUFFER_SECONDS")
            if isinstance(buf, (int, float)):
                buffer_sec = max(0.0, float(buf))
        except Exception:
            buffer_sec = 0.0

        if not use_semantic_segment_mode:
            log_info(f"[pipeline] æŒ‰è¯„åˆ†ä¼˜å…ˆå®‰æ’ç‰‡æ®µï¼Œä¿è¯æ— é‡å ï¼ˆç¼“å†²={buffer_sec:.1f}sï¼‰...")

            # è¯„åˆ†é«˜â†’ä½æ’åˆ—ï¼Œé€ä¸ªåœ¨æ—¶é—´è½´ä¸Šå®‰æ”¾
            candidates = sorted(segments_data, key=lambda x: x.get('score', 0.0), reverse=True)
            # æ ‡è®°åŸå§‹ç´¢å¼•ï¼Œä¾¿äºå›å¡«
            for _i, _seg in enumerate(candidates):
                try:
                    _seg['__orig_idx'] = _i
                except Exception:
                    pass
            scheduled = []  # å·²å ç”¨åŒºé—´
            placed_indices = set()

            def _windows_from_scheduled():
                # ç”±å·²å ç”¨æ„é€ ç©ºé—²çª—å£åˆ—è¡¨
                free = []
                cursor = 0.0
                for occ in sorted(scheduled, key=lambda x: x['start']):
                    os, oe = float(occ['start']), float(occ['end'])
                    if os - buffer_sec > cursor:
                        free.append((cursor, os - buffer_sec))
                    cursor = max(cursor, oe + buffer_sec)
                if cursor < video_duration:
                    free.append((cursor, video_duration))
                return free

            drops_due_to_space = 0

            for seg in candidates:
                try:
                    base_s = float(seg.get('start', 0.0))
                    base_e = float(seg.get('end', 0.0))
                except Exception:
                    continue
                if base_e <= base_s:
                    continue

                # å·²ç»åº”ç”¨è¿‡å‰åæ–‡æ‰©å±•ä¸æœ€å°æ—¶é•¿ï¼Œè¿™é‡Œåªç¡®ä¿åœ¨ç©ºçª—å†…è½ä½
                desired_s = max(0.0, min(base_s, video_duration))
                desired_e = max(0.0, min(base_e, video_duration))
                target_len = max(0.0, desired_e - desired_s)
                if target_len <= 0.0:
                    continue

                # éå†å½“å‰ç©ºçª—ï¼Œé€‰æ‹©ä¸åŸä¸­å¿ƒæœ€è¿‘çš„å¯æ”¾ç½®çª—å£
                free_windows = _windows_from_scheduled()
                if not free_windows:
                    drops_due_to_space += 1
                    continue
                center = (desired_s + desired_e) / 2.0
                free_windows.sort(key=lambda w: abs(((w[0] + w[1]) / 2.0) - center))

                placed = False
                for (L, R) in free_windows:
                    # åœ¨è¯¥çª—å£å†…å°½é‡ä¿æŒåŸåŒºé—´ï¼Œè‹¥ä¸å¤Ÿåˆ™å¤¹ç´§
                    s = max(L, desired_s)
                    e = min(R, desired_e)
                    if e - s <= 0.0:
                        continue
                    # ä¿æŒåŸé•¿åº¦çš„å‰æä¸‹ï¼Œè‹¥çª—å£è¾ƒå¤§ï¼Œå°è¯•å±…ä¸­æ”¾ç½®
                    length = min(target_len, R - L)
                    if length <= 0.0:
                        continue
                    # è°ƒæ•´ä¸ºä¸åŸä¸­å¿ƒå¯¹é½çš„ç­‰é•¿åŒºé—´
                    half = length / 2.0
                    s_candidate = max(L, min(center - half, R - length))
                    e_candidate = s_candidate + length
                    if e_candidate - s_candidate > 0.0:
                        new_seg = dict(seg)
                        new_seg['start'] = float(s_candidate)
                        new_seg['end'] = float(e_candidate)
                        scheduled.append(new_seg)
                        if '__orig_idx' in seg:
                            placed_indices.add(seg['__orig_idx'])
                        placed = True
                        break

                if not placed:
                    drops_due_to_space += 1

            # äºŒæ¬¡å›å¡«ï¼šé€æ­¥å‡å°‘ç¼“å†²ã€æŒ‰å¯ç”¨ç©ºçª—å‰ªè£æ”¾å…¥ï¼Œç›´è‡³å‡‘æ»¡Top-N
            if max_clips > 0 and len(scheduled) < max_clips:
                def windows_with_buffer(cur_sched, cur_buf):
                    free = []
                    cursor = 0.0
                    for occ in sorted(cur_sched, key=lambda x: x['start']):
                        os, oe = float(occ['start']), float(occ['end'])
                        if os - cur_buf > cursor:
                            free.append((cursor, os - cur_buf))
                        cursor = max(cursor, oe + cur_buf)
                    if cursor < video_duration:
                        free.append((cursor, video_duration))
                    return sorted(free, key=lambda w: (w[1]-w[0]), reverse=True)

                unplaced = [seg for seg in candidates if seg.get('__orig_idx') not in placed_indices]
                relax_buffers = [max(buffer_sec/2.0, 0.0), 0.0]
                relax_min = [min_clip_duration, max(min_clip_duration*0.75, 60.0), max(min_clip_duration*0.5, 45.0)]
                for rb in relax_buffers:
                    if len(scheduled) >= max_clips:
                        break
                    free_ws = windows_with_buffer(scheduled, rb)
                    for seg in unplaced:
                        if len(scheduled) >= max_clips:
                            break
                        base_s = float(seg.get('start', 0.0)); base_e = float(seg.get('end', 0.0))
                        if base_e <= base_s:
                            continue
                        orig_len = base_e - base_s
                        # é€‰æœ€å¤§çš„ç©ºçª—ï¼ŒæŒ‰é•¿åº¦å‰ªè£æ”¾å…¥
                        for L, R in free_ws:
                            win_len = max(0.0, R - L)
                            if win_len <= 0.0:
                                continue
                            # ä¾æ¬¡å°è¯•æ›´ä¸¥æ ¼çš„æœ€çŸ­é•¿åº¦
                            placed2 = False
                            for mn in relax_min:
                                if win_len < mn:
                                    continue
                                length = min(orig_len, win_len)
                                # å±…ä¸­æ‘†æ”¾åˆ°çª—å£
                                s_cand = L + max(0.0, (win_len - length)/2.0)
                                e_cand = s_cand + length
                                if e_cand - s_cand > 0.0:
                                    new_seg = dict(seg)
                                    new_seg['start'] = float(s_cand)
                                    new_seg['end'] = float(e_cand)
                                    scheduled.append(new_seg)
                                    placed2 = True
                                    break
                            if placed2:
                                # é‡æ–°è®¡ç®—ç©ºçª—
                                free_ws = windows_with_buffer(scheduled, rb)
                                break

                # å¦‚ä»ä¸è¶³ï¼Œè®°å½•ä½†ä¸é˜»å¡
                if len(scheduled) < max_clips:
                    remaining = max_clips - len(scheduled)
                    log_warning(f"[pipeline] ç”±äºæ—¶é—´è½´æ‹¥æŒ¤ï¼Œä»æœ‰ {remaining} ä¸ªæœªèƒ½å®‰æ”¾ï¼ˆå·²å›å¡«åˆ°æœ€å¤§å¯èƒ½ï¼‰")

            # è¾“å‡ºä¸º"è¯„åˆ†ä¼˜å…ˆ+ä¸¥æ ¼æ— é‡å "çš„åºåˆ—ï¼Œå¹¶è£å‰ªä¸ºTop-N
            segments_data = sorted(scheduled, key=lambda x: x.get('score', 0), reverse=True)
            if max_clips > 0 and len(segments_data) > max_clips:
                segments_data = segments_data[:max_clips]
        else:
            # è¯­ä¹‰åˆ†æ®µæ¨¡å¼ï¼šæŒ‰æ—¶é—´é¡ºåºè¾“å‡ºï¼Œè‹¥æœ‰çŸ­æ®µåˆ™é¡ºåºå¹¶å…¥åç»§ç›´è‡³è¾¾åˆ°æœ€å°æ—¶é•¿
            segments_data = sorted(segments_data, key=lambda x: float(x.get('start', 0.0)))
            merged = []
            i = 0
            while i < len(segments_data):
                s = float(segments_data[i].get('start', 0.0))
                e = float(segments_data[i].get('end', 0.0))
                txt = segments_data[i].get('text', '')
                j = i
                while (e - s) < min_clip_duration and (j + 1) < len(segments_data):
                    j += 1
                    e = float(segments_data[j].get('end', e))
                    txt = (txt + ' ' + segments_data[j].get('text', '')).strip()
                merged.append({'start': s, 'end': e, 'text': txt})
                i = j + 1
            segments_data = merged
            # å†æ¬¡ä¿è¯"æ°å¥½Næ®µä¸”ä¸é‡å "ï¼ˆåˆå¹¶åå¯èƒ½å‡å°‘æ•°é‡ï¼‰
            try:
                desired_count = int(cfg_manager.get("MAX_CLIP_COUNT") or 10)
            except Exception:
                desired_count = 10
            # è£å‰ªè¿‡å¤š
            if desired_count > 0 and len(segments_data) > desired_count:
                pass  # selection handled in normalize step
            # æ‹†åˆ†ä¸è¶³
            if desired_count > 0 and len(segments_data) < desired_count:
                # åŠ è½½è½¬å½•ä»¥ä¾¿æŒ‰è¾¹ç•Œæ‹†åˆ†
                transcription_list = []
                try:
                    if os.path.exists(transcription_output):
                        with open(transcription_output, 'r', encoding='utf-8') as f:
                            transcription_list = json.load(f) or []
                except Exception:
                    transcription_list = []
                def _split_by_mid(seg, min_child_len):
                    s0 = float(seg.get('start', 0.0)); e0 = float(seg.get('end', 0.0))
                    mid = (s0 + e0) / 2.0
                    base_score = float(seg.get('score', seg.get('interest_score', 0.0)) or 0.0)
                    dur = max(e0 - s0, 1e-6)
                    # è¯„åˆ†æŒ‰æ—¶é•¿æ¯”ä¾‹æ‹†åˆ†ï¼Œä¿æŒæ€»é‡å®ˆæ’
                    left_score = base_score * (mid - s0) / dur
                    right_score = base_score * (e0 - mid) / dur
                    left = {'start': s0, 'end': mid, 'text': seg.get('text',''), 'score': left_score}
                    right = {'start': mid, 'end': e0, 'text': seg.get('text',''), 'score': right_score}
                    if (right['end']-right['start']) < min_child_len or (left['end']-left['start']) < min_child_len:
                        return None
                    return [left, right]
                def _split_longest_semantic(segments, need_count, min_child_len):
                    safety = 0
                    while len(segments) < need_count and safety < 200:
                        safety += 1
                        # é€‰æœ€é•¿è€…
                        idx = max(range(len(segments)), key=lambda i: float(segments[i].get('end',0.0)) - float(segments[i].get('start',0.0))) if segments else -1
                        if idx < 0:
                            break
                        base = segments[idx]
                        s0 = float(base.get('start', 0.0)); e0 = float(base.get('end', 0.0))
                        if (e0 - s0) < (2.0 * min_child_len):
                            break
                        # é¦–é€‰åœ¨è½¬å½•è¾¹ç•Œä¸­ç‚¹é™„è¿‘æ‹†åˆ†
                        try:
                            candidates = []
                            for t in transcription_list:
                                try:
                                    ts = float(t.get('start', 0.0)); te = float(t.get('end', 0.0))
                                except Exception:
                                    continue
                                if ts <= s0 or te >= e0:
                                    continue
                                mid = (ts + te) / 2.0
                                if (mid - s0) >= min_child_len and (e0 - mid) >= min_child_len:
                                    candidates.append(mid)
                            if candidates:
                                pref = (s0 + e0)/2.0
                                split_t = min(candidates, key=lambda x: abs(x - pref))
                                base_score = float(base.get('score', base.get('interest_score', 0.0)) or 0.0)
                                dur = max(e0 - s0, 1e-6)
                                left_score = base_score * (split_t - s0) / dur
                                right_score = base_score * (e0 - split_t) / dur
                                left = {'start': s0, 'end': split_t, 'text': base.get('text',''), 'score': left_score}
                                right = {'start': split_t, 'end': e0, 'text': base.get('text',''), 'score': right_score}
                                segments.pop(idx)
                                segments.extend([left, right])
                                segments.sort(key=lambda x: float(x.get('start', 0.0)))
                                continue
                        except Exception:
                            pass
                        # å›é€€ï¼šç”¨ä¸­ç‚¹æ‹†åˆ†
                        sp = _split_by_mid(base, min_child_len)
                        if not sp:
                            break
                        segments.pop(idx)
                        segments.extend(sp)
                        segments.sort(key=lambda x: float(x.get('start', 0.0)))
                    return segments
                min_child = max(min_clip_duration * 0.5, 30.0)
                segments_data = _split_longest_semantic(segments_data, desired_count, min_child)
            # æœ€ç»ˆå»é‡å ï¼ˆå‹ç´§åˆ°ç›¸é‚»ï¼‰
            segments_data = sorted(segments_data, key=lambda x: float(x.get('start', 0.0)))
            for i in range(len(segments_data)-1):
                try:
                    if float(segments_data[i]['end']) > float(segments_data[i+1]['start']):
                        segments_data[i]['end'] = float(segments_data[i+1]['start'])
                except Exception:
                    pass

        # ç¡®ä¿æ¯ä¸ªç‰‡æ®µéƒ½æœ‰ scoreï¼ˆè¯­ä¹‰æ‹†åˆ†åç»§æ‰¿/ä¼°ç®—ï¼‰
        for _seg in segments_data:
            if 'score' not in _seg or _seg['score'] is None:
                base_val = float(_seg.get('interest_score', 0.0) or 0.0)
                # ç»™ä¸€ä¸ªå¾ˆå°çš„æ­£å€¼é˜²æ­¢éƒ½æ˜¯ 0.000
                _seg['score'] = max(base_val, 0.005) if base_val > 0 else 0.005
        
        # æ˜¾ç¤ºæœ€ç»ˆçš„è¯„åˆ†é¡ºåº
        final_scores = [f"{seg.get('score', 0):.3f}" for seg in segments_data[:5]]
        log_info(f"[pipeline] æœ€ç»ˆç‰‡æ®µé¡ºåºï¼ˆæŒ‰è¯„åˆ†ï¼‰: {final_scores}")
        
        log_info(f"[pipeline] åˆ‡ç‰‡æ—¶é•¿æ‰©å±•å®Œæˆï¼Œå…± {len(segments_data)} ä¸ªç‰‡æ®µ")

        # æœ€åä¿é™©ï¼šæ¸…æ´—ç‰‡æ®µæ—¶é—´ï¼Œé¿å…å‡ºç° end < start æˆ–æŒç»­æ—¶é—´ä¸ºéæ­£å¯¼è‡´ -t è´Ÿæ•°
        try:
            cleaned_segments = []
            auto_fixed = 0
            for _seg in segments_data:
                try:
                    s = float(_seg.get('start', 0.0))
                    e = float(_seg.get('end', 0.0))
                except Exception:
                    continue
                # ä¿®å¤é¢ å€’
                if e < s:
                    s, e = e, s
                    auto_fixed += 1
                # çº¦æŸåˆ°è§†é¢‘èŒƒå›´
                s = max(0.0, min(s, video_duration))
                e = max(0.0, min(e, video_duration))
                # ç¡®ä¿æœ€å°æ­£æ—¶é•¿
                if e <= s:
                    e = min(video_duration, s + 1.0)
                if e <= s:
                    continue
                _seg['start'] = s
                _seg['end'] = e
                cleaned_segments.append(_seg)
            if auto_fixed > 0:
                log_warning(f"[pipeline] ç‰‡æ®µæ—¶é—´å­˜åœ¨é¢ å€’ï¼Œå·²è‡ªåŠ¨ä¿®å¤ {auto_fixed} ä¸ª")
            if len(cleaned_segments) != len(segments_data):
                log_warning(f"[pipeline] æ¸…æ´—åç‰‡æ®µæ•°é‡: {len(cleaned_segments)}/{len(segments_data)}")
            segments_data = cleaned_segments
        except Exception as _e:
            log_warning(f"[pipeline] ç‰‡æ®µæ—¶é—´æ¸…æ´—å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸç‰‡æ®µ: {_e}")
    
    log_info(f"[pipeline] Clipping video directly to: {output_clips_dir}")
    os.makedirs(output_clips_dir, exist_ok=True)
    clip_files = []
    
    if segments_data:
        def sequential_clip_generation(segments, video_path, output_dir, audio_source=None, progress_callback=None):
            """ä¸²è¡Œåˆ‡ç‰‡ç”Ÿæˆ"""
            # éªŒè¯è¾“å…¥å‚æ•°
            if not segments:
                log_error("[pipeline] æ²¡æœ‰ç‰‡æ®µæ•°æ®ï¼Œæ— æ³•ç”Ÿæˆåˆ‡ç‰‡")
                return []
            
            if not os.path.exists(video_path):
                log_error(f"[pipeline] è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                return []
            
            log_info(f"[pipeline] å¼€å§‹ä¸²è¡Œåˆ‡ç‰‡ç”Ÿæˆï¼Œå…± {len(segments)} ä¸ªç‰‡æ®µ")
            
            clip_files = []
            video_base = _sanitize_component(Path(video_path).stem)
            
            # é¢„å…ˆæ¢æµ‹ä¸€æ¬¡è§†é¢‘æ—¶é•¿ï¼Œé¿å…æ¯ä¸ªç‰‡æ®µé‡å¤ffprobe
            try:
                _probe_cmd = ['ffprobe', '-v', 'quiet', '-print_format', 'json', '-show_format', video_path]
                _probe_result = subprocess.run(_probe_cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore', timeout=30)
                if _probe_result.returncode == 0:
                    import json as _json
                    _probe_data = _json.loads(_probe_result.stdout)
                    video_duration_global = float(_probe_data['format']['duration'])
                else:
                    video_duration_global = 30000.0
            except Exception:
                video_duration_global = 30000.0

            # ç»Ÿè®¡æ€»è¾“å‡ºç§’æ•°ç”¨äº"åˆ‡ç‰‡ç”Ÿæˆ"é˜¶æ®µè¿›åº¦ä¼°è®¡
            try:
                total_output_seconds = sum(max(0.0, float(seg['end']) - float(seg['start'])) for seg in segments)
            except Exception:
                total_output_seconds = float(len(segments)) * 60.0
            processed_output_seconds = 0.0

            def generate_single_clip(segment, index):
                """ç”Ÿæˆå•ä¸ªåˆ‡ç‰‡"""
                try:
                    start_time = segment['start']
                    end_time = segment['end']
                    duration = end_time - start_time
                    # ä¿é™©ï¼šè‹¥å¤–å±‚å­˜åœ¨å¼‚å¸¸ï¼Œé˜²æ­¢å‡ºç°éæ­£æ—¶é•¿
                    if duration <= 0:
                        end_time = min(video_duration_global, start_time + 1.0)
                        duration = max(0.5, end_time - start_time)
                    
                    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å - ç¡®ä¿ç´¢å¼•æ­£ç¡®
                    segment_index = index + 1  # ç¡®ä¿ä»1å¼€å§‹
                    clip_filename = f"{video_base}__clip_{segment_index:03d}_{start_time:.1f}s-{end_time:.1f}s.mp4"
                    output_path = os.path.join(output_dir, clip_filename)
                    
                    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§æ–‡ä»¶
                    if os.path.exists(output_path):
                        try:
                            os.remove(output_path)
                            log_info(f"[pipeline] æ¸…ç†æ—§æ–‡ä»¶: {output_path}")
                        except Exception as e:
                            log_warning(f"[pipeline] æ¸…ç†æ—§æ–‡ä»¶å¤±è´¥: {e}")
                    
                    log_info(f"[pipeline] ç”Ÿæˆåˆ‡ç‰‡ {index+1}/{len(segments)}: {clip_filename} ({duration:.1f}s)")
                    
                    # ä½¿ç”¨é¢„å…ˆæ¢æµ‹çš„è§†é¢‘æ—¶é•¿
                    video_duration = video_duration_global
                    use_fast_seek = start_time > video_duration_global * 0.5
                    
                    # ä½¿ç”¨å¿«é€Ÿåˆ‡ç‰‡æ–¹æ³•ï¼ˆç›´æ¥å¤åˆ¶æµï¼Œä¸é‡æ–°ç¼–ç ï¼‰
                    def cut_video_ffmpeg_fast(input_path, output_path, start_time, duration):
                        """ä½¿ç”¨FFmpegå¿«é€Ÿåˆ‡ç‰‡ï¼šå¤åˆ¶è§†é¢‘æµï¼ŒéŸ³é¢‘è½¬AACï¼Œé¿å…æ— å£°/ä¸å…¼å®¹å®¹å™¨"""
                        cmd = [
                            "ffmpeg", "-y",
                            "-hide_banner", "-loglevel", "error", "-nostdin",
                            "-ss", str(start_time),         # èµ·å§‹æ—¶é—´ï¼ˆè¾“å…¥å¯»å€ï¼Œå¿«ï¼‰
                            "-i", input_path,               # è¾“å…¥è§†é¢‘
                            "-t", str(duration),            # ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰
                            "-map", "0:v:0",               # æ˜ç¡®æ˜ å°„è§†é¢‘
                            "-map", "0:a?",                # å¯é€‰æ˜ å°„éŸ³é¢‘
                            "-c:v", "copy",                # å¤åˆ¶è§†é¢‘
                            "-c:a", "aac",                 # ç»Ÿä¸€AACéŸ³é¢‘
                            "-b:a", "160k",
                            "-movflags", "+faststart",     # å¿«é€Ÿå¯åŠ¨
                            output_path                     # è¾“å‡ºæ–‡ä»¶è·¯å¾„
                        ]
                        subprocess.run(cmd, check=True)
                    
                    # æ„å»ºFFmpegå‘½ä»¤ - ä½¿ç”¨å¿«é€Ÿåˆ‡ç‰‡
                    if audio_source and os.path.exists(audio_source):
                        # å¦‚æœæœ‰éŸ³é¢‘æºï¼Œå°½é‡å¤åˆ¶è§†é¢‘æµï¼Œä»…ç¼–ç éŸ³é¢‘ï¼ŒåŠ é€Ÿè¾“å‡º
                        if use_fast_seek:
                            cmd = [
                                'ffmpeg', '-y',
                                '-hide_banner', '-loglevel', 'error', '-nostdin',
                                '-ss', str(start_time),
                                '-i', str(video_path),
                                '-i', str(audio_source),
                                '-map', '0:v',
                                '-map', '1:a',
                                '-t', str(duration),
                                '-c:v', 'copy',              # å¤åˆ¶è§†é¢‘ï¼Œé¿å…é‡ç¼–ç 
                                '-c:a', 'aac',
                                '-preset', 'veryfast',
                                '-avoid_negative_ts', 'make_zero',
                                '-movflags', '+faststart',
                                '-threads', '0',
                                '-max_muxing_queue_size', '1024',
                                str(output_path)
                            ]
                        else:
                            cmd = [
                                'ffmpeg', '-y',
                                '-hide_banner', '-loglevel', 'error', '-nostdin',
                                '-i', str(video_path),
                                '-i', str(audio_source),
                                '-map', '0:v',
                                '-map', '1:a',
                                '-ss', str(start_time),
                                '-t', str(duration),
                                '-c:v', 'copy',              # å¤åˆ¶è§†é¢‘ï¼Œé¿å…é‡ç¼–ç 
                                '-c:a', 'aac',
                                '-preset', 'veryfast',
                                '-movflags', '+faststart',
                                '-threads', '0',
                                '-max_muxing_queue_size', '1024',
                                str(output_path)
                            ]
                    else:
                        # æ²¡æœ‰éŸ³é¢‘æºï¼Œä½¿ç”¨å¿«é€Ÿåˆ‡ç‰‡ï¼›è‹¥è¾“å‡ºå¼‚å¸¸ï¼ˆ0ç§’/æ— è§†é¢‘æµï¼‰ï¼Œå›é€€åˆ°ç¼–ç æ¨¡å¼
                        try:
                            cut_video_ffmpeg_fast(str(video_path), str(output_path), start_time, duration)
                            # å…ˆä¸è¿”å›ï¼Œåšä¸€æ¬¡å®Œæ•´æ€§æ£€æŸ¥
                            try:
                                probe_cmd = [
                                    'ffprobe', '-v', 'quiet', '-print_format', 'json',
                                    '-show_format', '-show_streams', str(output_path)
                                ]
                                probe_result_fast = subprocess.run(
                                    probe_cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore', timeout=15
                                )
                                need_fallback = True
                                if probe_result_fast.returncode == 0:
                                    import json as _json
                                    pdata = _json.loads(probe_result_fast.stdout or '{}')
                                    streams = pdata.get('streams', []) or []
                                    has_video_stream = any(s.get('codec_type') == 'video' for s in streams)
                                    duration_val = 0.0
                                    try:
                                        duration_val = float((pdata.get('format') or {}).get('duration') or 0.0)
                                    except Exception:
                                        duration_val = 0.0
                                    need_fallback = (not has_video_stream) or (duration_val <= 0.5)
                                if need_fallback:
                                    # æ„å»ºå›é€€ç¼–ç å‘½ä»¤
                                    if use_fast_seek:
                                        cmd = [
                                            'ffmpeg', '-y',
                                            '-hide_banner', '-loglevel', 'error', '-nostdin',
                                            '-ss', str(start_time),
                                            '-i', str(video_path),
                                            '-t', str(duration),
                                            '-c:v', 'libx264',
                                            '-c:a', 'aac',
                                            '-preset', 'veryfast',
                                            '-crf', '23',
                                            '-avoid_negative_ts', 'make_zero',
                                            '-movflags', '+faststart',
                                            '-threads', '0',
                                            '-max_muxing_queue_size', '1024',
                                            str(output_path)
                                        ]
                                    else:
                                        cmd = [
                                            'ffmpeg', '-y',
                                            '-hide_banner', '-loglevel', 'error', '-nostdin',
                                            '-i', str(video_path),
                                            '-ss', str(start_time),
                                            '-t', str(duration),
                                            '-c:v', 'libx264',
                                            '-c:a', 'aac',
                                            '-preset', 'veryfast',
                                            '-crf', '23',
                                            '-threads', '0',
                                            '-max_muxing_queue_size', '1024',
                                            str(output_path)
                                        ]
                                # å¦‚æœ need_fallback ä¸º Falseï¼Œåˆ™ä¸å®šä¹‰ cmdï¼Œè®©åç»­éªŒè¯ç›´æ¥é€šè¿‡
                            except Exception:
                                # æ¢æµ‹å¤±è´¥æ—¶ä¿æŒç°çŠ¶ï¼Œç”±åç»­å¤§å°/æ¢æµ‹æ£€æŸ¥å…œåº•
                                pass
                        except subprocess.CalledProcessError as e:
                            log_warning(f"[pipeline] å¿«é€Ÿåˆ‡ç‰‡å¤±è´¥ï¼Œå›é€€åˆ°ç¼–ç æ¨¡å¼: {e}")
                            # å›é€€åˆ°ç¼–ç æ¨¡å¼
                            if use_fast_seek:
                                cmd = [
                                    'ffmpeg', '-y',
                                    '-hide_banner', '-loglevel', 'error', '-nostdin',
                                    '-ss', str(start_time),
                                    '-i', str(video_path),
                                    '-t', str(duration),
                                    '-c:v', 'libx264',
                                    '-c:a', 'aac',
                                    '-preset', 'veryfast',
                                    '-crf', '23',
                                    '-avoid_negative_ts', 'make_zero',
                                    '-movflags', '+faststart',
                                    '-threads', '0',
                                    '-max_muxing_queue_size', '1024',
                                    str(output_path)
                                ]
                            else:
                                cmd = [
                                    'ffmpeg', '-y',
                                    '-hide_banner', '-loglevel', 'error', '-nostdin',
                                    '-i', str(video_path),
                                    '-ss', str(start_time),
                                    '-t', str(duration),
                                    '-c:v', 'libx264',
                                    '-c:a', 'aac',
                                    '-preset', 'veryfast',
                                    '-crf', '23',
                                    '-threads', '0',
                                    '-max_muxing_queue_size', '1024',
                                    str(output_path)
                                ]
                    
                    # åŠ¨æ€è¶…æ—¶æ—¶é—´ - åŸºäºåˆ‡ç‰‡æ—¶é•¿å’Œä½ç½®
                    base_timeout = 1800  # å¢åŠ åˆ°30åˆ†é’Ÿ
                    safe_duration = max(float(duration), 1.0)
                    duration_factor = min(safe_duration / 10.0, 3.0)  # åŸºäºåˆ‡ç‰‡æ—¶é•¿ï¼Œæœ€å¤§3å€
                    position_factor = 1.0
                    if start_time > video_duration * 0.8:
                        position_factor = 2.0  # è§†é¢‘æœ«å°¾éœ€è¦æ›´å¤šæ—¶é—´
                    elif start_time > video_duration * 0.6:
                        position_factor = 1.5
                    
                    timeout = int(base_timeout * duration_factor * position_factor)
                    log_info(f"[pipeline] åˆ‡ç‰‡ {index+1} è¶…æ—¶è®¾ç½®: {timeout}s (æ—¶é•¿:{duration:.1f}s, ä½ç½®:{start_time:.1f}s)")
                    
                    # æ‰§è¡ŒFFmpegå‘½ä»¤ï¼ˆåªæœ‰åœ¨éœ€è¦ç¼–ç æ—¶æ‰æ‰§è¡Œï¼‰
                    if 'cmd' in locals():
                        result = subprocess.run(
                            cmd, 
                            capture_output=True, 
                            text=True, 
                            encoding='utf-8',
                            errors='ignore',
                            timeout=timeout
                        )
                    
                    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
                    if os.path.exists(output_path):
                        file_size = os.path.getsize(output_path)
                        if file_size > 1024 * 1024:  # è‡³å°‘1MB
                            # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§ - æ”¹è¿›æ£€æŸ¥é€»è¾‘
                            try:
                                probe_cmd = [
                                    'ffprobe', '-v', 'quiet', '-print_format', 'json',
                                    '-show_format', '-show_streams', output_path
                                ]
                                probe_result = subprocess.run(probe_cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore', timeout=30)
                                
                                if probe_result.returncode == 0:
                                    # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘æµ
                                    import json
                                    probe_data = json.loads(probe_result.stdout)
                                    streams = probe_data.get('streams', [])
                                    has_video_stream = any(stream.get('codec_type') == 'video' for stream in streams)
                                    has_audio_stream = any(stream.get('codec_type') == 'audio' for stream in streams)
                                    
                                    if has_video_stream:
                                        if not has_audio_stream:
                                            log_warning(f"[pipeline] åˆ‡ç‰‡ {index+1} ç¼ºå°‘éŸ³é¢‘æµ: {output_path}")
                                        log_info(f"[pipeline] åˆ‡ç‰‡ {index+1} ç”ŸæˆæˆåŠŸ: {output_path} ({file_size} bytes)")
                                        # æ›´æ–°åˆ‡ç‰‡é˜¶æ®µè¿›åº¦ï¼ˆåŸºäºç´¯è®¡è¾“å‡ºç§’æ•°ï¼‰
                                        try:
                                            nonlocal processed_output_seconds
                                            processed_output_seconds += max(0.0, float(end_time) - float(start_time))
                                            if 'smart_predictor' in locals() and smart_predictor and total_output_seconds > 0:
                                                progress_ratio = min(max(processed_output_seconds / total_output_seconds, 0.0), 1.0)
                                                smart_predictor.update_stage_progress("åˆ‡ç‰‡ç”Ÿæˆ", progress_ratio)
                                        except Exception:
                                            pass
                                        return output_path
                                    else:
                                        log_error(f"[pipeline] åˆ‡ç‰‡ {index+1} ç¼ºå°‘è§†é¢‘æµ")
                                        if os.path.exists(output_path):
                                            os.remove(output_path)
                                        return None
                                else:
                                    log_error(f"[pipeline] åˆ‡ç‰‡ {index+1} æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {probe_result.stderr}")
                                    if os.path.exists(output_path):
                                        os.remove(output_path)
                                    return None
                            except Exception as e:
                                log_error(f"[pipeline] åˆ‡ç‰‡ {index+1} æ–‡ä»¶æ£€æŸ¥å¼‚å¸¸: {e}")
                                # å¦‚æœæ–‡ä»¶è¶³å¤Ÿå¤§ï¼Œå¯èƒ½æ˜¯æ£€æŸ¥å·¥å…·é—®é¢˜ï¼Œä¿ç•™æ–‡ä»¶
                                if file_size > 1024 * 1024:  # å¤§äº1MB
                                    log_info(f"[pipeline] åˆ‡ç‰‡ {index+1} æ–‡ä»¶è¾ƒå¤§ï¼Œä¿ç•™: {output_path} ({file_size} bytes)")
                                    return output_path
                                else:
                                    if os.path.exists(output_path):
                                        os.remove(output_path)
                                    return None
                        else:
                            log_error(f"[pipeline] åˆ‡ç‰‡ {index+1} æ–‡ä»¶å¤ªå°: {file_size} bytes (éœ€è¦è‡³å°‘1MB)")
                            if os.path.exists(output_path):
                                os.remove(output_path)
                            return None
                    else:
                        log_error(f"[pipeline] åˆ‡ç‰‡ {index+1} è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
                        return None

                        
                except Exception as e:
                    log_error(f"[pipeline] åˆ‡ç‰‡ {index+1} å¼‚å¸¸: {e}")
                    if os.path.exists(output_path):
                        try:
                            os.remove(output_path)
                        except:
                            pass
                    return None
            
            # ä¸²è¡Œæ‰§è¡Œåˆ‡ç‰‡ç”Ÿæˆ
            successful_clips = []
            
            for i, segment in enumerate(segments):
                try:
                    clip_path = generate_single_clip(segment, i)
                    if clip_path:
                        successful_clips.append(clip_path)
                        log_info(f"[pipeline] åˆ‡ç‰‡ {i+1} å®Œæˆï¼Œå½“å‰æˆåŠŸ: {len(successful_clips)}/{len(segments)}")
                    
                    if progress_callback:
                        progress_callback(i + 1, len(segments))
                        
                except Exception as e:
                    log_error(f"[pipeline] åˆ‡ç‰‡ {i+1} ä»»åŠ¡å¼‚å¸¸: {e}")
                    if progress_callback:
                        progress_callback(i + 1, len(segments))
            
            log_info(f"[pipeline] ä¸²è¡Œåˆ‡ç‰‡ç”Ÿæˆå®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(successful_clips)} ä¸ªåˆ‡ç‰‡")
            return successful_clips
        
        def clip_progress_callback(current, total):
            emit_progress("ä¸²è¡Œè§†é¢‘åˆ‡ç‰‡", current_step, total_steps, f"æ­£åœ¨ç”Ÿæˆç¬¬{current}/{total}ä¸ªåˆ‡ç‰‡...")
        
        try:
            # ä½¿ç”¨ä¸²è¡Œåˆ‡ç‰‡ç”Ÿæˆ
            clip_files = sequential_clip_generation(
                segments_data, video, output_clips_dir, 
                audio_source=host_audio_path, 
                progress_callback=clip_progress_callback
            )
        except Exception as e:
            log_error(f"[pipeline] ä¸²è¡Œåˆ‡ç‰‡å¤±è´¥: {e}")
            # é™çº§åˆ°clip_videoå‡½æ•°
            try:
                import inspect
                clip_sig = inspect.signature(clip_video)
                if 'progress_callback' in clip_sig.parameters:
                    clip_video(video_path=video, analysis_file=analysis_output, output_dir=output_clips_dir, 
                              progress_callback=clip_progress_callback, audio_source=host_audio_path)
                else:
                    clip_video(video_path=video, analysis_file=analysis_output, output_dir=output_clips_dir, 
                              audio_source=host_audio_path)
            except Exception as e2:
                log_error(f"[pipeline] é™çº§åˆ‡ç‰‡ä¹Ÿå¤±è´¥: {e2}")
        
        # æ£€æŸ¥æœ€ç»ˆç›®å½•ä¸­çš„åˆ‡ç‰‡æ–‡ä»¶
        final_clips = []
        for file in os.listdir(output_clips_dir):
            if file.lower().endswith('.mp4'):
                clip_path = os.path.join(output_clips_dir, file)
                if os.path.isfile(clip_path) and os.path.getsize(clip_path) > 1024:  # è‡³å°‘1KB
                    final_clips.append(clip_path)
        
        log_info(f"[pipeline] æœ€ç»ˆåˆ‡ç‰‡ç»Ÿè®¡: æˆåŠŸç”Ÿæˆ {len(final_clips)} ä¸ªæœ‰æ•ˆåˆ‡ç‰‡")
        if len(final_clips) != len(segments_data):
            log_warning(f"[pipeline] åˆ‡ç‰‡æ•°é‡ä¸åŒ¹é…: æœŸæœ› {len(segments_data)} ä¸ªï¼Œå®é™… {len(final_clips)} ä¸ª")
        
        # å°†æœ€ç»ˆåˆ‡ç‰‡æ·»åŠ åˆ°clip_filesåˆ—è¡¨
        for clip_path in final_clips:
            if clip_path not in clip_files:
                clip_files.append(clip_path)
                log_info(f"[pipeline] Found clip: {clip_path}")
        
        log_info(f"[pipeline] Successfully generated {len(clip_files)} clip files")
        
        # æ›´æ–°æ™ºèƒ½è¿›åº¦é¢„æµ‹
        if smart_predictor:
            smart_predictor.finish_stage("åˆ‡ç‰‡ç”Ÿæˆ")
            
    else:
        log_error("[pipeline] No segments to clip")

    emit_progress("å®Œæˆ", total_steps, total_steps, f"å¤„ç†å®Œæˆï¼ç”Ÿæˆäº†{len(clip_files)}ä¸ªåˆ‡ç‰‡")
    
    # ç”Ÿæˆæ¯ä¸ªåˆ‡ç‰‡çš„è¯­ä¹‰å­—å¹•ï¼ˆSRTï¼‰
    try:
        from acfv.processing.subtitle_generator import generate_semantic_subtitles_for_clips
        transcription_output = os.path.join(os.path.dirname(analysis_output), "transcription.json")
        if os.path.exists(transcription_output) and clip_files:
            count = generate_semantic_subtitles_for_clips(output_clips_dir, transcription_output, cfg_manager, clip_files)
            log_info(f"[pipeline] å·²ä¸º {count} ä¸ªåˆ‡ç‰‡ç”Ÿæˆè¯­ä¹‰å­—å¹•")
        else:
            log_info("[pipeline] è·³è¿‡å­—å¹•ç”Ÿæˆï¼ˆæ— è½¬å½•æˆ–æ— åˆ‡ç‰‡ï¼‰")
    except Exception as e:
        log_error(f"[pipeline] è¯­ä¹‰å­—å¹•ç”Ÿæˆå¤±è´¥: {e}")

    # ç»“æŸæ™ºèƒ½ä¼šè¯è®°å½•
    try:
        if 'smart_predictor' in locals() and smart_predictor:
            smart_predictor.end_session(success=True)
    except Exception:
        pass

    return output_clips_dir, clip_files, has_chat


def generate_content_indexes(cfg_manager):
    """Generate semantic indexes for rated clips.
    ä½¿ç”¨åˆ‡ç‰‡æ–‡æœ¬ä¸è¯„åˆ†æ„å»ºRAGç´¢å¼•ï¼Œä¼˜å…ˆä½¿ç”¨æœ€è¿‘ä¸€æ¬¡è¿è¡Œï¼ˆruns/latestï¼‰ã€‚
    """
    log_info("[generate_content_indexes] Starting to generate content indexes")

    clips_base_dir = cfg_manager.get("CLIPS_BASE_DIR")
    if not os.path.exists(clips_base_dir):
        log_info("[generate_content_indexes] Clips base dir doesn't exist")
        return "ç´¢å¼•ç”Ÿæˆå®Œæˆï¼ˆæ— éœ€å¤„ç†ï¼‰"

    processed_count = 0
    for video_dir in os.listdir(clips_base_dir):
        video_path = os.path.join(clips_base_dir, video_dir)
        if not os.path.isdir(video_path):
            continue

        # æ”¯æŒæ–°ç»“æ„ï¼šruns/<run_xxx>/ratings.json ä¼˜å…ˆæœ€æ–°ä¸€æ¬¡
        ratings_path = os.path.join(video_path, "ratings.json")
        runs_dir = os.path.join(video_path, "runs")
        latest_run_dir = None
        if os.path.isdir(runs_dir):
            run_names = sorted([d for d in os.listdir(runs_dir) if os.path.isdir(os.path.join(runs_dir, d))])
            if run_names:
                latest_run_dir = os.path.join(runs_dir, run_names[-1])
                candidate = os.path.join(latest_run_dir, "ratings.json")
                if os.path.exists(candidate):
                    ratings_path = candidate

        if not os.path.exists(ratings_path):
            continue

        # å¦‚æœç´¢å¼•å·²å­˜åœ¨åˆ™è·³è¿‡ï¼ˆè§†é¢‘ç›®å½•çº§åˆ«æˆ–æœ€æ–°runçº§åˆ«ä»»ä¸€å­˜åœ¨å³å¯ï¼‰
        index_dir = os.path.join(video_path, "index")
        index_file = os.path.join(index_dir, "content_index.faiss")
        run_index_dir = os.path.join(latest_run_dir, "index") if latest_run_dir else None
        run_index_file = os.path.join(run_index_dir, "content_index.faiss") if run_index_dir else None
        if (index_file and os.path.exists(index_file)) or (run_index_file and os.path.exists(run_index_file)):
            continue

        try:
            with open(ratings_path, "r", encoding="utf-8") as f:
                ratings = json.load(f)
            if not ratings:
                continue

            log_info(f"[generate_content_indexes] Generating index for {video_dir}")

            # è¯»å–è½¬å½•æ–‡ä»¶ï¼ˆlegacy æˆ– mappingï¼‰ï¼Œä½†æˆ‘ä»¬åªéœ€è¦æ¯ä¸ªåˆ‡ç‰‡çš„æ–‡æœ¬
            # ä» ratings.json é‡Œä¼˜å…ˆå– 'text' å­—æ®µ
            segments = []
            weights = []
            for clip_filename, data in ratings.items():
                text = data.get("text", "")
                rating = float(data.get("rating", 0.0))
                if text and text.strip():
                    segments.append({"text": text})
                    weights.append(rating)

            if not segments:
                log_info(f"[generate_content_indexes] No clip texts in ratings for {video_dir}")
                continue

            if not FAISS_AVAILABLE:
                log_info(f"[generate_content_indexes] Skipping index generation for {video_dir} (faiss not available)")
                continue

            index, vectorizer, _ = build_content_index(segments, weights=weights)
            if index and vectorizer:
                # ä¿å­˜åˆ°è§†é¢‘ç›®å½•çº§åˆ«
                os.makedirs(index_dir, exist_ok=True)
                faiss.write_index(index, index_file)
                with open(os.path.join(index_dir, "vectorizer.pkl"), "wb") as f:
                    pickle.dump(vectorizer, f)
                # åŒæ—¶ä¿å­˜åˆ°æœ€æ–°runç›®å½•ï¼Œä¾¿äºæŒ‰runè°ƒè¯•
                if latest_run_dir:
                    os.makedirs(run_index_dir, exist_ok=True)
                    faiss.write_index(index, run_index_file)
                    with open(os.path.join(run_index_dir, "vectorizer.pkl"), "wb") as f:
                        pickle.dump(vectorizer, f)
                log_info(f"[generate_content_indexes] Successfully generated index for {video_dir}")
                processed_count += 1
        except Exception as e:
            log_error(f"[generate_content_indexes] Error processing {video_dir}: {e}")

    log_info("[generate_content_indexes] Finished generating content indexes")
    return f"ç´¢å¼•ç”Ÿæˆå®Œæˆï¼Œå¤„ç†äº† {processed_count} ä¸ªè§†é¢‘ç›®å½•"
