import os

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…å¤šçº¿ç¨‹å†²çª
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['VECLIB_MAXIMUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'

# å¦‚æœä½¿ç”¨torch
try:
    import torch
    torch.set_num_threads(1)
except ImportError:
    pass

import json
from pathlib import Path
from acfv import config
from acfv.main_logging import log_debug, log_info, log_error, log_warning
import math
import datetime
import time
import threading
import subprocess
import hashlib
import tempfile
import concurrent.futures
from functools import lru_cache
import multiprocessing as mp
import shutil

from acfv.runtime.storage import processing_path, settings_path

# å»¶è¿Ÿå¯¼å…¥é‡åº“
def import_heavy_libraries():
    """å»¶è¿Ÿå¯¼å…¥é‡åº“"""
    global np, faiss, TfidfVectorizer, pickle, AudioSegment, librosa, nltk, SentimentIntensityAnalyzer
    
    try:
        import numpy as np
    except ImportError:
        log_error("numpyå¯¼å…¥å¤±è´¥")
        np = None
    
    try:
        import faiss
    except ImportError:
        log_error("faisså¯¼å…¥å¤±è´¥")
        faiss = None
    
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
    except ImportError:
        log_error("sklearnå¯¼å…¥å¤±è´¥")
        TfidfVectorizer = None
    
    try:
        import pickle
    except ImportError:
        log_error("pickleå¯¼å…¥å¤±è´¥")
        pickle = None
    
    try:
        from pydub import AudioSegment
    except ImportError:
        log_error("pydubå¯¼å…¥å¤±è´¥")
        AudioSegment = None
    
    try:
        import librosa
    except ImportError:
        log_error("librosaå¯¼å…¥å¤±è´¥")
        librosa = None
    
    try:
        import nltk
        from nltk.sentiment.vader import SentimentIntensityAnalyzer
    except ImportError:
        log_error("nltkå¯¼å…¥å¤±è´¥")
        nltk = None
        SentimentIntensityAnalyzer = None

# å°è¯•å¯¼å…¥RAGå‘é‡æ•°æ®åº“
try:
    from acfv.rag_vector_database import RAGVectorDatabase
    RAG_DATABASE_AVAILABLE = True
except ImportError as e:
    log_info(f"RAGå‘é‡æ•°æ®åº“ä¸å¯ç”¨: {e}")
    RAG_DATABASE_AVAILABLE = False

# å°è¯•å¯¼å…¥tqdmï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨ç®€å•è¿›åº¦æ˜¾ç¤º
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False

# GPUåŠ é€Ÿç›¸å…³å¯¼å…¥
try:
    import torch
    import torch.nn.functional as F
    from torch.cuda.amp import autocast
    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
    GPU_AVAILABLE = torch.cuda.is_available()
except ImportError as e:
    log_error(f"[analyze_data] GPUåº“å¯¼å…¥å¤±è´¥: {e}")
    GPU_AVAILABLE = False
    torch = None

# å»¶è¿Ÿåˆå§‹åŒ–VADERè¯åº“
sid = None

def init_vader():
    """å»¶è¿Ÿåˆå§‹åŒ–VADERæƒ…æ„Ÿåˆ†æ"""
    global sid
    if sid is None:
        try:
            import_heavy_libraries()
            if SentimentIntensityAnalyzer:
                sid = SentimentIntensityAnalyzer()
            else:
                log_error("[analyze_data] VADERæƒ…æ„Ÿåˆ†æåº“ä¸å¯ç”¨")
        except:
            try:
                if nltk:
                    nltk.download('vader_lexicon')
                    sid = SentimentIntensityAnalyzer()
            except:
                log_error("[analyze_data] VADERæƒ…æ„Ÿåˆ†æåº“åˆå§‹åŒ–å¤±è´¥")
                sid = None

# ============================================================================
# æ–­ç‚¹ç»­ä¼ ç³»ç»Ÿ
# ============================================================================

class CheckpointManager:
    """æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨"""
    
    def __init__(self, base_dir=None):
        if base_dir is None:
            base_dir = str(processing_path())
        self.base_dir = base_dir
        self.checkpoint_file = os.path.join(base_dir, "analysis_checkpoint.json")
        self.metadata_file = os.path.join(base_dir, "analysis_metadata.json")
        self.backup_dir = os.path.join(base_dir, "checkpoints_backup")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(base_dir, exist_ok=True)
        os.makedirs(self.backup_dir, exist_ok=True)
    
    def has_checkpoint(self):
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ£€æŸ¥ç‚¹"""
        return os.path.exists(self.checkpoint_file) and os.path.exists(self.metadata_file)
    
    def get_checkpoint_info(self):
        """è·å–æ£€æŸ¥ç‚¹ä¿¡æ¯"""
        if not self.has_checkpoint():
            return None
        
        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            return {
                'metadata': metadata,
                'processed_count': len(checkpoint_data.get('processed_segments', [])),
                'total_count': metadata.get('total_segments', 0),
                'last_save_time': metadata.get('last_save_time', ''),
                'video_path': metadata.get('video_path', ''),
                'config_hash': metadata.get('config_hash', '')
            }
        except Exception as e:
            log_error(f"[æ£€æŸ¥ç‚¹] è¯»å–æ£€æŸ¥ç‚¹ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def save_checkpoint(self, processed_segments, metadata, current_index):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            # å¤‡ä»½ç°æœ‰æ£€æŸ¥ç‚¹
            if self.has_checkpoint():
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_checkpoint = os.path.join(self.backup_dir, f"checkpoint_{timestamp}.json")
                backup_metadata = os.path.join(self.backup_dir, f"metadata_{timestamp}.json")
                
                try:
                    shutil.copy2(self.checkpoint_file, backup_checkpoint)
                    shutil.copy2(self.metadata_file, backup_metadata)
                except:
                    pass  # å¤‡ä»½å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            
            # æ›´æ–°å…ƒæ•°æ®
            metadata.update({
                'last_save_time': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'current_index': current_index,
                'processed_count': len(processed_segments)
            })
            
            # ä¿å­˜æ£€æŸ¥ç‚¹æ•°æ®
            checkpoint_data = {
                'processed_segments': processed_segments,
                'current_index': current_index,
                'save_time': time.time()
            }
            
            # åŸå­å†™å…¥ï¼ˆå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½åï¼‰
            temp_checkpoint = self.checkpoint_file + ".tmp"
            temp_metadata = self.metadata_file + ".tmp"
            
            with open(temp_checkpoint, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            
            with open(temp_metadata, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # åŸå­é‡å‘½å
            os.replace(temp_checkpoint, self.checkpoint_file)
            os.replace(temp_metadata, self.metadata_file)
            
            log_info(f"[æ£€æŸ¥ç‚¹] å·²ä¿å­˜æ£€æŸ¥ç‚¹: {len(processed_segments)}/{metadata.get('total_segments', 0)} ç‰‡æ®µ")
            return True
            
        except Exception as e:
            log_error(f"[æ£€æŸ¥ç‚¹] ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False
    
    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not self.has_checkpoint():
            return None, None
        
        try:
            with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            processed_segments = checkpoint_data.get('processed_segments', [])
            current_index = checkpoint_data.get('current_index', 0)
            
            log_info(f"[æ£€æŸ¥ç‚¹] å·²åŠ è½½æ£€æŸ¥ç‚¹: {len(processed_segments)} ä¸ªå·²å¤„ç†ç‰‡æ®µï¼Œä»ç¬¬ {current_index} ä¸ªç»§ç»­")
            return processed_segments, metadata
            
        except Exception as e:
            log_error(f"[æ£€æŸ¥ç‚¹] åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return None, None
    
    def clear_checkpoint(self):
        """æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        try:
            if os.path.exists(self.checkpoint_file):
                os.remove(self.checkpoint_file)
            if os.path.exists(self.metadata_file):
                os.remove(self.metadata_file)
            log_info("[æ£€æŸ¥ç‚¹] æ£€æŸ¥ç‚¹æ–‡ä»¶å·²æ¸…ç†")
        except Exception as e:
            log_error(f"[æ£€æŸ¥ç‚¹] æ¸…ç†æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def create_metadata(self, video_path, transcription_file, chat_file, config_params):
        """åˆ›å»ºå…ƒæ•°æ®"""
        config_hash = hashlib.md5(json.dumps(config_params, sort_keys=True).encode()).hexdigest()
        
        return {
            'video_path': video_path,
            'transcription_file': transcription_file,
            'chat_file': chat_file,
            'config_hash': config_hash,
            'config_params': config_params,
            'start_time': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'total_segments': 0  # å°†åœ¨å¤„ç†æ—¶æ›´æ–°
        }
    
    def is_config_compatible(self, current_config, saved_metadata):
        """æ£€æŸ¥é…ç½®æ˜¯å¦å…¼å®¹"""
        if not saved_metadata:
            return False
        
        current_hash = hashlib.md5(json.dumps(current_config, sort_keys=True).encode()).hexdigest()
        saved_hash = saved_metadata.get('config_hash', '')
        
        return current_hash == saved_hash

# å…¨å±€æ£€æŸ¥ç‚¹ç®¡ç†å™¨
checkpoint_manager = CheckpointManager()

# ============================================================================
# ğŸš€ è¶…å¿«ç‰¹å¾æå–å™¨ - è§£å†³27ç§’/ç‰‡æ®µçš„é—®é¢˜
# ============================================================================

class UltraFastExtractor:
    """è¶…å¿«ç‰¹å¾æå–å™¨ - ä¸€æ¬¡æ€§åŠ è½½éŸ³é¢‘ï¼Œé¢„è®¡ç®—æ‰€æœ‰ç‰¹å¾"""
    
    def __init__(self, video_path, max_workers=4):
        self.video_path = video_path
        self.max_workers = max_workers
        
        log_info("ğŸš€ [è¶…å¿«æå–å™¨] åˆå§‹åŒ–...")
        start_time = time.time()
        
        try:
            # å…³é”®ä¼˜åŒ–1ï¼šä¸€æ¬¡æ€§åŠ è½½å®Œæ•´éŸ³é¢‘æ–‡ä»¶åˆ°å†…å­˜
            log_info("ğŸ”„ [è¶…å¿«æå–å™¨] é¢„åŠ è½½å®Œæ•´éŸ³é¢‘æ–‡ä»¶...")
            self.full_audio, self.sr = librosa.load(video_path, sr=22050)
            self.duration = len(self.full_audio) / self.sr
            load_time = time.time() - start_time
            log_info(f"âœ… [è¶…å¿«æå–å™¨] éŸ³é¢‘å·²é¢„åŠ è½½: {self.duration:.1f}s, è€—æ—¶: {load_time:.1f}s")
            
            # å…³é”®ä¼˜åŒ–2ï¼šé¢„è®¡ç®—æ•´ä¸ªéŸ³é¢‘çš„é¢‘è°±ç‰¹å¾
            log_info("ğŸ”„ [è¶…å¿«æå–å™¨] é¢„è®¡ç®—å…¨å±€éŸ³é¢‘ç‰¹å¾...")
            self._precompute_global_features()
            
            total_time = time.time() - start_time
            log_info(f"âœ… [è¶…å¿«æå–å™¨] åˆå§‹åŒ–å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.1f}s")
            
        except Exception as e:
            log_error(f"âŒ [è¶…å¿«æå–å™¨] åˆå§‹åŒ–å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•æ¨¡å¼
            self.full_audio = None
            self.use_fallback = True
    
    def _precompute_global_features(self):
        """é¢„è®¡ç®—æ•´ä¸ªéŸ³é¢‘çš„ç‰¹å¾ï¼Œé¿å…é‡å¤è®¡ç®—"""
        try:
            hop_length = 512
            n_fft = 1024
            
            # è®¡ç®—æ•´ä¸ªéŸ³é¢‘çš„STFTï¼ˆæœ€è€—æ—¶çš„æ“ä½œï¼‰
            self.stft = librosa.stft(self.full_audio, hop_length=hop_length, n_fft=n_fft)
            self.magnitude = np.abs(self.stft)
            
            # é¢„è®¡ç®—å¸¸ç”¨ç‰¹å¾
            self.spectral_flatness = librosa.feature.spectral_flatness(S=self.magnitude)[0]
            self.zero_crossing_rate = librosa.feature.zero_crossing_rate(
                self.full_audio, hop_length=hop_length
            )[0]
            self.rms_energy = librosa.feature.rms(y=self.full_audio, hop_length=hop_length)[0]
            
            # æ—¶é—´è½´è½¬æ¢
            self.feature_times = librosa.frames_to_time(
                np.arange(len(self.spectral_flatness)), 
                sr=self.sr, hop_length=hop_length
            )
            
            log_info(f"ğŸ”¢ [è¶…å¿«æå–å™¨] é¢„è®¡ç®—ç‰¹å¾ç»´åº¦: {len(self.spectral_flatness)} å¸§")
            self.use_fallback = False
            
        except Exception as e:
            log_error(f"âŒ [è¶…å¿«æå–å™¨] é¢„è®¡ç®—å¤±è´¥: {e}")
            self.use_fallback = True
    
    def extract_music_features_optimized(self, start_sec, end_sec):
        """ä¼˜åŒ–çš„éŸ³ä¹ç‰¹å¾æå– - ä»é¢„è®¡ç®—ç»“æœä¸­åˆ‡ç‰‡"""
        if hasattr(self, 'use_fallback') and self.use_fallback:
            return self._fallback_music_features(start_sec, end_sec)
        
        try:
            # æ‰¾åˆ°å¯¹åº”çš„ç‰¹å¾å¸§ç´¢å¼•
            start_frame = np.searchsorted(self.feature_times, start_sec)
            end_frame = np.searchsorted(self.feature_times, end_sec)
            
            if start_frame >= end_frame or end_frame > len(self.feature_times):
                return 0.0
            
            # ç›´æ¥ä»é¢„è®¡ç®—ç‰¹å¾ä¸­åˆ‡ç‰‡ (æ¯«ç§’çº§æ“ä½œ!)
            segment_spectral_flatness = self.spectral_flatness[start_frame:end_frame]
            segment_zcr = self.zero_crossing_rate[start_frame:end_frame]
            
            # å¿«é€Ÿç»Ÿè®¡è®¡ç®—
            if len(segment_spectral_flatness) == 0 or len(segment_zcr) == 0:
                return 0.0
                
            spectral_flatness_mean = np.mean(segment_spectral_flatness)
            zcr_mean = np.mean(segment_zcr)
            
            # éŸ³ä¹æ¦‚ç‡è®¡ç®—
            music_prob = spectral_flatness_mean * 0.6 + (1 - zcr_mean) * 0.4
            return float(np.clip(music_prob, 0, 1))
            
        except Exception as e:
            log_debug(f"éŸ³ä¹ç‰¹å¾æå–å¤±è´¥ {start_sec}-{end_sec}: {e}")
            return 0.0
    
    def extract_volume_features_optimized(self, start_sec, end_sec):
        """ä¼˜åŒ–çš„éŸ³é‡ç‰¹å¾æå– - ä»é¢„è®¡ç®—ç»“æœä¸­åˆ‡ç‰‡"""
        if hasattr(self, 'use_fallback') and self.use_fallback:
            return self._fallback_volume_features(start_sec, end_sec)
        
        try:
            # æ‰¾åˆ°å¯¹åº”çš„ç‰¹å¾å¸§ç´¢å¼•
            start_frame = np.searchsorted(self.feature_times, start_sec)
            end_frame = np.searchsorted(self.feature_times, end_sec)
            
            if start_frame >= end_frame or end_frame > len(self.feature_times):
                return -100.0
            
            # ç›´æ¥ä»é¢„è®¡ç®—ç‰¹å¾ä¸­åˆ‡ç‰‡
            segment_rms = self.rms_energy[start_frame:end_frame]
            
            if len(segment_rms) == 0:
                return -100.0
            
            rms_mean = np.mean(segment_rms)
            
            if rms_mean > 0:
                db_value = 20 * np.log10(rms_mean)
                return float(max(db_value, -100.0))
            else:
                return -100.0
                
        except Exception as e:
            log_debug(f"éŸ³é‡è®¡ç®—å¤±è´¥: {e}")
            return -100.0
    
    def _fallback_music_features(self, start_sec, end_sec):
        """å›é€€æ–¹æ³• - ç®€å•ä¼°ç®—"""
        return 0.3  # é»˜è®¤ä¸­ç­‰éŸ³ä¹æ¦‚ç‡
    
    def _fallback_volume_features(self, start_sec, end_sec):
        """å›é€€æ–¹æ³• - ç®€å•ä¼°ç®—"""
        return -30.0  # é»˜è®¤éŸ³é‡
    
    def batch_extract_features(self, segments_batch):
        """æ‰¹é‡æå–ç‰¹å¾ - è¶…å¿«ç‰ˆæœ¬"""
        if not segments_batch:
            return []
        
        results = []
        
        if hasattr(self, 'use_fallback') and self.use_fallback:
            # å›é€€æ¨¡å¼
            for seg_info in segments_batch:
                results.append({
                    'music_probability': 0.3,
                    'loud_db': -30.0
                })
            return results
        
        try:
            # å‘é‡åŒ–æ‰¹é‡å¤„ç†
            starts = np.array([seg['start'] for seg in segments_batch])
            ends = np.array([seg['end'] for seg in segments_batch])
            
            # æ‰¹é‡æŸ¥æ‰¾å¸§ç´¢å¼•
            start_frames = np.searchsorted(self.feature_times, starts)
            end_frames = np.searchsorted(self.feature_times, ends)
            
            for i, (start_frame, end_frame) in enumerate(zip(start_frames, end_frames)):
                if start_frame >= end_frame or end_frame > len(self.feature_times):
                    results.append({'music_probability': 0.0, 'loud_db': -100.0})
                    continue
                
                # ç›´æ¥åˆ‡ç‰‡è®¡ç®—ï¼ˆè¶…å¿«ï¼ï¼‰
                sf_segment = self.spectral_flatness[start_frame:end_frame]
                zcr_segment = self.zero_crossing_rate[start_frame:end_frame]
                rms_segment = self.rms_energy[start_frame:end_frame]
                
                # å¿«é€Ÿç»Ÿè®¡
                sf_mean = np.mean(sf_segment) if len(sf_segment) > 0 else 0
                zcr_mean = np.mean(zcr_segment) if len(zcr_segment) > 0 else 0
                rms_mean = np.mean(rms_segment) if len(rms_segment) > 0 else 0
                
                music_prob = np.clip(sf_mean * 0.6 + (1 - zcr_mean) * 0.4, 0, 1)
                loud_db = 20 * np.log10(rms_mean) if rms_mean > 0 else -100.0
                loud_db = max(loud_db, -100.0)
                
                results.append({
                    'music_probability': float(music_prob),
                    'loud_db': float(loud_db)
                })
            
            return results
            
        except Exception as e:
            log_error(f"æ‰¹é‡ç‰¹å¾æå–å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•ç»“æœ
            return [
                {'music_probability': 0.3, 'loud_db': -30.0} 
                for _ in segments_batch
            ]

def ultra_fast_parallel_extraction(feature_extractor, all_segments, max_workers=4, 
                                  checkpoint_interval=10, progress_callback=None):
    """è¶…å¿«å¹¶è¡Œç‰¹å¾æå– - æ›¿æ¢åŸæ¥çš„æ…¢é€Ÿç‰ˆæœ¬"""
    log_info(f"âš¡ [è¶…å¿«å¹¶è¡Œ] å¼€å§‹è¶…å¿«ç‰¹å¾æå–: {len(all_segments)} ä¸ªç‰‡æ®µ")
    
    start_time = time.time()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯è¶…å¿«æå–å™¨
    if isinstance(feature_extractor, UltraFastExtractor):
        # ä½¿ç”¨è¶…å¿«æå–å™¨ï¼Œä¸éœ€è¦å¤æ‚çš„å¹¶è¡Œå¤„ç†
        batch_size = 100  # æ‰¹æ¬¡å¤„ç†
        all_features = []
        processed_count = 0
        
        if TQDM_AVAILABLE:
            progress_bar = tqdm(total=len(all_segments), desc="âš¡è¶…å¿«ç‰¹å¾æå–", unit="seg")
        
        for i in range(0, len(all_segments), batch_size):
            batch = all_segments[i:i + batch_size]
            batch_features = feature_extractor.batch_extract_features(batch)
            all_features.extend(batch_features)
            processed_count += len(batch)
            
            if TQDM_AVAILABLE:
                progress_bar.update(len(batch))
            
            # å‘é€è¿›åº¦å›è°ƒ
            if progress_callback:
                try:
                    progress_callback("âš¡è¶…å¿«ç‰¹å¾è®¡ç®—", processed_count, len(all_segments), 
                                    f"å·²å¤„ç† {processed_count}/{len(all_segments)} ä¸ªç‰‡æ®µ")
                except:
                    pass
        
        if TQDM_AVAILABLE:
            progress_bar.close()
        
        elapsed = time.time() - start_time
        speed = len(all_segments) / elapsed if elapsed > 0 else float('inf')
        log_info(f"âš¡ [è¶…å¿«å¹¶è¡Œ] å®Œæˆ! è€—æ—¶: {elapsed:.1f}s, é€Ÿåº¦: {speed:.0f} ç‰‡æ®µ/ç§’")
        
        return all_features
    
    else:
        # å›é€€åˆ°åŸæ¥çš„æ–¹æ³•
        log_info("ğŸ”„ [è¶…å¿«å¹¶è¡Œ] å›é€€åˆ°æ ‡å‡†å¹¶è¡Œå¤„ç†...")
        return parallel_feature_extraction_with_checkpoint_original(
            feature_extractor, all_segments, max_workers, checkpoint_interval, progress_callback
        )

# ä¿ç•™åŸæ¥çš„å¹¶è¡Œå¤„ç†å‡½æ•°ä½œä¸ºå¤‡ç”¨
def parallel_feature_extraction_with_checkpoint_original(feature_extractor, all_segments, max_workers=4, 
                                                        checkpoint_interval=10, progress_callback=None):
    """åŸæ¥çš„å¹¶è¡Œç‰¹å¾æå–å‡½æ•° - ä½œä¸ºå¤‡ç”¨"""
    log_info(f"[æ ‡å‡†å¹¶è¡Œ] å¼€å§‹æ ‡å‡†å¹¶è¡Œç‰¹å¾æå–ï¼Œä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹")
    
    batch_size = max(1, len(all_segments) // (max_workers * 2))
    batches = [all_segments[i:i + batch_size] for i in range(0, len(all_segments), batch_size)]
    
    all_features = []
    processed_count = 0
    
    if TQDM_AVAILABLE:
        progress_bar = tqdm(total=len(all_segments), desc="æ ‡å‡†ç‰¹å¾æå–", unit="seg")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_batch = {
            executor.submit(feature_extractor.batch_extract_features, batch): (i, batch) 
            for i, batch in enumerate(batches)
        }
        
        for future in concurrent.futures.as_completed(future_to_batch):
            batch_index, batch = future_to_batch[future]
            try:
                batch_features = future.result()
                all_features.extend(batch_features)
                processed_count += len(batch)
                
                if TQDM_AVAILABLE:
                    progress_bar.update(len(batch))
                
                # å‘é€è¿›åº¦å›è°ƒ
                if progress_callback:
                    try:
                        progress_callback("ç‰¹å¾è®¡ç®—", processed_count, len(all_segments), 
                                        f"å·²å¤„ç† {processed_count}/{len(all_segments)} ä¸ªç‰‡æ®µ")
                    except:
                        pass
                        
            except Exception as e:
                log_error(f"[æ ‡å‡†å¹¶è¡Œ] æ‰¹æ¬¡ {batch_index} å¤„ç†å¤±è´¥: {e}")
                all_features.extend([
                    {'music_probability': 0.0, 'loud_db': -100.0} 
                    for _ in batch
                ])
                
                if TQDM_AVAILABLE:
                    progress_bar.update(len(batch))
    
    if TQDM_AVAILABLE:
        progress_bar.close()
    
    log_info(f"[æ ‡å‡†å¹¶è¡Œ] æ ‡å‡†å¹¶è¡Œç‰¹å¾æå–å®Œæˆï¼Œå¤„ç†äº† {len(all_features)} ä¸ªç‰‡æ®µ")
    return all_features

# å…¼å®¹åŸæ¥å£çš„å‡½æ•°åˆ«å
parallel_feature_extraction_with_checkpoint = ultra_fast_parallel_extraction

# ============================================================================
# ä¼˜åŒ–çš„æ–‡æœ¬åˆ†æå™¨
# ============================================================================

class OptimizedTextAnalyzer:
    """ä¼˜åŒ–çš„æ–‡æœ¬åˆ†æå™¨ - å‡å°‘GPUæ¨¡å‹é‡å¤åŠ è½½"""

    def __init__(self, device=None):
        self.device = device
        self.model_loaded = False
        self.sentiment_pipeline = None

        # å°è¯•é¢„åŠ è½½GPUæ¨¡å‹
        if device and torch and GPU_AVAILABLE:
            self._try_load_gpu_model()

    def _try_load_gpu_model(self):
        """å°è¯•é¢„åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆä¼˜å…ˆGPUï¼Œå¤±è´¥åˆ™CPUï¼‰"""
        try:
            log_info("ğŸ”„ [æ–‡æœ¬åˆ†æå™¨] é¢„åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...")

            device_id = self.device.index if self.device and getattr(self.device, 'type', None) == 'cuda' else -1
            stable_model = "cardiffnlp/twitter-roberta-base-sentiment-latest"

            try:
                self.sentiment_pipeline = pipeline(
                    "sentiment-analysis",
                    model=stable_model,
                    device=device_id,
                    batch_size=16,
                    truncation=True,
                    max_length=128,
                    return_all_scores=False,
                )
            except Exception:
                log_info("ğŸ”„ [æ–‡æœ¬åˆ†æå™¨] åˆ‡æ¢ä¸ºCPUåŠ è½½...")
                self.sentiment_pipeline = pipeline(
                    "sentiment-analysis",
                    model=stable_model,
                    device=-1,
                    batch_size=8,
                    truncation=True,
                    max_length=128,
                )

            self.model_loaded = True
            log_info("âœ… [æ–‡æœ¬åˆ†æå™¨] æ¨¡å‹å·²é¢„åŠ è½½")
        except Exception as e:
            log_error(f"âš ï¸ [æ–‡æœ¬åˆ†æå™¨] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.model_loaded = False

# ============================================================================
# ACFV å…¼å®¹å¯¼å‡ºï¼ˆæ¨¡å—çº§å‡½æ•°ï¼‰
# ============================================================================

def _write_acfv_exports(output_dir, ratings_data, selected_segments):
    """å†™å‡ºä¸ ACFV é£æ ¼å…¼å®¹çš„å¯¼å‡ºæ–‡ä»¶ã€‚

    è¾“å‡º:
    - acfv_ratings.jsonl: æ¯è¡Œä¸€ä¸ªç‰‡æ®µï¼ŒåŒ…å« start/end/score/text/file
    - acfv_selected.json: é€‰æ‹©çš„ç‰‡æ®µåˆ—è¡¨ clipsï¼š[...]
    """
    try:
        os.makedirs(output_dir, exist_ok=True)

        # 1) å…¨é‡è¯„åˆ† JSONLï¼ˆä¾¿äºå¤–éƒ¨å·¥å…·æ¶ˆè´¹ï¼‰
        acfv_ratings_path = os.path.join(output_dir, 'acfv_ratings.jsonl')
        with open(acfv_ratings_path, 'w', encoding='utf-8') as f:
            for file_name, data in ratings_data.items():
                rec = {
                    'file': file_name,
                    'start': float(data.get('start', 0.0)),
                    'end': float(data.get('end', 0.0)),
                    'duration': float(data.get('duration', 0.0)),
                    'score': float(data.get('rating', 0.0)),
                    'text': data.get('text', ''),
                }
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")

        # 2) å·²é€‰ç‰‡æ®µ JSONï¼ˆTop-Nï¼‰
        acfv_selected_path = os.path.join(output_dir, 'acfv_selected.json')
        clips = []
        for i, seg in enumerate(selected_segments, 1):
            s = float(seg.get('start', 0.0))
            e = float(seg.get('end', 0.0))
            name = f"clip_{i:03d}_{s:.1f}s-{e:.1f}s.mp4"
            clips.append({
                'index': i,
                'file': name,
                'start': s,
                'end': e,
                'duration': max(0.0, e - s),
                'score': float(seg.get('score', 0.0)),
                'text': seg.get('text', ''),
            })
        with open(acfv_selected_path, 'w', encoding='utf-8') as f:
            json.dump({'clips': clips, 'count': len(clips)}, f, ensure_ascii=False, indent=2)

        log_info(f"âœ… ACFVå…¼å®¹å¯¼å‡ºå®Œæˆ: {acfv_ratings_path}, {acfv_selected_path}")
    except Exception as e:
        log_error(f"âŒ å†™å…¥ ACFV å…¼å®¹å¯¼å‡ºå¤±è´¥: {e}")

# å…¨å±€æ–‡æœ¬åˆ†æå™¨å®ä¾‹
_global_text_analyzer = None

def get_text_analyzer(device=None):
    """è·å–å…¨å±€æ–‡æœ¬åˆ†æå™¨å®ä¾‹"""
    global _global_text_analyzer
    if _global_text_analyzer is None:
        _global_text_analyzer = OptimizedTextAnalyzer(device)
    return _global_text_analyzer

# ============================================================================
# åŸºç¡€å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
# ============================================================================

def load_gui_config():
    """ä»GUIé…ç½®æ–‡ä»¶åŠ è½½é…ç½® (settings/config.json)"""
    config = {
        "WHISPER_MODEL": "small",
        "GPU_DEVICE": "cuda:0",
        "ENABLE_GPU_ACCELERATION": True,
        "LLM_DEVICE": 0,
        "CHAT_DENSITY_WEIGHT": 0.3,
        "CHAT_SENTIMENT_WEIGHT": 0.4,
        "VIDEO_EMOTION_WEIGHT": 0.3,
        "AUDIO_TARGET_BONUS": 1.0,
        "TEXT_TARGET_BONUS": 1.0,
        "INTEREST_SCORE_THRESHOLD": 0.5,
        "LOCAL_EMOTION_MODEL_PATH": "",
        "VIDEO_EMOTION_MODEL_PATH": "",
        "VIDEO_EMOTION_SEGMENT_LENGTH": 4.0,
        "ENABLE_VIDEO_EMOTION": False,
        "MAX_CLIP_COUNT": 10,
        "CLIPS_BASE_DIR": "clips",
        "MAX_WORKERS": 4,
    }
    config_file = settings_path("config.json")
    try:
        if config_file.exists():
            with config_file.open("r", encoding="utf-8") as f:
                gui_config = json.load(f)
            config.update(gui_config)
    except Exception as e:
        log_error(f"[é…ç½®] åŠ è½½GUIé…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")

    return config

def write_progress_file(stage, current, total, message=""):
    """å†™å…¥è¿›åº¦æ–‡ä»¶ï¼Œä¾›GUIè¯»å–è¿›åº¦ä¿¡æ¯"""
    try:
        if str(os.environ.get("ACFV_DISABLE_PROGRESS_FILE", "")).lower() in ("1", "true", "yes"):
            return
        progress_file = processing_path("analysis_progress.json")
        progress_file.parent.mkdir(parents=True, exist_ok=True)

        progress_data = {
            "stage": stage,
            "current": current,
            "total": total,
            "message": message,
            "timestamp": time.time(),
            "percentage": (current / total * 100) if total > 0 else 0,
        }

        with progress_file.open("w", encoding="utf-8") as f:
            json.dump(progress_data, f, ensure_ascii=False)
    except Exception as e:
        log_error(f"[è¿›åº¦æ–‡ä»¶] å†™å…¥å¤±è´¥: {e}")

def get_optimal_device(device_preference=None):
    """è·å–æœ€ä¼˜è®¾å¤‡"""
    config = load_gui_config()
    
    if device_preference and torch:
        if isinstance(device_preference, str):
            wants_cuda = device_preference.strip().lower().startswith("cuda")
        else:
            wants_cuda = torch and isinstance(device_preference, torch.device) and device_preference.type == "cuda"
        if wants_cuda and not torch.cuda.is_available():
            log_warning("[è®¾å¤‡ç®¡ç†] CUDA ä¸å¯ç”¨ï¼Œæ”¹ç”¨ CPU")
        else:
            try:
                if isinstance(device_preference, str):
                    device = torch.device(device_preference)
                else:
                    device = device_preference
                test_tensor = torch.tensor([1.0]).to(device)
                del test_tensor
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
                return device
            except Exception as e:
                log_error(f"[è®¾å¤‡ç®¡ç†] æŒ‡å®šè®¾å¤‡ä¸å¯ç”¨: {e}")
    
    try:
        gui_device = config.get("GPU_DEVICE", "cuda:0")
        enable_gpu = config.get("ENABLE_GPU_ACCELERATION", True)
        
        if enable_gpu and gui_device != "cpu" and torch and torch.cuda.is_available():
            device = torch.device(gui_device)
            test_tensor = torch.tensor([1.0]).to(device)
            del test_tensor
            torch.cuda.empty_cache()
            log_info(f"[è®¾å¤‡ç®¡ç†] ä½¿ç”¨GPUè®¾å¤‡: {device}")
            return device
        if enable_gpu and gui_device != "cpu" and torch and not torch.cuda.is_available():
            log_warning("[è®¾å¤‡ç®¡ç†] CUDA ä¸å¯ç”¨ï¼Œæ”¹ç”¨ CPU")
    except Exception as e:
        log_error(f"[è®¾å¤‡ç®¡ç†] GPUè®¾å¤‡ä¸å¯ç”¨: {e}")
    
    return torch.device("cpu") if torch else None

def get_video_path():
    """å®‰å…¨åœ°è·å–è§†é¢‘è·¯å¾„"""
    video_path_file = processing_path("selected_video.txt")
    
    if not video_path_file.exists():
        log_error(f"[è§†é¢‘è·¯å¾„] è§†é¢‘è·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨: {video_path_file}")
        return None
        
    try:
        with open(video_path_file, 'r', encoding='utf-8') as f:
            video_path = f.read().strip()
            
        if not os.path.exists(video_path):
            log_error(f"[è§†é¢‘è·¯å¾„] è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return None
            
        log_info(f"[è§†é¢‘è·¯å¾„] è§†é¢‘è·¯å¾„: {video_path}")
        return video_path
    except Exception as e:
        log_error(f"[è§†é¢‘è·¯å¾„] è¯»å–å¤±è´¥: {e}")
        return None

def emotion_avg(records, seg_start, seg_end):
    """è®¡ç®—åŒºé—´å†…æƒ…ç»ªåˆ†å€¼çš„æ—¶é—´åŠ æƒå¹³å‡"""
    if not records:
        return 0.0
        
    num = den = 0.0
    for r in records:
        try:
            r_start = float(r.get('start', 0))
            r_end = float(r.get('end', 0))
            r_score = float(r.get('score', 0))
            
            overlap = max(0, min(seg_end, r_end) - max(seg_start, r_start))
            if overlap > 0:
                num += overlap * r_score
                den += overlap
        except (ValueError, TypeError):
            continue
    
    return num / den if den > 0 else 0.0

def vader_interest_score(text):
    """ä½¿ç”¨VADERè¯„ä¼°æ–‡æœ¬çš„æƒ…æ„Ÿå¼ºåº¦ä½œä¸ºå…´è¶£åˆ†æ•°"""
    if not text or not isinstance(text, str) or text.isspace():
        return 0.0
    
    # å»¶è¿Ÿåˆå§‹åŒ–VADER
    init_vader()
    
    if not sid:
        return 0.0
    
    try:
        sentiment_scores = sid.polarity_scores(text)
        interest_intensity = abs(sentiment_scores['compound'])
        emotional_content = sentiment_scores['pos'] + sentiment_scores['neg']
        interest_score = (interest_intensity * 0.7) + (emotional_content * 0.3)
        return min(max(interest_score, 0.0), 1.0)
    except Exception as e:
        log_error(f"[VADER] æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
        return 0.0

def normalize_transcription_data(data):
    """æ ‡å‡†åŒ–è½¬å½•ç»“æ„ä¸ºåŒ…å« start/end/text çš„åˆ—è¡¨"""
    if not data:
        return []

    if isinstance(data, dict):
        for key in ("segments", "chunks", "results", "data"):
            value = data.get(key)
            if isinstance(value, list):
                data = value
                break
        else:
            if all(k in data for k in ("start", "end", "text")):
                data = [data]
            else:
                return []

    if not isinstance(data, list):
        return []

    normalized = []
    for item in data:
        if not isinstance(item, dict):
            continue

        record = dict(item)
        start = record.get("start")
        end = record.get("end")
        text = record.get("text")

        if (start is None or end is None):
            ts = record.get("timestamp")
            if isinstance(ts, (list, tuple)) and len(ts) == 2:
                start, end = ts
        if (start is None or end is None):
            ts = record.get("timestamps")
            if isinstance(ts, (list, tuple)) and len(ts) == 2:
                start, end = ts
        if start is None:
            start = record.get("start_time")
        if end is None:
            end = record.get("end_time")

        if text is None:
            text = (
                record.get("transcript")
                or record.get("sentence")
                or record.get("label")
                or record.get("content")
                or record.get("utterance")
            )

        try:
            start = float(start) if start is not None else None
            end = float(end) if end is not None else None
        except (TypeError, ValueError):
            start = None
            end = None

        if start is None or end is None:
            continue

        if text is None:
            text = ""
        if not isinstance(text, str):
            text = str(text)

        record["start"] = start
        record["end"] = end
        record["text"] = text
        normalized.append(record)

    return normalized

def validate_json_structure(data, is_chat=False):
    """éªŒè¯JSONæ•°æ®ç»“æ„æ˜¯å¦ç¬¦åˆé¢„æœŸ"""
    if not isinstance(data, list):
        return False
    if not data:
        return True
        
    sample_size = min(5, len(data))
    for i in range(sample_size):
        item = data[i]
        if not isinstance(item, dict):
            return False
            
        if is_chat:
            if 'message' not in item or 'timestamp' not in item:
                return False
        else:
            if 'text' not in item or 'start' not in item or 'end' not in item:
                return False
                
    return True

def compute_chat_density(chat_data, start, end):
    """ç»Ÿè®¡æ—¶é—´æ®µå†…çš„èŠå¤©æ¡æ•°"""
    if not chat_data:
        return 0
    count = sum(1 for c in chat_data if start <= float(c.get("timestamp", 0)) <= end)
    return count

def compute_chat_sentiment_strength(chat_data, start, end):
    """è®¡ç®—æ—¶é—´æ®µå†…èŠå¤©æƒ…æ„Ÿå¾—åˆ†ç»å¯¹å€¼çš„å¹³å‡"""
    if not chat_data:
        return 0.0
    scores = [
        abs(c.get("sentiment", {}).get("score", 0))
        for c in chat_data
        if start <= float(c.get("timestamp", 0)) <= end
        and isinstance(c.get("sentiment", {}).get("score"), (int, float))
    ]
    return sum(scores) / len(scores) if scores else 0.0

def compute_relative_interest_score(all_scores, score):
    """å¯¹åˆ†æ•°è¿›è¡Œæ ‡å‡†åŒ–"""
    if not all_scores:
        return 0.0
    
    # å»¶è¿Ÿå¯¼å…¥numpy
    import_heavy_libraries()
    if np is None:
        # å¦‚æœnumpyä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•è®¡ç®—
        if not all_scores:
            return 0.0
        mean = sum(all_scores) / len(all_scores)
        variance = sum((x - mean) ** 2 for x in all_scores) / len(all_scores)
        std = variance ** 0.5
        return (score - mean) / std if std > 0 else score
    
    arr = np.array(all_scores, dtype=float)
    mean = arr.mean() if arr.size else 0.0
    std = arr.std() if arr.size else 0.0
    return (score - mean) / std if std > 0 else score

def load_video_emotion_data(video_emotion_file):
    """åŠ è½½è§†é¢‘æƒ…ç»ªæ•°æ®"""
    if not video_emotion_file or not os.path.exists(video_emotion_file):
        log_info(f"[è§†é¢‘æƒ…ç»ª] è§†é¢‘æƒ…ç»ªæ–‡ä»¶ä¸å­˜åœ¨: {video_emotion_file}")
        return []
        
    try:
        with open(video_emotion_file, 'r', encoding='utf-8') as f:
            emotion_data = json.load(f)
            
        if not isinstance(emotion_data, list):
            log_error(f"[è§†é¢‘æƒ…ç»ª] æƒ…ç»ªæ•°æ®æ ¼å¼æ— æ•ˆ")
            return []
            
        log_info(f"[è§†é¢‘æƒ…ç»ª] å·²åŠ è½½ {len(emotion_data)} æ¡æƒ…ç»ªè®°å½•")
        return emotion_data
        
    except Exception as e:
        log_error(f"[è§†é¢‘æƒ…ç»ª] åŠ è½½å¤±è´¥: {e}")
        return []

def batch_sentiment_analysis(texts, device=None):
    """æ‰¹é‡æƒ…æ„Ÿåˆ†æ - ä¼˜åŒ–ç‰ˆæœ¬"""
    if not texts:
        return []
    
    # è·å–å…¨å±€æ–‡æœ¬åˆ†æå™¨
    text_analyzer = get_text_analyzer(device)
    
    if not torch or not GPU_AVAILABLE or not device or device.type == 'cpu':
        log_info(f"[æƒ…æ„Ÿåˆ†æ] ä½¿ç”¨VADERå¿«é€Ÿåˆ†æ {len(texts)} ä¸ªæ–‡æœ¬")
        return [vader_interest_score(text) for text in texts]
    
    if text_analyzer.model_loaded and text_analyzer.sentiment_pipeline:
        try:
            log_info(f"[GPUæƒ…æ„Ÿåˆ†æ] å¼€å§‹GPUæ‰¹é‡åˆ†æ: {len(texts)}ä¸ªæ–‡æœ¬")
            
            batch_size = 32
            all_scores = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                results = text_analyzer.sentiment_pipeline(batch_texts)
                
                batch_scores = []
                for result in results:
                    if isinstance(result, dict):
                        score = result.get('score', 0.5)
                    elif isinstance(result, list) and result:
                        score = max(r.get('score', 0) for r in result)
                    else:
                        score = 0.5
                    batch_scores.append(score)
                
                all_scores.extend(batch_scores)
            
            log_info(f"[GPUæƒ…æ„Ÿåˆ†æ] GPUæ‰¹é‡åˆ†æå®Œæˆ")
            return all_scores
            
        except Exception as e:
            log_error(f"[GPUæƒ…æ„Ÿåˆ†æ] GPUåˆ†æå¤±è´¥ï¼Œå›é€€åˆ°VADER: {e}")
    else:
        log_info(f"[æƒ…æ„Ÿåˆ†æ] æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨VADERå¿«é€Ÿåˆ†æ")
    
    # å›é€€åˆ°VADER
    return [vader_interest_score(text) for text in texts]

# ============================================================================
# å¸¦æ–­ç‚¹ç»­ä¼ çš„ä¸»è¦æ¥å£ - ä½¿ç”¨è¶…å¿«ç‰¹å¾æå–å™¨
# ============================================================================

def check_and_prompt_resume():
    """æ£€æŸ¥å¹¶æç¤ºæ˜¯å¦æ¢å¤ä¹‹å‰çš„åˆ†æ"""
    if not checkpoint_manager.has_checkpoint():
        return False
    
    checkpoint_info = checkpoint_manager.get_checkpoint_info()
    if not checkpoint_info:
        return False
    
    log_info("=" * 60)
    log_info("ğŸ” å‘ç°æœªå®Œæˆçš„åˆ†æä»»åŠ¡")
    log_info("=" * 60)
    log_info(f"ğŸ“¹ è§†é¢‘æ–‡ä»¶: {os.path.basename(checkpoint_info['video_path'])}")
    log_info(f"ğŸ“Š è¿›åº¦: {checkpoint_info['processed_count']}/{checkpoint_info['total_count']} ç‰‡æ®µ")
    log_info(f"â° ä¸Šæ¬¡ä¿å­˜: {checkpoint_info['last_save_time']}")
    log_info(f"ğŸ’¾ å®Œæˆåº¦: {checkpoint_info['processed_count']/checkpoint_info['total_count']*100:.1f}%")
    log_info("=" * 60)
    
    return True

def analyze_data_with_checkpoint(chat_file, transcription_file, output_file, 
                                video_emotion_file=None, video_emotion_weight=0.3, 
                                top_n=None, enable_video_emotion=None, device='cuda:0',
                                progress_callback=None, resume_checkpoint=None):
    """
    å¸¦æ–­ç‚¹ç»­ä¼ çš„ä¸»è¦åˆ†æå‡½æ•° - ç»Ÿä¸€ä½¿ç”¨5åˆ†é’Ÿå›ºå®šæ¨¡å¼
    """
    log_info("=" * 80)
    log_info("ğŸš€ å¼€å§‹è§†é¢‘å†…å®¹åˆ†æ (è¯­ä¹‰å¯å˜æ—¶é•¿+éé‡å  ä¼˜åŒ–ç‰ˆ)")
    log_info("=" * 80)
    
    start_time = time.time()
    
    # åŠ è½½GUIé…ç½®
    config = load_gui_config()
    
    # ä½¿ç”¨GUIé…ç½®å¡«å……å‚æ•°
    if top_n is None:
        top_n = config.get("MAX_CLIP_COUNT", 10)
    if enable_video_emotion is None:
        enable_video_emotion = config.get("ENABLE_VIDEO_EMOTION", False)
    if video_emotion_weight is None:
        video_emotion_weight = config.get("VIDEO_EMOTION_WEIGHT", 0.3)
    


    max_workers = config.get("MAX_WORKERS", 4)
    checkpoint_interval = config.get("CHECKPOINT_INTERVAL", 10)
    
    # å¯ç”¨è¶…å¿«æ¨¡å¼
    enable_ultra_fast = config.get("ENABLE_ULTRA_FAST", True)
    
    # RAGè®¾ç½®
    rag_enable = bool(config.get("RAG_ENABLE", False))
    rag_weight = float(config.get("RAG_WEIGHT", 0.2))
    rag_db_path = config.get("RAG_DB_PATH", "rag_database.json")
    # å…¼å®¹ï¼šè‹¥é…ç½®ä½¿ç”¨é»˜è®¤æ–‡ä»¶åä½†çœŸå®æ–‡ä»¶ä½äº data/ ç›®å½•ï¼Œåˆ™è‡ªåŠ¨å›é€€
    try:
        if not os.path.isabs(rag_db_path) and not os.path.exists(rag_db_path):
            # å¸¸è§å€™é€‰
            acfv_root = Path(__file__).resolve().parents[2]
            candidates = [
                os.path.join("data", "rag_database.json"),
                str(acfv_root / "data" / "rag_database.json"),
            ]
            for c in candidates:
                if os.path.exists(c):
                    log_info(f"[RAG] æ£€æµ‹åˆ°å€™é€‰æ•°æ®åº“: {c} (åŸè·¯å¾„ {rag_db_path} ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨é‡‡ç”¨)")
                    rag_db_path = c
                    break
    except Exception:
        pass
    rag_db = None
    if rag_enable:
        try:
            from acfv.rag_vector_database import RAGVectorDatabase
            rag_dir = os.path.dirname(os.path.abspath(rag_db_path))
            if rag_dir and not os.path.exists(rag_dir):
                os.makedirs(rag_dir, exist_ok=True)
            rag_db = RAGVectorDatabase(database_path=rag_db_path)
            log_info(f"[RAG] å·²å¯ç”¨ï¼Œæ•°æ®åº“: {rag_db_path}, æƒé‡: {rag_weight}")
        except Exception as e:
            log_warning(f"[RAG] å¯ç”¨å¤±è´¥ï¼Œå°†å¿½ç•¥RAGåŠ æˆ: {e}")
            rag_enable = False

    # è·å–æœ€ä¼˜è®¾å¤‡
    optimal_device = get_optimal_device(device)
    
    # åˆ›å»ºå½“å‰é…ç½®çš„å…ƒæ•°æ®
    current_config = {
        'video_emotion_weight': video_emotion_weight,
        'top_n': top_n,
        'enable_video_emotion': enable_video_emotion,


        'max_workers': max_workers
    }
    
    video_path = get_video_path()
    if not video_path:
        log_error("âŒ æ— æ³•è·å–æœ‰æ•ˆçš„è§†é¢‘è·¯å¾„")
        return []
    
    # æ£€æŸ¥æ˜¯å¦æ¢å¤æ£€æŸ¥ç‚¹
    should_resume = False
    processed_segments = []
    start_index = 0
    
    if resume_checkpoint is None:
        # è‡ªåŠ¨æ£€æµ‹
        should_resume = checkpoint_manager.has_checkpoint()
    elif resume_checkpoint:
        # å¼ºåˆ¶æ¢å¤
        should_resume = checkpoint_manager.has_checkpoint()
    else:
        # å¼ºåˆ¶é‡æ–°å¼€å§‹
        checkpoint_manager.clear_checkpoint()
        should_resume = False
    
    if should_resume:
        loaded_segments, saved_metadata = checkpoint_manager.load_checkpoint()
        
        if loaded_segments and saved_metadata:
            # æ£€æŸ¥é…ç½®å…¼å®¹æ€§
            if checkpoint_manager.is_config_compatible(current_config, saved_metadata):
                processed_segments = loaded_segments
                start_index = saved_metadata.get('current_index', 0)
                log_info(f"âœ… æ¢å¤æ£€æŸ¥ç‚¹: ä»ç¬¬ {start_index} ä¸ªç‰‡æ®µç»§ç»­ï¼Œå·²å®Œæˆ {len(processed_segments)} ä¸ª")
            else:
                log_info("âš ï¸ é…ç½®å·²æ›´æ”¹ï¼Œé‡æ–°å¼€å§‹åˆ†æ")
                checkpoint_manager.clear_checkpoint()
                should_resume = False
        else:
            log_error("âŒ æ£€æŸ¥ç‚¹æ•°æ®æŸåï¼Œé‡æ–°å¼€å§‹åˆ†æ")
            checkpoint_manager.clear_checkpoint()
            should_resume = False
    
    log_info(f"ğŸ“‹ åˆ†æå‚æ•°:")
    log_info(f"   - æ¢å¤æ¨¡å¼: {'âœ… ç»§ç»­ä¹‹å‰çš„åˆ†æ' if should_resume else 'ğŸ†• é‡æ–°å¼€å§‹'}")
    log_info(f"   - è®¡ç®—è®¾å¤‡: {optimal_device}")

    log_info(f"   - æ£€æŸ¥ç‚¹é—´éš”: æ¯ {checkpoint_interval} ä¸ªç‰‡æ®µ")
    log_info(f"   - å¹¶è¡Œå·¥ä½œæ•°: {max_workers}")
    
    write_progress_file("åˆå§‹åŒ–", 0, 10, "å¼€å§‹è§†é¢‘å†…å®¹åˆ†æ...")
    
    try:
        # ç¬¬1é˜¶æ®µï¼šæ•°æ®åŠ è½½å’ŒéªŒè¯
        write_progress_file("æ•°æ®åŠ è½½", 1, 10, "åŠ è½½è½¬å½•å’Œå¼¹å¹•æ•°æ®...")
        
        # æ£€æŸ¥å¼¹å¹•æ–‡ä»¶
        has_chat = os.path.exists(chat_file) and os.path.getsize(chat_file) > 10
        log_info(f"ğŸ“º å¼¹å¹•æ–‡ä»¶: {'âœ… å­˜åœ¨' if has_chat else 'âŒ ä¸å­˜åœ¨'}")
        
        # åŠ è½½è½¬å½•æ•°æ®
        try:
            with open(transcription_file, 'r', encoding='utf-8') as f:
                transcription_raw = json.load(f)
            transcription_data = normalize_transcription_data(transcription_raw)
            log_info(f"ğŸ¤ è½¬å½•æ•°æ®: âœ… å·²åŠ è½½ {len(transcription_data)} ä¸ªç‰‡æ®µ")
            
            if not validate_json_structure(transcription_data, is_chat=False):
                log_error("âŒ è½¬å½•æ•°æ®ç»“æ„æ— æ•ˆ")
                return []
            if not transcription_data:
                try:
                    file_size = os.path.getsize(transcription_file)
                except Exception:
                    file_size = -1
                log_warning(f"âš ï¸ è½¬å½•æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ (file_size={file_size} bytes, path={transcription_file})")
                return []
        except Exception as e:
            log_error(f"âŒ è½¬å½•æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return []
        
        # åŠ è½½å¼¹å¹•æ•°æ®
        chat_data = []
        if has_chat:
            try:
                with open(chat_file, 'r', encoding='utf-8') as f:
                    chat_data = json.load(f)
                
                if not validate_json_structure(chat_data, is_chat=True):
                    log_error("âŒ å¼¹å¹•æ•°æ®ç»“æ„æ— æ•ˆ")
                    has_chat = False
                else:
                    log_info(f"ğŸ’¬ å¼¹å¹•æ•°æ®: âœ… å·²åŠ è½½ {len(chat_data)} æ¡å¼¹å¹•")
            except Exception as e:
                log_error(f"âŒ å¼¹å¹•æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                has_chat = False
        
        # åŠ è½½è§†é¢‘æƒ…ç»ªæ•°æ®
        video_emotion_data = []
        has_video_emotion = False
        if enable_video_emotion and video_emotion_file:
            video_emotion_data = load_video_emotion_data(video_emotion_file)
            has_video_emotion = len(video_emotion_data) > 0
        
        if not enable_video_emotion:
            video_emotion_weight = 0.0
        
        log_info(f"ğŸ§  è§†é¢‘æƒ…ç»ªæ•°æ®: {'âœ… å·²åŠ è½½' if has_video_emotion else 'âŒ æ— æ•°æ®'}")
        
        # ç¬¬2é˜¶æ®µï¼šæ•°æ®é¢„å¤„ç†
        write_progress_file("æ•°æ®é¢„å¤„ç†", 2, 10, "å‡†å¤‡ç‰‡æ®µæ•°æ®...")
        
        # å‡†å¤‡æœ‰æ•ˆç‰‡æ®µ
        valid_segments = []
        texts_for_analysis = []
        
        for seg in transcription_data:
            start = float(seg.get('start', 0))
            end = float(seg.get('end', 0))
            text = seg.get('text', '')
            
            if not text or not isinstance(text, str):
                text = "æ— æ–‡æœ¬å†…å®¹"
            
            valid_segments.append({
                'start': start,
                'end': end,
                'text': text
            })
            texts_for_analysis.append(text)
        
        log_info(f"ğŸ“Š æœ‰æ•ˆç‰‡æ®µ: âœ… {len(valid_segments)}ä¸ª")
        
        # éªŒè¯ï¼šç¡®ä¿æ¯ä¸ªåŸå§‹è½¬å½•ç‰‡æ®µéƒ½ä¼šè¢«å¤„ç†
        log_info(f"ğŸ” éªŒè¯: åŸå§‹è½¬å½•æ•°æ®æœ‰ {len(transcription_data)} ä¸ªç‰‡æ®µ")
        log_info(f"ğŸ” éªŒè¯: å‡†å¤‡åˆ†æ {len(valid_segments)} ä¸ªæœ‰æ•ˆç‰‡æ®µ")
        if len(valid_segments) != len(transcription_data):
            log_info(f"âš ï¸  æ³¨æ„: æœ‰æ•ˆç‰‡æ®µæ•°é‡ä¸åŸå§‹æ•°é‡ä¸ä¸€è‡´!")
        
        # åˆ›å»ºæˆ–æ›´æ–°å…ƒæ•°æ®
        if not should_resume:
            metadata = checkpoint_manager.create_metadata(video_path, transcription_file, chat_file, current_config)
            metadata['total_segments'] = len(valid_segments)
        else:
            # ä½¿ç”¨å·²ä¿å­˜çš„å…ƒæ•°æ®
            _, metadata = checkpoint_manager.load_checkpoint()
            metadata.update(current_config)
        
        # ç¬¬3é˜¶æ®µï¼šAIæƒ…æ„Ÿåˆ†æï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not should_resume or start_index == 0:
            write_progress_file("æƒ…æ„Ÿåˆ†æ", 3, 10, "è¿›è¡ŒAIæƒ…æ„Ÿåˆ†æ...")
            
            sentiment_start = time.time()
            interest_scores = batch_sentiment_analysis(texts_for_analysis, optimal_device)
            sentiment_time = time.time() - sentiment_start
            
            log_info(f"ğŸ§  æƒ…æ„Ÿåˆ†æ: âœ… å®Œæˆ {len(interest_scores)} ä¸ªåˆ†æ•°è®¡ç®— (è€—æ—¶: {sentiment_time:.1f}s)")
        else:
            log_info("ğŸ§  æƒ…æ„Ÿåˆ†æ: â­ï¸ è·³è¿‡ï¼ˆä½¿ç”¨ç¼“å­˜ç»“æœï¼‰")
            interest_scores = [0.5] * len(texts_for_analysis)  # å ä½ç¬¦ï¼Œå®é™…ä»æ£€æŸ¥ç‚¹åŠ è½½
        
        # ç¬¬4é˜¶æ®µï¼šğŸš€ è¶…å¿«ç‰¹å¾è®¡ç®—
        write_progress_file("âš¡è¶…å¿«ç‰¹å¾è®¡ç®—", 4, 10, "ä½¿ç”¨è¶…å¿«ç‰¹å¾æå–å™¨...")
        
        feature_start = time.time()
        
        # åªå¤„ç†æœªå®Œæˆçš„ç‰‡æ®µ
        remaining_segments = valid_segments[start_index:]
        log_info(f"ğŸ”„ éœ€è¦å¤„ç†çš„å‰©ä½™ç‰‡æ®µ: {len(remaining_segments)}")
        
        if remaining_segments:
            if enable_ultra_fast:
                # ğŸš€ ä½¿ç”¨è¶…å¿«ç‰¹å¾æå–å™¨
                log_info("âš¡ å¯åŠ¨è¶…å¿«ç‰¹å¾æå–å™¨...")
                feature_extractor = UltraFastExtractor(video_path, max_workers=max_workers)
            else:
                # ä½¿ç”¨æ ‡å‡†æå–å™¨ï¼ˆå·²ç§»é™¤ï¼Œè¿™é‡Œç”¨è¶…å¿«ç‰ˆæœ¬ä½œä¸ºå¤‡ç”¨ï¼‰
                log_info("ğŸ“Š å¯åŠ¨æ ‡å‡†ç‰¹å¾æå–å™¨...")
                feature_extractor = UltraFastExtractor(video_path, max_workers=max_workers)
            
            # è¶…å¿«å¹¶è¡Œæå–éŸ³è§†é¢‘ç‰¹å¾
            remaining_features = ultra_fast_parallel_extraction(
                feature_extractor, remaining_segments, max_workers, 
                checkpoint_interval, progress_callback
            )
        else:
            remaining_features = []
        
        feature_time = time.time() - feature_start
        speed = len(remaining_segments) / feature_time if feature_time > 0 and remaining_segments else 0
        log_info(f"âš¡ è¶…å¿«ç‰¹å¾è®¡ç®—: âœ… å®Œæˆ {len(remaining_features)} ä¸ªæ–°ç‰‡æ®µ (è€—æ—¶: {feature_time:.1f}s, é€Ÿåº¦: {speed:.1f} ç‰‡æ®µ/ç§’)")
        
        # ç¬¬5é˜¶æ®µï¼šå¢é‡åˆ†æ•°è®¡ç®—å¹¶ä¿å­˜æ£€æŸ¥ç‚¹
        write_progress_file("åˆ†æ•°è®¡ç®—", 5, 10, "å¢é‡è®¡ç®—ç»¼åˆå…´è¶£åˆ†æ•°...")
        
        all_segments = processed_segments.copy()  # ä»æ£€æŸ¥ç‚¹æ¢å¤çš„ç‰‡æ®µ
        
        # å¤„ç†å‰©ä½™ç‰‡æ®µ
        for idx, seg_info in enumerate(remaining_segments):
            actual_idx = start_index + idx
            start = seg_info['start']
            end = seg_info['end']
            text = seg_info['text']
            
            # è·å–éŸ³è§†é¢‘ç‰¹å¾
            if idx < len(remaining_features):
                audio_feature = remaining_features[idx]
            else:
                audio_feature = {'music_probability': 0.0, 'loud_db': -100.0}
            
            music_probability = audio_feature['music_probability']
            loud_db = audio_feature['loud_db']
            
            # æƒ…æ„Ÿåˆ†æ•°
            if actual_idx < len(interest_scores):
                interest_score = interest_scores[actual_idx]
            else:
                interest_score = vader_interest_score(text)
            
            # è§†é¢‘æƒ…ç»ªåˆ†æ•°
            vid_emo = 0.0
            if has_video_emotion:
                vid_emo = emotion_avg(video_emotion_data, start, end)
            
            # è®¡ç®—ç»¼åˆåˆ†æ•°
            if has_chat:
                density = compute_chat_density(chat_data, start, end)
                sentiment = compute_chat_sentiment_strength(chat_data, start, end)
                
                music_penalty = 1.0 - music_probability * 0.6
                
                score = (
                    config.get("CHAT_DENSITY_WEIGHT", 0.3) * density +
                    config.get("CHAT_SENTIMENT_WEIGHT", 0.4) * sentiment +
                    config.get("TEXT_TARGET_BONUS", 1.0) * interest_score +
                    video_emotion_weight * vid_emo
                ) * music_penalty
                
                # RAGå…ˆéªŒåŠ æˆï¼ˆåŸºäºå·²æ”¶è—/é«˜è¯„åˆ†åˆ‡ç‰‡çš„ç›¸ä¼¼åº¦ï¼‰
                rag_prior = 0.0
                if rag_enable and rag_db:
                    try:
                        rag_prior = float(rag_db.calculate_similarity_score(text))
                    except Exception:
                        rag_prior = 0.0

                info = {
                    'start': start, 'end': end, 'density': density, 'sentiment': sentiment,
                    'interest_score': interest_score, 'video_emotion': vid_emo,
                    'music_probability': music_probability, 'score': score,
                    'text': text, 'loud_db': loud_db,
                    'rag_prior': rag_prior
                }
            else:
                music_penalty = 1.0 - music_probability * 0.7
                
                score = (
                    interest_score * config.get("TEXT_TARGET_BONUS", 1.0) + 
                    video_emotion_weight * vid_emo
                ) * music_penalty
                
                # RAGå…ˆéªŒåŠ æˆ
                rag_prior = 0.0
                if rag_enable and rag_db:
                    try:
                        rag_prior = float(rag_db.calculate_similarity_score(text))
                    except Exception:
                        rag_prior = 0.0

                info = {
                    'start': start, 'end': end, 'interest_score': interest_score,
                    'video_emotion': vid_emo, 'music_probability': music_probability,
                    'score': score, 'text': text, 'no_chat': True, 'loud_db': loud_db,
                    'rag_prior': rag_prior
                }
            
            # åº”ç”¨RAGæƒé‡åŠ æˆ
            if rag_enable and info.get('rag_prior', 0.0) > 0:
                info['score'] += rag_weight * info['rag_prior']

            # çŸ­æ–‡æœ¬æƒ©ç½š
            word_count = len(text.split())
            if word_count < 5:
                info['score'] *= 0.7
            
            if info['score'] <= 0:
                info['score'] = 0.01
            
            all_segments.append(info)
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (idx + 1) % checkpoint_interval == 0:
                current_index = start_index + idx + 1
                log_info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {len(all_segments)}/{len(valid_segments)} ç‰‡æ®µ")
                checkpoint_manager.save_checkpoint(all_segments, metadata, current_index)
                
                # å‘é€è¿›åº¦å›è°ƒ
                if progress_callback:
                    try:
                        progress_callback("åˆ†æ•°è®¡ç®—", len(all_segments), len(valid_segments), 
                                        f"å·²å®Œæˆ {len(all_segments)}/{len(valid_segments)} ç‰‡æ®µ")
                    except:
                        pass
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        if remaining_segments:
            checkpoint_manager.save_checkpoint(all_segments, metadata, len(valid_segments))
        
        # éŸ³é‡å½’ä¸€åŒ–å’Œæƒ©ç½š
        write_progress_file("éŸ³é‡å¤„ç†", 6, 10, "éŸ³é‡å½’ä¸€åŒ–å¤„ç†...")
        
        loud_vals = [s.get('loud_db', -100.0) for s in all_segments]
        if loud_vals:
            max_loud = max(loud_vals)
            min_loud = min(loud_vals)
            
            for s in all_segments:
                vol_norm = 0.0
                if max_loud > min_loud:
                    vol_norm = (s.get('loud_db', -100.0) - min_loud) / (max_loud - min_loud)
                    
                if s.get('music_probability', 0) > 0.7:
                    penalty = 1.0 - vol_norm * 0.5
                else:
                    penalty = 0.5 + vol_norm ** 2 * 0.5
                    
                s['score'] *= penalty
                s['volume_penalty'] = penalty
        
        # æ›´æ–°åˆ†æ•°å¹¶è®¡ç®—ç›¸å¯¹åˆ†æ•°
        all_scores = [seg.get('score', 0) for seg in all_segments]
        for seg in all_segments:
            seg['relative_score'] = compute_relative_interest_score(all_scores, seg['score'])
        
        log_info(f"ğŸ“ˆ åˆ†æ•°ç»Ÿè®¡: æœ€å°={min(all_scores):.3f}, æœ€å¤§={max(all_scores):.3f}, å¹³å‡={np.mean(all_scores):.3f}")
        
        # ç¬¬6é˜¶æ®µï¼šæ™ºèƒ½è¿‡æ»¤å’Œæ’åºï¼ˆè¯­ä¹‰å¯å˜æ—¶é•¿ï¼Œä¸ä½¿ç”¨å›ºå®š5åˆ†é’Ÿï¼‰
        write_progress_file("æ™ºèƒ½è¿‡æ»¤", 7, 10, "è¿‡æ»¤å’Œæ’åºç‰‡æ®µ...")
        
        # è¾…åŠ©å‡½æ•°ï¼šæŒ‰è¯„åˆ†è´ªå¿ƒæŒ‘é€‰â€œä¸¥æ ¼ä¸é‡å â€ç‰‡æ®µ
        def _select_top_non_overlapping(candidates, max_count, buffer_sec=0.0):
            try:
                # å…ˆæŒ‰è¯„åˆ†é™åºï¼Œå†æŒ‰æ—¶é•¿é™åºï¼Œå°½é‡ä¼˜å…ˆé€‰æ‹©é«˜åˆ†ä¸”æ›´é•¿çš„ç‰‡æ®µ
                sorted_by_score = sorted(
                    candidates,
                    key=lambda x: (float(x.get('score', 0.0)), float(x.get('end', 0.0)) - float(x.get('start', 0.0))),
                    reverse=True
                )
                selected = []
                for seg in sorted_by_score:
                    s = float(seg.get('start', 0.0))
                    e = float(seg.get('end', 0.0))
                    if e <= s:
                        continue
                    no_conflict = True
                    for chosen in selected:
                        cs = float(chosen.get('start', 0.0))
                        ce = float(chosen.get('end', 0.0))
                        # åˆ¤æ–­æ˜¯å¦æœ‰é‡å ï¼ˆå«ç¼“å†²ï¼‰
                        if not (e <= cs - buffer_sec or s >= ce + buffer_sec):
                            no_conflict = False
                            break
                    if no_conflict:
                        selected.append(seg)
                        if len(selected) >= max_count:
                            break
                return selected
            except Exception as _e:
                log_warning(f"[é€‰æ‹©] éé‡å é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹Top-N: {_e}")
                return sorted(candidates, key=lambda x: x.get('score', 0), reverse=True)[:max_count]

        if not has_chat:
            # æ— å¼¹å¹•æ¨¡å¼çš„è¿‡æ»¤
            old_count = len(all_segments)
            all_segments = [seg for seg in all_segments if seg.get('music_probability', 0) < 0.95]
            new_count = len(all_segments)
            log_info(f"ğŸµ éŸ³ä¹è¿‡æ»¤: {old_count} â†’ {new_count} (ç§»é™¤ {old_count - new_count} ä¸ª)")
            
            threshold = 0.2
            old_count = len(all_segments)
            filtered_segments = [seg for seg in all_segments if seg.get('relative_score', 0) > threshold]
            new_count = len(filtered_segments)
            log_info(f"ğŸ“Š åˆ†æ•°è¿‡æ»¤: {old_count} â†’ {new_count} (é˜ˆå€¼: {threshold})")
            
            buffer_sec = float(config.get("NON_OVERLAP_BUFFER_SECONDS", 0.0)) if isinstance(config.get("NON_OVERLAP_BUFFER_SECONDS", 0.0), (int, float)) else 0.0
            candidates = filtered_segments if len(filtered_segments) >= 1 else all_segments
            # å…ˆå‡†å¤‡ä¸€ä¸ªè¾ƒå¤§çš„å€™é€‰é›†åˆï¼Œå†åšâ€œéé‡å è´ªå¿ƒâ€æŒ‘é€‰
            candidates_sorted = sorted(candidates, key=lambda x: x.get('score', 0), reverse=True)
            top_pool = candidates_sorted[: max(top_n * 5, top_n)]  # å¢å¤§å€™é€‰æ± ï¼Œæå‡éé‡å å¯é€‰æ€§
            top_segments = _select_top_non_overlapping(top_pool, top_n, buffer_sec=buffer_sec)
            if len(top_segments) < top_n:
                log_warning(f"[é€‰æ‹©] éé‡å çº¦æŸä¸‹ä»…é€‰å‡º {len(top_segments)}/{top_n} ä¸ªç‰‡æ®µ")
        else:
            # æœ‰å¼¹å¹•æ¨¡å¼çš„è¿‡æ»¤
            old_count = len(all_segments)
            filtered_segments = [seg for seg in all_segments if seg.get('music_probability', 0) < 0.95]
            new_count = len(filtered_segments)
            log_info(f"ğŸµ éŸ³ä¹è¿‡æ»¤: {old_count} â†’ {new_count}")
            
            buffer_sec = float(config.get("NON_OVERLAP_BUFFER_SECONDS", 0.0)) if isinstance(config.get("NON_OVERLAP_BUFFER_SECONDS", 0.0), (int, float)) else 0.0
            candidates = filtered_segments if len(filtered_segments) >= 1 else all_segments
            candidates_sorted = sorted(candidates, key=lambda x: x.get('score', 0), reverse=True)
            top_pool = candidates_sorted[: max(top_n * 5, top_n)]
            top_segments = _select_top_non_overlapping(top_pool, top_n, buffer_sec=buffer_sec)
            if len(top_segments) < top_n:
                log_warning(f"[é€‰æ‹©] éé‡å çº¦æŸä¸‹ä»…é€‰å‡º {len(top_segments)}/{top_n} ä¸ªç‰‡æ®µ")
        
        # ç¡®ä¿æœ‰ç»“æœ
        if not top_segments and all_segments:
            log_info("ğŸ”„ ç´§æ€¥å›é€€ï¼šä½¿ç”¨æ‰€æœ‰ç‰‡æ®µä¸­çš„æœ€é«˜åˆ†")
            top_segments = sorted(all_segments, key=lambda x: x.get('score', 0), reverse=True)[:min(top_n, len(all_segments))]
        
        log_info(f"ğŸ¯ æœ€ç»ˆé€‰æ‹©: âœ… {len(top_segments)} ä¸ªé«˜å…´è¶£ç‰‡æ®µ")
        
        # ğŸ†• è¯¦ç»†æ˜¾ç¤ºTopç‰‡æ®µä¿¡æ¯ï¼ˆæ˜¾ç¤ºæ›´å¤šç‰‡æ®µï¼‰
        log_info("ğŸ“Š è¯¦ç»†ç‰‡æ®µä¿¡æ¯:")
        for i, seg in enumerate(top_segments[:min(20, len(top_segments))], 1):
            score = seg.get('score', 0)
            music_prob = seg.get('music_probability', 0)
            text_preview = seg.get('text', '')[:50] + "..." if len(seg.get('text', '')) > 50 else seg.get('text', '')
            log_info(f"   #{i:2d}: {seg['start']:.1f}-{seg['end']:.1f}s, "
                    f"åˆ†æ•°={score:.3f}, éŸ³ä¹æ¦‚ç‡={music_prob:.2f}, "
                    f"æ–‡æœ¬=\"{text_preview}\"")
        
        if len(top_segments) > 20:
            log_info(f"   ... è¿˜æœ‰ {len(top_segments) - 20} ä¸ªç‰‡æ®µæœªæ˜¾ç¤º")
        
        # ğŸ†• ç»Ÿè®¡è¯„åˆ†åˆ†å¸ƒ
        scores = [seg.get('score', 0) for seg in top_segments]
        if scores:
            avg_score = sum(scores) / len(scores)
            max_score = max(scores)
            min_score = min(scores)
            log_info(f"ğŸ“ˆ è¯„åˆ†ç»Ÿè®¡: æœ€é«˜={max_score:.3f}, æœ€ä½={min_score:.3f}, å¹³å‡={avg_score:.3f}")
        
        # ç¬¬7é˜¶æ®µï¼šç»“æœä¿å­˜ï¼ˆè¯­ä¹‰è‡ªé€‚åº”ç‰‡æ®µï¼Œä¸å†å¼ºåˆ¶5åˆ†é’Ÿï¼‰
        write_progress_file("ä¿å­˜ç»“æœ", 8, 10, "ä¿å­˜åˆ†æç»“æœ...")

        output_dir = os.path.dirname(output_file)
        os.makedirs(output_dir, exist_ok=True)

        # åŸºäºè¯­ä¹‰è¯„åˆ†çš„å¯å˜æ—¶é•¿ç‰‡æ®µï¼šç›´æ¥ä½¿ç”¨ top_segments
        # æ„å»º ratings.jsonï¼ˆä¾›ç®¡ç†é¡µä¸å¤–éƒ¨å·¥å…·ä½¿ç”¨ï¼‰
        ratings_data = {}
        for i, seg in enumerate(top_segments, 1):
            s = float(seg.get('start', 0.0))
            e = float(seg.get('end', 0.0))
            name = f"clip_{i:03d}_{s:.1f}s-{e:.1f}s.mp4"
            ratings_data[name] = {
                'rating': round(float(seg.get('score', 0.0)), 2),
                'start': s,
                'end': e,
                'duration': max(0.0, e - s),
                'text': seg.get('text', ''),
                'semantic_variable': True,
                'segment_index': i
            }

        try:
            ratings_file = os.path.join(os.path.dirname(output_file), 'ratings.json')
            with open(ratings_file, 'w', encoding='utf-8') as f:
                json.dump(ratings_data, f, ensure_ascii=False, indent=4)
            log_info(f"âœ… è¯­ä¹‰è‡ªé€‚åº” ratings.json å·²ä¿å­˜: {ratings_file}")
        except Exception as e:
            log_error(f"âŒ ratings.json ä¿å­˜å¤±è´¥: {e}")

        # ACFV å…¼å®¹å¯¼å‡ºï¼šä½¿ç”¨ top_segments ä¸ ratings_data
        try:
            _write_acfv_exports(os.path.dirname(output_file), ratings_data, top_segments)
        except Exception as e:
            log_error(f"âŒ ACFVå¯¼å‡ºå¤±è´¥: {e}")

        # ä¿å­˜æœ€ç»ˆç»“æœï¼ˆtop_segmentsï¼‰
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(top_segments, f, ensure_ascii=False, indent=4)
            log_info(f"âœ… æœ€ç»ˆç»“æœå·²ä¿å­˜: {output_file}")
        except Exception as e:
            log_error(f"âŒ æœ€ç»ˆç»“æœä¿å­˜å¤±è´¥: {e}")

        # å¯é€‰ï¼šå°†ç”¨æˆ·æ‰‹åŠ¨è¯„åˆ†çš„æ­£åé¦ˆåˆ‡ç‰‡å†™å…¥RAGæ•°æ®åº“ï¼ˆä»…å¯ç”¨æ—¶ï¼‰
        try:
            if rag_enable and rag_db and ratings_data:
                base_dir = os.path.dirname(output_file)
                # å°†Topç‰‡æ®µå†™å…¥RAGåº“ï¼ˆä½¿ç”¨ratings.jsonä¸­çš„æ–‡æœ¬ä¸è¯„åˆ†ï¼‰
                for name, rec in ratings_data.items():
                    if float(rec.get('rating', 0.0)) > 0:
                        clip_path = os.path.join(base_dir, "..", "output_clips", name)
                        rag_db.add_liked_clip_vector(
                            clip_path=clip_path,
                            transcript_text=rec.get('text', ''),
                            video_name=os.path.basename(os.path.dirname(base_dir)),
                            clip_start_time=float(rec.get('start', 0.0)),
                            clip_end_time=float(rec.get('end', 0.0)),
                            user_rating=int(round(float(rec.get('rating', 0.0))*5)) if isinstance(rec.get('rating', 0.0), (int, float)) else 5
                        )
                # è¡¥å…¨å‘é‡
                try:
                    created = rag_db.ensure_embeddings()
                    log_info(f"[RAG] æœ¬æ¬¡æ–°å¢å‘é‡: {created}")
                except Exception:
                    pass
        except Exception as e:
            log_warning(f"[RAG] å†™å…¥ç”¨æˆ·è¯„åˆ†åˆ‡ç‰‡å¤±è´¥ï¼ˆä¸å½±å“æµç¨‹ï¼‰: {e}")

        # ä¿å­˜åˆ†ææŠ¥å‘Š
        try:
            interest_txt = os.path.splitext(output_file)[0] + '.txt'
            with open(interest_txt, 'w', encoding='utf-8') as f:
                f.write("High-Interest Segments (è¶…å¿«ä¼˜åŒ–ç‰ˆ)\n")
                f.write("=" * 80 + "\n")
                f.write(f"æ¢å¤æ¨¡å¼: {'âœ… ç»§ç»­ä¹‹å‰çš„åˆ†æ' if should_resume else 'ğŸ†• é‡æ–°å¼€å§‹'}\n")
                f.write(f"è¶…å¿«æ¨¡å¼: {'âœ… å¯ç”¨' if enable_ultra_fast else 'âŒ ç¦ç”¨'}\n")
                f.write(f"å¤„ç†ç‰‡æ®µ: {len(all_segments)} ä¸ª\n")
                f.write(f"æœ€ç»ˆé€‰æ‹©: {len(top_segments)} ä¸ª\n")
                f.write(f"ç‰¹å¾è®¡ç®—é€Ÿåº¦: {speed:.1f} ç‰‡æ®µ/ç§’\n")
                f.write("-" * 80 + "\n")
                f.write(f"{'Idx':<5}{'Start':<10}{'End':<10}{'Score':<10}{'MusicProb':<12}{'Text':<30}\n")
                f.write("-" * 80 + "\n")
                for i, seg in enumerate(top_segments, 1):
                    music_prob = seg.get('music_probability', 0.0)
                    f.write(
                        f"{i:<5}"
                        f"{seg['start']:<10.1f}"
                        f"{seg['end']:<10.1f}"
                        f"{seg['score']:<10.2f}"
                        f"{music_prob:<12.2f}"
                        f"{seg.get('text', '')[:30]}\n"
                    )
            log_info("âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜")
        except Exception as e:
            log_error(f"âŒ åˆ†ææŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")

        # ç¬¬8é˜¶æ®µï¼šæ¸…ç†èµ„æº
        write_progress_file("æ¸…ç†èµ„æº", 9, 10, "æ¸…ç†ä¸´æ—¶èµ„æº...")

        # æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼ˆæˆåŠŸå®Œæˆåï¼‰
        checkpoint_manager.clear_checkpoint()
        log_info("ğŸ§¹ æ£€æŸ¥ç‚¹æ–‡ä»¶å·²æ¸…ç†")

        # GPUå†…å­˜æ¸…ç†
        if torch and optimal_device and optimal_device.type == 'cuda':
            try:
                torch.cuda.empty_cache()
                log_info("âœ… GPUå†…å­˜æ¸…ç†å®Œæˆ")
            except Exception:
                pass

        # æ¸…ç†è¿›åº¦æ–‡ä»¶
        try:
            progress_file = processing_path("analysis_progress.json")
            if progress_file.exists():
                progress_file.unlink()
        except Exception:
            pass

        write_progress_file("å®Œæˆ", 10, 10, "è§†é¢‘å†…å®¹åˆ†æå®Œæˆ")

        # å®Œæˆç»Ÿè®¡
        total_time = time.time() - start_time
        log_info("=" * 80)
        log_info("ğŸ‰ è§†é¢‘å†…å®¹åˆ†æå®Œæˆ! (è¶…å¿«ä¼˜åŒ–ç‰ˆ)")
        log_info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        log_info(f"   - æ¢å¤æ¨¡å¼: {'âœ… ç»§ç»­ä¹‹å‰çš„åˆ†æ' if should_resume else 'ğŸ†• é‡æ–°å¼€å§‹'}")
        log_info(f"   - å¤„ç†ç‰‡æ®µæ•°: {len(all_segments)}")
        log_info(f"   - æœ€ç»ˆé€‰æ‹©: {len(top_segments)}")
        log_info(f"   - æ€»è€—æ—¶: {total_time:.1f}ç§’")
        log_info(f"   - å¹³å‡æ¯ç‰‡æ®µ: {total_time/len(remaining_segments if remaining_segments else [1]):.3f}ç§’")
        log_info(f"   - ç‰¹å¾è®¡ç®—é€Ÿåº¦: {speed:.1f} ç‰‡æ®µ/ç§’")
        log_info(f"   - ç»“æœæ–‡ä»¶: {output_file}")
        log_info("=" * 80)

        return top_segments
        
    except KeyboardInterrupt:
        log_info("â¸ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æï¼Œä¿å­˜å½“å‰è¿›åº¦...")
        
        # ä¿å­˜ä¸­æ–­æ—¶çš„æ£€æŸ¥ç‚¹
        if 'all_segments' in locals() and 'metadata' in locals():
            current_index = len(all_segments)
            checkpoint_manager.save_checkpoint(all_segments, metadata, current_index)
            log_info(f"ğŸ’¾ ä¸­æ–­æ£€æŸ¥ç‚¹å·²ä¿å­˜: {len(all_segments)} ä¸ªç‰‡æ®µ")
        
        write_progress_file("ä¸­æ–­", 0, 10, "ç”¨æˆ·ä¸­æ–­åˆ†æï¼Œå·²ä¿å­˜è¿›åº¦")
        return []
        
    except Exception as e:
        log_error(f"âŒ åˆ†æè¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
        write_progress_file("é”™è¯¯", 0, 10, f"åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return []

# å…¼å®¹åŸæ¥å£
def analyze_data(chat_file, transcription_file, output_file, 
                video_emotion_file=None, video_emotion_weight=0.3, 
                top_n=None, enable_video_emotion=None, device='cuda:0',
                progress_callback=None):
    """
    ä¸»è¦åˆ†æå‡½æ•° - å…¼å®¹åŸæ¥å£ï¼Œè‡ªåŠ¨æ£€æµ‹æ–­ç‚¹ç»­ä¼ 
    """
    return analyze_data_with_checkpoint(
        chat_file, transcription_file, output_file,
        video_emotion_file, video_emotion_weight,
        top_n, enable_video_emotion, device,
        progress_callback, resume_checkpoint=None  # è‡ªåŠ¨æ£€æµ‹
    )

def analyze_data_with_checkpoint_new(video_clips_dir, config_manager, resume_mode=None, progress_callback=None):
    """
    æ–°çš„åˆ†æå‡½æ•°ï¼Œæ¥å—è§†é¢‘å‰ªè¾‘ç›®å½•å’Œé…ç½®ç®¡ç†å™¨
    
    Args:
        video_clips_dir: è§†é¢‘å‰ªè¾‘ç›®å½•
        config_manager: é…ç½®ç®¡ç†å™¨
        resume_mode: æ¢å¤æ¨¡å¼ (None=è‡ªåŠ¨æ£€æµ‹, True=ç»§ç»­, False=é‡æ–°å¼€å§‹)
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (stage_name, substage_index, progress)
    
    Returns:
        dict: åˆ†æç»“æœ
    """
    log_info("=" * 80)
    log_info("ğŸš€ å¼€å§‹è§†é¢‘å†…å®¹åˆ†æ (é›†æˆç‰ˆ)")
    log_info("=" * 80)
    
    def update_progress(substage_index, progress):
        """å†…éƒ¨è¿›åº¦æ›´æ–°å‡½æ•°"""
        if progress_callback:
            try:
                progress_callback("å†…å®¹åˆ†æ", substage_index, progress)
            except Exception as e:
                log_info(f"è¿›åº¦æ›´æ–°å¤±è´¥: {e}")
    
    def should_stop():
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢å¤„ç†"""
        try:
            stop_flag_file = processing_path("stop_flag.txt")
            return stop_flag_file.exists()
        except Exception:
            return False
    
    # æ·»åŠ åœæ­¢æ£€æŸ¥
    if should_stop():
        log_info("ğŸ›‘ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œåˆ†æè¢«ä¸­æ–­")
        return None
    
    # ğŸ†• å­é˜¶æ®µ0: å…³é”®è¯æå– - åˆå§‹åŒ–
    update_progress(0, 0.0)
    
    start_time = time.time()
    
    # ä»é…ç½®ç®¡ç†å™¨è·å–å‚æ•°
    data_dir = os.path.join(video_clips_dir, "data")
    chat_file = os.path.join(data_dir, "chat_with_emotes.json")
    transcription_file = os.path.join(data_dir, "transcription.json")
    host_transcription_file = os.path.join(data_dir, "host_transcription.json")
    video_emotion_file = os.path.join(data_dir, "video_emotion_4s.json")
    analysis_output = os.path.join(data_dir, "high_interest_segments.json")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸»æ’­è½¬å½•æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ä¸»æ’­è½¬å½•
    use_host_transcription = False
    if os.path.exists(host_transcription_file) and os.path.getsize(host_transcription_file) > 10:
        log_info(f"ğŸ¯ å‘ç°ä¸»æ’­è½¬å½•æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ä¸»æ’­è½¬å½•è¿›è¡Œå…´è¶£åˆ¤æ–­: {host_transcription_file}")
        transcription_file = host_transcription_file
        use_host_transcription = True
    else:
        log_info(f"ğŸ“ ä½¿ç”¨å®Œæ•´è½¬å½•æ–‡ä»¶è¿›è¡Œå…´è¶£åˆ¤æ–­: {transcription_file}")
        use_host_transcription = False
    
    # åœæ­¢æ£€æŸ¥
    if should_stop():
        log_info("ğŸ›‘ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œåˆ†æåœ¨æ–‡ä»¶æ£€æŸ¥åè¢«ä¸­æ–­")
        return None
    
    # ğŸ†• æ–‡ä»¶æ£€æŸ¥å®Œæˆ
    update_progress(0, 0.3)
    
    # è·å–é…ç½®å‚æ•°
    top_n = int(config_manager.get("MAX_CLIP_COUNT") or 10)
    enable_video_emotion = config_manager.get("ENABLE_VIDEO_EMOTION", False)
    video_emotion_weight = float(config_manager.get("VIDEO_EMOTION_WEIGHT") or 0.3)
    device = config_manager.get("GPU_DEVICE") or "cuda:0"
    
    # ğŸ†• å‚æ•°åŠ è½½å®Œæˆ
    update_progress(0, 0.6)
    
    # æ£€æŸ¥æ˜¯å¦æ¢å¤æ£€æŸ¥ç‚¹
    should_resume = False
    if resume_mode is None:
        # è‡ªåŠ¨æ£€æµ‹
        should_resume = checkpoint_manager.has_checkpoint()
    elif resume_mode:
        # å¼ºåˆ¶æ¢å¤
        should_resume = checkpoint_manager.has_checkpoint()
    else:
        # å¼ºåˆ¶é‡æ–°å¼€å§‹
        checkpoint_manager.clear_checkpoint()
        should_resume = False
    
    # ğŸ†• å…³é”®è¯æå–å®Œæˆ
    update_progress(0, 1.0)
    
    log_info(f"ğŸ“‹ åˆ†æå‚æ•°:")
    log_info(f"   - è§†é¢‘ç›®å½•: {video_clips_dir}")
    log_info(f"   - æ¢å¤æ¨¡å¼: {'âœ… ç»§ç»­ä¹‹å‰çš„åˆ†æ' if should_resume else 'ğŸ†• é‡æ–°å¼€å§‹'}")
    log_info(f"   - è®¡ç®—è®¾å¤‡: {device}")
    log_info(f"   - æœ€å¤§åˆ‡ç‰‡æ•°: {top_n}")
    log_info(f"   - è§†é¢‘æƒ…ç»ªåˆ†æ: {'âœ… å¯ç”¨' if enable_video_emotion else 'âŒ ç¦ç”¨'}")
    log_info(f"   - è½¬å½•ç±»å‹: {'ğŸ¯ ä¸»æ’­è½¬å½•' if use_host_transcription else 'ğŸ“ å®Œæ•´è½¬å½•'}")
    
    # ğŸ†• å­é˜¶æ®µ1: å…´è¶£è¯„åˆ† - å¼€å§‹
    update_progress(1, 0.0)
    
    try:
        # è°ƒç”¨åŸæœ‰çš„åˆ†æå‡½æ•°ï¼ŒåŒæ—¶ä¼ é€’è‡ªå®šä¹‰çš„è¿›åº¦å›è°ƒ
        def detailed_progress_callback(current, total, detail=""):
            """è¯¦ç»†è¿›åº¦å›è°ƒ"""
            if total > 0:
                progress = current / total
                update_progress(1, progress)  # å…´è¶£è¯„åˆ†é˜¶æ®µ
        
        result = analyze_data_with_checkpoint(
            chat_file=chat_file,
            transcription_file=transcription_file,
            output_file=analysis_output,
            video_emotion_file=video_emotion_file if enable_video_emotion else None,
            video_emotion_weight=video_emotion_weight,
            top_n=top_n,
            enable_video_emotion=enable_video_emotion,
            device=device,
            progress_callback=detailed_progress_callback,  # ğŸ†• ä¼ é€’è¿›åº¦å›è°ƒ
            resume_checkpoint=should_resume
        )
        
        # ğŸ†• å…´è¶£è¯„åˆ†å®Œæˆ
        update_progress(1, 1.0)
        
        # ğŸ†• å­é˜¶æ®µ2: ç‰‡æ®µæ’åº
        update_progress(2, 0.0)
        
        processing_time = time.time() - start_time
        log_info(f"âœ… åˆ†æå®Œæˆï¼Œè€—æ—¶: {processing_time:.1f}ç§’")
        
        # ğŸ†• ç‰‡æ®µæ’åºå®Œæˆ
        update_progress(2, 1.0)
        
        return {
            "success": True,
            "segments": result,
            "processing_time": processing_time,
            "output_file": analysis_output
        }
        
    except Exception as e:
        log_error(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        log_error(f"âŒ åˆ†æå¤±è´¥è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        raise  # é‡æ–°
